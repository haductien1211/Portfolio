{
  "hash": "ae4d511c90ad4c7ac47d44bf7f8cd812",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Project 2\"\nauthor: \"Ha Duc Tien\"\ndate: \"September 23, 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\n\n# 1. Objectives and Tasks\n\n## 1.1 Objectives\n\nTourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion USD from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion USD in 2020. However, it is important to note that the tourism economy of Thailand are not evenly distributed.\n\nWe are interested to discover:\n\n-   If the key indicators of tourism economy of Thailand are independent from space and space and time.\n-   If the tourism economy is indeed spatial and spatio-temporal dependent, then, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.\n\n## 1.2 The Tasks\n\nThe specific tasks of this take-home exercise are as follows:\n\n-   Using appropriate function of `sf` and `tidyverse`, preparing the following geospatial data layer:\n    -   a study area layer in sf polygon features. It must be at [province level](https://en.wikipedia.org/wiki/Provinces_of_Thailand) (including Bangkok) of Thailand.\n    -   a tourism economy indicators layer within the study area in sf polygon features.\n    -   a derived tourism economy indicator layer in [spacetime s3](https://sfdep.josiahparry.com/articles/spacetime-s3) class of `sfdep`. Keep the time series at **month** and **year** levels.\n-   Using the extracted data, perform [global spatial autocorrelation analysis](https://r4gdsa.netlify.app/chap09) by using `sfdep` methods.\n-   Using the extracted data, perform [local spatial autocorrelation analysis](https://r4gdsa.netlify.app/chap10.html) by using `sfdep` methods.\n-   Using the extracted data, perform [emerging hotspot analysis](https://isss626-ay2024-25aug.netlify.app/in-class_ex/in-class_ex06/in-class_ex06-ehsa#/title-slide) by using `sfdep` methods.\n-   Describe the spatial patterns revealed by the analysis above.\n\n## 1.3 The packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, sfdep, spdep, tmap, plotly, tidyverse, Kendall)\n```\n:::\n\n\n\n# 2. The data\n\n## 2.1 Importing the raw data\n\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\n-   [Thailand Domestic Tourism Statistics](https://www.kaggle.com/datasets/thaweewatboy/thailand-domestic-tourism-statistics) at Kaggle. We are required to use *version 2* of the data set.\n\n-   [Thailand - Subnational Administrative Boundaries](https://data.humdata.org/dataset/cod-ab-tha?) at HDX. We are required to use the province boundary data set.\n\n\nThe code chunk below is used to import **Thailand - Subnational Administrative Boundaries** as well as filtering out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create `THSAB_sf`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTHSAB_sf <- st_read(dsn = \"data/geo\", \n                         layer = \"tha_admbnda_adm1_rtsd_20220121\") %>%\n  st_transform(crs = 32647)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\Portfolio\\R-GAA\\Project02\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n:::\n\n\nThis code chunk is to import **Thailand Domestic Tourism Statistics** data and create `tourism`. \n\nIn the below code I will also create 2 new columns for the Month and Year separately for the purpose of using them for later analysis as well as converting the `province_thai` column name to `ADM1_TH` for the purpose of left-joining with the GEO data later and removing the date column as the 2 new Month and Year column are already created.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourism <- read_csv(\"data/non-geo/thailand_domestic_tourism_2019_2023_ver2.csv\") %>%\n  mutate(`month` = as.numeric(format(as.Date(`date`), \"%m\"))) %>%\n  mutate(`year` = as.numeric(format(as.Date(`date`), \"%Y\"))) %>%\n  rename(`ADM1_TH` = `province_thai`) %>%\n  select(2:9)\n```\n:::\n\n\n\n# 2.2 Data exploration and wrangling\n\n\n### 2.2.1 The GEO data\n\nLet first look at what we have for the provinces\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(THSAB_sf) +\n  tm_polygons() +\n  tm_text(\"ADM1_EN\", size=0.5)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-4-1.png){width=1100}\n:::\n:::\n\n\n\n\nSince most of the analysis that I would be doing later involved (QUEEN) contiguity weight matrix computation I am curious to see if there was any problem with the computation such as geo location without any neigbors or missing links\n\nThe code chunk below is to compute Queen contiguity weight matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthai_wm_q <- poly2nb(THSAB_sf, queen=TRUE)\n```\n:::\n\n\n\nThe code below is to quickly review the summary of the results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(thai_wm_q)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n67\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n14 with 1 link\n2 most connected regions:\n29 51 with 9 links\n```\n\n\n:::\n:::\n\n\nInteresting there was 1 region with no links, this mean the Queen contiguity weight matrix computation later needs some adjustments\n\nReview the `thai_wm_q` data itselft I know that the problematic province is 67. The same could be seen using the code below\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthai_wm_q[[67]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nLet check which one are they using the code below\n \n\n\n::: {.cell}\n\n```{.r .cell-code}\nTHSAB_sf$ADM1_EN[67]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Phuket\"\n```\n\n\n:::\n:::\n\n\n\nNow let's try to visualise this on a contiguity based neighbours connectivity graph.\n\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and \n\n1. This allows us to get only the longitude, which is the first value in each centroid.\n2. We do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n3. Now that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlongitude <- map_dbl(THSAB_sf$geometry, ~st_centroid(.x)[[1]])\nlatitude <- map_dbl(THSAB_sf$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitude, latitude)\n```\n:::\n\n\n\nThe code below is to plot Queen contiguity based neighbours connectivity map\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(THSAB_sf$geometry, border=\"lightgrey\")\nplot(thai_wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-10-1.png){width=1100}\n:::\n:::\n\n\n\n:::callout-note\n\nWe could see that **Phuket** is disjoint from the rest of the neighbors (bottom left region), hence during the Queen computation a `snap` argument would most likely need to be used to resolve this. Therefore, I'll be using a snap value of 400 in the Queen computation of neigbors moving forward\n\n:::\n\nLet rerun the code with `snap` = 400 and see the results using the code below\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthai_wm_q <- poly2nb(THSAB_sf, queen=TRUE, snap = 400)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(THSAB_sf$geometry, border=\"lightgrey\")\nplot(thai_wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-12-1.png){width=1100}\n:::\n:::\n\n\n**The issue with disjointed Phuket seems to be resolved**\n\n### 2.2.2 The tourism data\n\nThis code below is to explore the different variable or indicator available in the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(tourism$variable)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ratio_tourist_stay\" \"no_tourist_stay\"    \"no_tourist_all\"    \n[4] \"no_tourist_thai\"    \"no_tourist_foreign\" \"revenue_all\"       \n[7] \"revenue_thai\"       \"revenue_foreign\"   \n```\n\n\n:::\n:::\n\n\n\nThere seems to be 8 different variable for analysis in this case, for the purpose of this study I'll be focusing on Revenue mostly, mainly `revenue_foreign` the revenue from Foreign Tourist and if we have the opportunity, diving into other variable including `revenue_foreign` and `revenue_all`. The reason for this change is because majority of revenue of Thai Tourism is from Foreign visitors and therefore the impact of Covid 19 to this maybe more significant. In addition the study will look more into the changes over the `year` period\n\n\nFirst is to get all the data filter out using the specific revenue variable for the analysis. To create 3 new table `tourism_revenue_thai`, `tourism_revenue_foreign`, `tourism_revenue_all`, the data would have its Month and Year columns intact useful for later study into changes over the Month\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntourism_revenue_thai <- tourism %>%\n  filter(variable == 'revenue_thai')\n\ntourism_revenue_foreign <- tourism %>%\n  filter(variable == 'revenue_foreign')\n\ntourism_revenue_all <- tourism %>%\n  filter(variable == 'revenue_all')\n```\n:::\n\n\n\n\n\nFor study into changes over the year, I will do a sum of revenue over the year and create 3 new table `revenue_thai_year`, `revenue_foreign_year`, `revenue_all_year`, using the `group_by` and `summarise`. One of the thing I notice during my reviewing of the data is that it would only go up to February of 2023 hence grouping and summing them that include 2023 would create disparity in the data hence for the year data study I would only consider from 2019 to 2022 and omitting the 2023 period.\n\nThe code below is used to do all the above\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrevenue_thai_year <- tourism_revenue_thai %>%\n  select(1:8) %>%\n  group_by(year, ADM1_TH, province_eng) %>%\n  summarise(sum_rev = sum(value)) %>%\n  filter(!year == 2023) %>%\n  ungroup()\n\nrevenue_foreign_year <- tourism_revenue_foreign %>%\n  select(1:8) %>%\n  group_by(year, ADM1_TH, province_eng) %>%\n  summarise(sum_rev = sum(value)) %>%\n  filter(!year == 2023) %>%\n  ungroup()\n\nrevenue_all_year <- tourism_revenue_all %>%\n  select(1:8) %>%\n  group_by(year, ADM1_TH, province_eng) %>%\n  summarise(sum_rev = sum(value)) %>%\n  filter(!year == 2023) %>%\n  ungroup()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = revenue_foreign_year,  \n       aes(sum_rev,\n           fct_reorder(province_eng, sum_rev))) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n# 3. The analysis\n\n## 3.1 Visualising Foreign Revenue Indicator\n\nFirst I want to merge the yearly foreign revenue table `revenue_foreign_year` with the GEO data `THSAB_sf` for easier analysis later. This is done using the code below\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrevenue_foreign <- revenue_foreign_year %>%\n  left_join(THSAB_sf) %>%\n  select(1:2,4, 7, 20)\n```\n:::\n\n\nBefore we start the analysis let create a spactime data `revenue_foreign_st` using `revenue_foreign` for the purpose of study later\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrevenue_foreign_st <- spacetime(revenue_foreign,\n                                THSAB_sf,\n                                .loc_col = \"ADM1_EN\",\n                                .time_col = \"year\")\n```\n:::\n\n\n\nFor the basic visualization I would still want to see if there are any potential cluster and I want to see the changes of Foreign Revenue cluster over the year. Hence for this purpose I would plot 4 graph using the data extract from `revenue_foreign_2019`. Therefore I will be using `bclust` style which is a good combination between `kmeans` and `hclust` to fill the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrevenue_foreign_2019 <- revenue_foreign %>%\n  filter(year == 2019)\nrevenue_foreign_2020 <- revenue_foreign %>%\n  filter(year == 2020)\nrevenue_foreign_2021 <- revenue_foreign %>%\n  filter(year == 2021)\nrevenue_foreign_2022 <- revenue_foreign %>%\n  filter(year == 2022)\n\nRF2019 <- tm_shape(st_as_sf(revenue_foreign_2019)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2019\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2019\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2020 <- tm_shape(st_as_sf(revenue_foreign_2020)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2020\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2020\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2021 <- tm_shape(st_as_sf(revenue_foreign_2021)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2021\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2021\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2022 <- tm_shape(st_as_sf(revenue_foreign_2022)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2022\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2022\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_arrange(RF2019, RF2020, RF2021, RF2022, asp=1, ncol=4)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-20-1.png){width=3000}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n```\n\n\n:::\n:::\n\n\n\n\n## 3.2 Global Measures of Spatial Autocorrelation\n\n\nI've previously created the Queen contiguity weight matrix `thai_wm_q` with snap = 400. Next we need to create Row-standardised weights matrix using the code below\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthai_rswm_q <- nb2listw(thai_wm_q,\n                        style=\"W\",\n                        zero.policy = TRUE)\nthai_rswm_q\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 354 \nPercentage nonzero weights: 5.970653 \nAverage number of links: 4.597403 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1      S2\nW 77 5929 77 37.54724 320.752\n```\n\n\n:::\n:::\n\n\n\n\n### 3.2.1 Computing Global Carlo Moran’s I\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep. from 2019 to 2022\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q_2019 <- revenue_foreign_2019 %>%\n  # select(3:5) %>%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q_2020 <- revenue_foreign_2020 %>%\n  # select(3:5) %>%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q_2021 <- revenue_foreign_2021 %>%\n  # select(3:5) %>%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q_2022 <- revenue_foreign_2022 %>%\n  # select(3:5) %>%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n```\n:::\n\n\n\n2019 \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(wm_q_2019$sum_rev,\n                  wm_q_2019$nb,\n                  wm_q_2019$wt,\n                  nsim = 999)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.0065291, observed rank = 751, p-value = 0.498\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n\n2020 \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(wm_q_2020$sum_rev,\n                  wm_q_2020$nb,\n                  wm_q_2020$wt,\n                  nsim = 999)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.0044982, observed rank = 788, p-value = 0.424\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n\n2021\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(wm_q_2021$sum_rev,\n                  wm_q_2021$nb,\n                  wm_q_2021$wt,\n                  nsim = 999)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.011613, observed rank = 942, p-value = 0.116\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n\n2022 \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(wm_q_2022$sum_rev,\n                  wm_q_2022$nb,\n                  wm_q_2022$wt,\n                  nsim = 999)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.020738, observed rank = 691, p-value = 0.618\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\nAnother way to do this is using the below test method code chunk since we already have the listw of `thai_rswm_q`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nbperm_2019 = moran.mc(revenue_foreign_2019$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2019\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2019$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.024516, observed rank = 862, p-value = 0.138\nalternative hypothesis: greater\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nbperm_2020 = moran.mc(revenue_foreign_2020$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2020\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2020$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.027343, observed rank = 879, p-value = 0.121\nalternative hypothesis: greater\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nbperm_2021 = moran.mc(revenue_foreign_2021$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2021\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2021$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.014846, observed rank = 855, p-value = 0.145\nalternative hypothesis: greater\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nbperm_2022 = moran.mc(revenue_foreign_2022$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2022\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2022$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.015294, observed rank = 799, p-value = 0.201\nalternative hypothesis: greater\n```\n\n\n:::\n:::\n\n\n:::callout-tip\n\nAll the test over the year from 2019-2022 indicate that p-value > 0.05 hence the null hypothesis are not rejected\n\n:::\n\n### 3.2.2 Visualising Global Moran’s I\n\nThe code chunk below is used to plot a histogram of Simulated Moran's I\n\n:::panel-tabset\n\n## 2019 \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(bperm_2019$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2019 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-34-1.png){width=1500}\n:::\n:::\n\n\n\n## 2020\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(bperm_2020$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2020 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-35-1.png){width=1500}\n:::\n:::\n\n\n\n## 2021\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(bperm_2021$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2021 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-36-1.png){width=1500}\n:::\n:::\n\n\n\n## 2022\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(bperm_2022$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2022 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-37-1.png){width=1500}\n:::\n:::\n\n\n\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMI_corr_2019 <- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2019$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2019)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nMI_corr_2020 <- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2020$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2020)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nMI_corr_2021 <- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2021$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2021)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nMI_corr_2022 <- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2022$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2022)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n\n### 3.2.3 Computing local Moran’s I\n\nUsing the above created `wm_q` data, we could create the LISA Map and visualizaing the local Moran’s I. The below code is used to create the lisa mapping for Local Moran’s I of Foreign revenue at Province level by using local_moran() of `sfdep` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa_2019 <- wm_q_2019 %>% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %>%\n  unnest(local_moran)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa_2020 <- wm_q_2020 %>% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %>%\n  unnest(local_moran)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa_2021 <- wm_q_2021 %>% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %>%\n  unnest(local_moran)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa_2022 <- wm_q_2022 %>% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %>%\n  unnest(local_moran)\n```\n:::\n\n\n  \n    \n### 3.2.4 Visualising local Moran’s I\n\nVisualising local Moran’s I and p-value for each year\n\n:::panel-tabset\n\n## 2019\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n\nmap2019_1<- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2019\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2019_2 <- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2019_1, map2019_2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-46-1.png){width=2000}\n:::\n:::\n\n\n## 2020\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n\nmap2020_1<- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2020\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2020_2 <- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2020_1, map2020_2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-47-1.png){width=2000}\n:::\n:::\n\n\n\n\n## 2021\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n\nmap2021_1<- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2021\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2021_2 <- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2021_1, map2021_2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-48-1.png){width=2000}\n:::\n:::\n\n\n\n## 2022\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n\nmap2022_1<- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2022\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2022_2 <- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2022_1, map2022_2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-49-1.png){width=2000}\n:::\n:::\n\n\n\n:::\n\n\n### 3.2.5 Plotting LISA map\n\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa_sig_2019 <- lisa_2019  %>%\n  filter(p_ii_sim < 0.05)\n\nlisa_map_2019 <- tm_shape(st_as_sf(lisa_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2019\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2019)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2020 <- lisa_2020  %>%\n  filter(p_ii_sim < 0.05)\n\nlisa_map_2020 <- tm_shape(st_as_sf(lisa_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2020\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2020)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2021 <- lisa_2021  %>%\n  filter(p_ii_sim < 0.05)\n\nlisa_map_2021 <- tm_shape(st_as_sf(lisa_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2021\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2021)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2022 <- lisa_2022  %>%\n  filter(p_ii_sim < 0.05)\n\nlisa_map_2022 <- tm_shape(st_as_sf(lisa_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2022\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2022)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntmap_arrange(lisa_map_2019, lisa_map_2020, lisa_map_2021, lisa_map_2022, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-51-1.png){width=3000}\n:::\n:::\n\n\n\n## 3.3 Hot Spot and Cold Spot Area Analysis (HCSA)\n\n### 3.3.1 Computing local Gi* statistics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_idw_2019 <- revenue_foreign_2019 %>%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_idw_2020 <- revenue_foreign_2020 %>%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_idw_2021 <- revenue_foreign_2021 %>%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_idw_2022 <- revenue_foreign_2022 %>%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA_2019 <- wm_idw_2019 %>% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA_2020 <- wm_idw_2020 %>% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA_2021 <- wm_idw_2021 %>% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA_2022 <- wm_idw_2022 %>% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA_sig_2019 <- HCSA_2019  %>%\n  filter(p_sim < 0.05)\n\nHCSA_sig_2020 <- HCSA_2020  %>%\n  filter(p_sim < 0.05)\n\nHCSA_sig_2021 <- HCSA_2021  %>%\n  filter(p_sim < 0.05)\n\nHCSA_sig_2022 <- HCSA_2022  %>%\n  filter(p_sim < 0.05)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n\nHCSA_map_2019 <- tm_shape(st_as_sf(HCSA_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2019\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2019)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2020 <- tm_shape(st_as_sf(HCSA_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2020\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2020)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2021 <- tm_shape(st_as_sf(HCSA_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2021\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2021)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2022 <- tm_shape(st_as_sf(HCSA_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2022\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2022)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\ntmap_arrange(HCSA_map_2019, HCSA_map_2020, HCSA_map_2021, HCSA_map_2022, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-61-1.png){width=3000}\n:::\n:::\n\n\n\n:::callout-tip\n\nFigure above reveals the changes of hotspot and cold spot over the year.\n\n:::\n\n## 3.4 Emerging Hotspot Analysis\n\nWe previously already create Spacetime `revenue_foreign_st` that included the foreign revenue data from 2019-2022\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis_spacetime_cube(revenue_foreign_st)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### 3.4.1 Computing Gi*\n\n**Deriving the spatial weights**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrevenue_foreign_nb <- revenue_foreign_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wt = st_inverse_distance(nb,\n                                  geometry,\n                                  scale = 1,\n                                  alpha = 1),\n  .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n```\n:::\n\n\n\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngi_stars <- revenue_foreign_nb %>%\n  group_by(year) %>%\n  mutate(gi_star = local_gstar_perm(sum_rev,\n                                    nb,\n                                    wt)) %>%\n  unnest(gi_star)\n```\n:::\n\n\n\n\n**Mann-Kendall test data.frame**\nWe can replicate this for each location by using group_by() of dplyr package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa <- gi_stars %>%\n  group_by(ADM1_EN) %>%\n  summarise(mk = list(unclass(\n      Kendall::MannKendall(gi_star)\n    )\n  )) %>%\n  unnest_wider(mk)\n```\n:::\n\n\n\n### 3.4.2 Mann-Kendall Test on Gi*\n\nWith these Gi* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses **Bankok** county.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbg <- gi_stars %>% \n  ungroup() %>% \n  filter(ADM1_EN == \"Bangkok\") %>%\n  select(ADM1_EN, year, gi_star)\n```\n:::\n\n\n\n**Interactive Mann-Kendall Plot**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplotly(ggplot(data = cbg, \n       aes(x = year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light())\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-fb3fb6effa27c278561f\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-fb3fb6effa27c278561f\">{\"x\":{\"data\":[{\"x\":[2019,2020,2021,2022],\"y\":[4.8437194907054053,4.5304337660370706,3.5470056044419764,4.3637807659305015],\"text\":[\"year: 2019<br />gi_star: 4.843719\",\"year: 2020<br />gi_star: 4.530434\",\"year: 2021<br />gi_star: 3.547006\",\"year: 2022<br />gi_star: 4.363781\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":26.228310502283104,\"r\":7.3059360730593621,\"b\":40.182648401826491,\"l\":43.105022831050235},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[2018.8499999999999,2022.1500000000001],\"tickmode\":\"array\",\"ticktext\":[\"2019\",\"2020\",\"2021\",\"2022\"],\"tickvals\":[2019,2020,2021,2022],\"categoryorder\":\"array\",\"categoryarray\":[\"2019\",\"2020\",\"2021\",\"2022\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(179,179,179,1)\",\"ticklen\":3.6529680365296811,\"tickwidth\":0.33208800332088001,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(222,222,222,1)\",\"gridwidth\":0.33208800332088001,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"year\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[3.4821699101288051,4.9085551850185771],\"tickmode\":\"array\",\"ticktext\":[\"3.5\",\"4.0\",\"4.5\"],\"tickvals\":[3.5,4,4.5],\"categoryorder\":\"array\",\"categoryarray\":[\"3.5\",\"4.0\",\"4.5\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(179,179,179,1)\",\"ticklen\":3.6529680365296811,\"tickwidth\":0.33208800332088001,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(222,222,222,1)\",\"gridwidth\":0.33208800332088001,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"gi_star\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(179,179,179,1)\",\"width\":0.66417600664176002,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.8897637795275593,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.68949771689498}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"561c3c4152f9\":{\"x\":{},\"y\":{},\"type\":\"scatter\"}},\"cur_data\":\"561c3c4152f9\",\"visdat\":{\"561c3c4152f9\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n### 3.4.3 Performing Emerging Hotspot Analysis\n\nUsing `ehsa` We can also sort to show significant emerging hot/cold spots\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:10)\nhead(emerging)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  ADM1_EN          tau    sl     S     D  varS\n  <chr>          <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Bangkok       -0.667 0.308    -4  6.00  8.67\n2 Chachoengsao  -0.667 0.308    -4  6.00  8.67\n3 Chanthaburi   -0.667 0.308    -4  6.00  8.67\n4 Chon Buri     -0.667 0.308    -4  6.00  8.67\n5 Lampang        0.667 0.308     4  6.00  8.67\n6 Nakhon Pathom -0.667 0.308    -4  6.00  8.67\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nrevenue_ehsa <- emerging_hotspot_analysis(\n  x = revenue_foreign_nb,\n  .var = \"sum_rev\",\n  k = 1,\n  nsim = 199,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrevenue_foreign_ehsa <- THSAB_sf %>%\n  left_join(revenue_ehsa,\n            by = join_by(ADM1_EN == location))\n```\n:::\n\n\n\nVisualising the distribution of EHSA classes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = revenue_foreign_ehsa,\n       aes(x = classification)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-71-1.png){width=672}\n:::\n:::\n\n\n\nWe could see majority of location does not has any pattern\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode('plot')\nrevenue_foreign_ehsa_sig <- revenue_foreign_ehsa  %>%\n  filter(p_value < 1)\n\ntm_shape(st_as_sf(revenue_foreign_ehsa)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_text(\"ADM1_EN\", size=0.5) +\n  tm_shape(st_as_sf(revenue_foreign_ehsa_sig)) +\n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n```\n\n::: {.cell-output-display}\n![](Project02_files/figure-html/unnamed-chunk-72-1.png){width=1100}\n:::\n:::\n\n\n\n# 4. Conclusion\n\nThe analysis showed there is clearly uneven distribution of **Foreign revenue of Thailand Tourism** from 2019-2022. The Foreign Revenue Indicator graph showcase the distribution of Foreign Revenue and its sudden changes in the period from 2019-2022. The orginal clustering using `bclust` has show some potential clustering that needed further attention\n\nThe computation of Global Moran's I show that we are not able to reject the null hypothesis but we do see the changes of this results over the years\n\nThe calculation of Local Moran's I and plotting LISA map show us the hotspots and coldspots and their movement over the year. It is interesting however that some of the coldspots seem to be consist of some area over the years\n\nLastly the Emerging hotspots analysis showcase some of the emerging hotspots and coldspots of interest over the year",
    "supporting": [
      "Project02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"../../site_libs/plotly-binding-4.10.4/plotly.js\"></script>\n<script src=\"../../site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"../../site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"../../site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}