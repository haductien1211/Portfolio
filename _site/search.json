[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "R-GAA/Project01/Project01.html",
    "href": "R-GAA/Project01/Project01.html",
    "title": "Project 1",
    "section": "",
    "text": "Previous studies have demonstrated the significant potential of Spatial Point Patterns Analysis (SPPA) in exploring and identifying factors influencing road traffic accidents. However, these studies often focus solely on either behavioral or environmental factors, with limited consideration of temporal factors such as season, day of the week, or time of day.\n\nIn view of this, I am tasked to discover factors affecting road traffic accidents in the Bangkok Metropolitan Region BMR by employing both spatial spatio-temporal point patterns analysis methods.\n\nThe specific objectives of this take-home exercise are as follows:\n\nTo visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\nTo conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\nTo conduct detailed spatio-temporal analysis of road traffic accidents using appropriate Temporal Network Spatial Point Patterns Analysis methods."
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#importing-and-wrangling-the-data",
    "href": "R-GAA/Project01/Project01.html#importing-and-wrangling-the-data",
    "title": "Project 1",
    "section": "Importing and wrangling the data",
    "text": "Importing and wrangling the data\nThe below code would import the Thailand Road Accident (2019-2022), make changes to the coordinates by filtering out the ones with empty or NA coordinates then filter out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647. The operation would also create 2 new columns that has the Month and the Year of the date accidents occured. This step would create accident_data_sf\n\naccident_data_sf &lt;- read_csv(\"data/nongeo/thai_road_accident_2019_2022.csv\") %&gt;%\n  filter(!is.na(longitude) & longitude != \"\",\n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), \n           crs = 4326) %&gt;%\n  filter(province_en %in% list(\"Bangkok\", \"Nonthaburi\", \n                                           \"Nakhon Pathom\", \"Pathum Thani\", \n                                           \"Samut Prakan\", \"Samut Sakhon\")) %&gt;%\n  mutate(`incident_monthyear` = format(as.Date(`incident_datetime`), \"%Y/%m\")) %&gt;%\n  mutate(`incident_year` = format(as.Date(`incident_datetime`), \"%Y\")) %&gt;%\n  st_transform(crs = 32647)\n\n\nstr(accident_data_sf)\n\nsf [12,986 × 19] (S3: sf/tbl_df/tbl/data.frame)\n $ acc_code                   : num [1:12986] 571882 600001 605043 629691 571887 ...\n $ incident_datetime          : POSIXct[1:12986], format: \"2019-01-01 02:25:00\" \"2019-01-01 03:00:00\" ...\n $ report_datetime            : POSIXct[1:12986], format: \"2019-01-02 17:32:00\" \"2019-01-05 10:33:00\" ...\n $ province_th                : chr [1:12986] \"นครปฐม\" \"นนทบุรี\" \"สมุทรปราการ\" \"กรุงเทพมหานคร\" ...\n $ province_en                : chr [1:12986] \"Nakhon Pathom\" \"Nonthaburi\" \"Samut Prakan\" \"Bangkok\" ...\n $ agency                     : chr [1:12986] \"department of rural roads\" \"department of highways\" \"department of highways\" \"expressway authority of thailand\" ...\n $ route                      : chr [1:12986] \"แยกทางหลวงหมายเลข 4 (กม.ที่ 51+920) - บ้านวัดละมุด\" \"คลองวัดแดง - บางบัวทอง\" \"ราษฎร์บูรณะ - พระสมุทรเจดีย์\" \"บางพลี-สุขสวัสดิ์\" ...\n $ vehicle_type               : chr [1:12986] \"motorcycle\" \"private/passenger car\" \"private/passenger car\" \"other\" ...\n $ presumed_cause             : chr [1:12986] \"speeding\" \"speeding\" \"running red lights/traffic signals\" \"other\" ...\n $ accident_type              : chr [1:12986] \"rollover/fallen on straight road\" \"rollover/fallen on straight road\" \"collision at intersection corner\" \"other\" ...\n $ number_of_vehicles_involved: num [1:12986] 1 1 2 1 1 1 1 1 1 1 ...\n $ number_of_fatalities       : num [1:12986] 0 0 0 0 0 1 1 0 0 0 ...\n $ number_of_injuries         : num [1:12986] 2 1 0 1 1 0 0 0 0 1 ...\n $ weather_condition          : chr [1:12986] \"clear\" \"clear\" \"clear\" \"clear\" ...\n $ road_description           : chr [1:12986] \"straight road\" \"straight road\" \"other\" \"other\" ...\n $ slope_description          : chr [1:12986] \"no slope\" \"no slope\" \"other\" \"other\" ...\n $ geometry                   :sfc_POINT of length 12986; first list element:  'XY' num [1:2] 627012 1533381\n $ incident_monthyear         : chr [1:12986] \"2019/01\" \"2019/01\" \"2019/01\" \"2019/01\" ...\n $ incident_year              : chr [1:12986] \"2019\" \"2019\" \"2019\" \"2019\" ...\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:18] \"acc_code\" \"incident_datetime\" \"report_datetime\" \"province_th\" ...\n\n\nThis part is to import Thailand Roads (OpenStreetMap Export) and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create THR_sf\n\nTHR_sf &lt;- st_read(dsn = \"data/geo\", \n                         layer = \"hotosm_tha_roads_lines_shp\")\n\nReading layer `hotosm_tha_roads_lines_shp' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\Portfolio\\R-GAA\\Project01\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2792590 features and 14 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA\n\nTHR_sf &lt;-  st_set_crs(THR_sf, 32647)\n\nThis part is to import Thailand - Subnational Administrative Boundaries as well as filtering out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create THSAB_sf\n\nTHSAB_sf &lt;- st_read(dsn = \"data/geo\", \n                         layer = \"tha_admbnda_adm2_rtsd_20220121\") %&gt;%\n  filter(ADM1_EN %in% list(\"Bangkok\", \"Nonthaburi\", \"Nakhon Pathom\",\n                      \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\")) %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm2_rtsd_20220121' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\Portfolio\\R-GAA\\Project01\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 928 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#quick-analysis",
    "href": "R-GAA/Project01/Project01.html#quick-analysis",
    "title": "Project 1",
    "section": "4.1 Quick analysis",
    "text": "4.1 Quick analysis\nNow let’s take a quick look at temporal distribution of accidents data. The idea of this is to identify whether there is any identifiable temporal patterns in the distribution of accidents.\nTo to this I will be plotting a bar chart showing the counts of accidents months on end from 2019 to 2022.\n\nggplot(accident_data_sf) +\n  geom_bar(aes(x = incident_monthyear), \n                 bin = 100, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  xlab(\"Time\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above graph review that accident tends to peak around January April, July, October and December. Interestingly, these period seems to coincide with holidays seasons in Thailand.\n\n\nBut what about the year by year number of cases ?\n\nggplot(accident_data_sf) +\n  geom_bar(aes(x = incident_year),\n                 color=\"black\", \n                 fill=\"light blue\") +\n  xlab(\"Province\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above graph review that number of accidents are around the same over the year and does not seem to be getting better.\n\n\nBut how about the distribution of cases of the different region within the review region?\n\nggplot(accident_data_sf) +\n  geom_bar(aes(x = province_en),\n                 color=\"black\", \n                 fill=\"light blue\") +\n  xlab(\"Province\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above results shown that majority of accidents cases from 2019 to 2022 is actually from Bangkok province itself which does make sense since the concentration of population are probably in this region as well which leads to higher traffics\n\n\nThe graphs below are showcasing the Distribution of accidents in different scale from overall to year by year and to month by year\n\ntmap_mode('plot')\ntm_shape(THSAB_sf) +\n  tm_polygons() +\ntm_shape(accident_data_sf) +\n  tm_dots(size = 0.01, \"red\") +\ntm_layout(main.title = \"Distribution of accidents\")\n\n\n\n\n\n\n\n# tm_shape(THR_sf) +\n#   tm_lines()\n\n\ntm_shape(THSAB_sf) +\n  tm_polygons() +\ntm_shape(accident_data_sf) +\n  tm_dots(size = 0.01, \"red\") +\ntm_facets(by = \"incident_year\",\n          free.coords = FALSE,\n          free.scales = FALSE,\n          drop.units = TRUE) +\ntm_layout(main.title = \"Distribution of accidents by year\")\n\n\n\n\n\n\n\n\n\ntm_shape(THSAB_sf) +\n  tm_polygons() +\ntm_shape(accident_data_sf) +\n  tm_dots(size = 0.1, \"red\") +\ntm_facets(by = \"incident_monthyear\",\n          free.coords = FALSE,\n          free.scales = FALSE,\n          drop.units = TRUE,\n          ncol = 12,\n          nrow = 4)+\ntm_layout(main.title = \"Distribution of accidents by month by year\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe most comprehensive one are probably the month by year distribution which showcase the changes of distribution of accidents depending on the period and different seasons"
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#first-order-spatial-point-patterns-analysis",
    "href": "R-GAA/Project01/Project01.html#first-order-spatial-point-patterns-analysis",
    "title": "Project 1",
    "section": "4.2 First-order Spatial Point Patterns Analysis",
    "text": "4.2 First-order Spatial Point Patterns Analysis\nKernel Density Estimation\n\nGeospatial Data wrangling\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\naccident_data &lt;- as_Spatial(accident_data_sf)\nTHSAB &lt;- as_Spatial(THSAB_sf)\n\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\naccident_data_sp &lt;- as(accident_data, \"SpatialPoints\")\nTHSAB_sp &lt;- as(THSAB, \"SpatialPolygons\")\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\naccident_data_ppp &lt;- as.ppp(accident_data_sf)\nany(duplicated(accident_data_ppp))\n\n[1] FALSE\n\n\nSince the results return false there’s no duplicated points in the data\n\nplot(accident_data_ppp)\n\n\n\n\n\n\n\n\n\n\nCreating owin object\nThe code chunk below is used to covert the SpatialPolygon object into owin object of spatstat\n\nTHSAB_owin &lt;- as.owin(THSAB_sf)\n\n\n\nRescalling KDE values\nwe will extract childcare events that are located within the review region and re-scale the unit of measurement from meter to kilometer by using the code chunk below.\n\naccident_data_owin_ppp &lt;- accident_data_ppp[THSAB_owin]\naccident_data_owin_ppp.km &lt;- rescale.ppp(accident_data_owin_ppp, 1000, \"km\")\n\nNow, we can run density() using the resale data set and plot the output kde map. In this case I’ll be using bw.diggle() since it seems to be working best\n\nkde_accident_data_bw &lt;- density(accident_data_owin_ppp.km,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                              kernel=\"gaussian\") \nplot(kde_accident_data_bw, main = \"Fixed bandwidth\")\n\n\n\n\n\n\n\n\nTo compared the above method I’ll also be running another density using adaptive method\n\nkde_accident_data_adaptive &lt;- adaptive.density(accident_data_owin_ppp.km, method=\"kernel\")\nplot(kde_accident_data_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth graph produce roughly the same results which highlight some of the hot zone of accidents that could happen throughout the period. The KDE shown that there are hotspots in the roads between Bangkok - Samut Prakan, Bangkok - Pathum Thani and with a lower but still significant hotspot between Bangkok - Nakhon Pathom and Bangkok - Samut Sakhon\n\n\n\n\nTesting spatial point patterns using Clark and Evans Test\n\nclarkevans.test(accident_data_ppp,\n                correction=\"none\",\n                clipregion=\"THSAB_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accident_data_ppp\nR = 0.16207, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the test results, we rejected the null hypothesis that the point patterns are randomly distributed."
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#second-order-spatial-point-patterns-analysis",
    "href": "R-GAA/Project01/Project01.html#second-order-spatial-point-patterns-analysis",
    "title": "Project 1",
    "section": "4.3 Second-order Spatial Point Patterns Analysis",
    "text": "4.3 Second-order Spatial Point Patterns Analysis\n\nAnalysing Spatial Point Process Using G-Function\n\nG_CK = Gest(accident_data_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,2000))\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test Using G-Function\n\nG_CK.csr &lt;- envelope(accident_data_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the test results it seems likely that the null hypothesis is rejected and distribution of accidents at Bangkok Metropolitan Region BMR is not randomly distributed. The estimated G(r) lies above the upper envelope, the estimated G(r) is statistically significant\n\n\n\n\nAnalysing Spatial Point Process Using F-Function\n\nF_CK = Fest(accident_data_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test Using F-Function\n\nF_CK.csr &lt;- envelope(accident_data_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the test results it seems likely that the null hypothesis is rejected and distribution of accidents at Bangkok Metropolitan Region BMR is not randomly distributed"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html",
    "href": "R-GAA/Project03/Project03.html",
    "title": "Project 3",
    "section": "",
    "text": "In this project, I planned to calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023. For the purpose of this take-home exercise, HDB Resale Flat Prices provided by Data.gov.sg should be used as the core data set. The study should focus on either three-room, four-room or five-room flat.\nhttps://isss626-ay2024-25aug.netlify.app/take-home_ex03b\nThe below packages are used and loaded in using the p_load() function of pacman package\n\npacman::p_load(tidyverse, sf, httr, jsonlite, tmap, SpatialAcc, \n               spdep, GWmodel, SpatialML, rsample, Metrics, kableExtra,\n               knitr, ggstatsplot, spatstat, see, performance)"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#first-phase-of-data-preparation-and-wrangling",
    "href": "R-GAA/Project03/Project03.html#first-phase-of-data-preparation-and-wrangling",
    "title": "Project 3",
    "section": "3.1 First phase of data preparation and wrangling",
    "text": "3.1 First phase of data preparation and wrangling\n\nResale data\nFirst the resale data will be loaded into data call resale using the read_cvs()\n\nresale &lt;- read_csv(\"data/non-geo/resale.csv\") %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\nhead(resale)\n\n# A tibble: 6 × 11\n  month town  flat_type block street_name storey_range floor_area_sqm flat_model\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;     \n1 2023… ANG … 2 ROOM    406   ANG MO KIO… 01 TO 03                 44 Improved  \n2 2023… ANG … 2 ROOM    323   ANG MO KIO… 04 TO 06                 49 Improved  \n3 2023… ANG … 2 ROOM    314   ANG MO KIO… 04 TO 06                 44 Improved  \n4 2023… ANG … 2 ROOM    314   ANG MO KIO… 07 TO 09                 44 Improved  \n5 2023… ANG … 2 ROOM    170   ANG MO KIO… 01 TO 03                 45 Improved  \n6 2023… ANG … 3 ROOM    225   ANG MO KIO… 04 TO 06                 67 New Gener…\n# ℹ 3 more variables: lease_commence_date &lt;dbl&gt;, remaining_lease &lt;chr&gt;,\n#   resale_price &lt;dbl&gt;\n\n\nFirst look at the data we could see that there is a range of story under storey_range and remaining_lease are actully a string instead of numeric data that need to be convert to a better data for better modelling later. Other data such as floor_area_sqm and resale_price seems to be in appropriate\nQuickly checking unique data for storey_range\n\nunique(resale$storey_range)\n\n [1] \"01 TO 03\" \"04 TO 06\" \"07 TO 09\" \"25 TO 27\" \"10 TO 12\" \"13 TO 15\"\n [7] \"16 TO 18\" \"22 TO 24\" \"19 TO 21\" \"34 TO 36\" \"28 TO 30\" \"37 TO 39\"\n[13] \"31 TO 33\" \"40 TO 42\" \"43 TO 45\" \"46 TO 48\" \"49 TO 51\"\n\n\nThere is 17 unique data which I’ll convert to numeric from 1-17 separately..\nThe remaining_lease would go through being separate into remaining_lease_yr column and remaining_lease_mth separated then recalculate under remaining_lease_time with function remaining_lease_yr*12 + remaining_lease_mth.\nAll of the above steps would be done with the code below creating resale_tidy data\n\nresale_tidy &lt;- resale %&gt;%\n  mutate(address = paste(block,street_name)) %&gt;%\n  mutate(remaining_lease_yr = as.integer(\n    str_sub(remaining_lease, 0, 2)))%&gt;%\n  mutate(remaining_lease_mth = as.integer(\n    str_sub(remaining_lease, 9, 11))) %&gt;%\n  mutate_if(is.numeric , replace_na, replace = 0) %&gt;%\n  mutate(remaining_lease_time = remaining_lease_yr*12 + remaining_lease_mth) %&gt;%\n  mutate(storey_level =  case_when(\n    storey_range == \"01 TO 03\" ~ as.integer(1),\n    storey_range == \"04 TO 06\" ~ as.integer(2),\n    storey_range == \"07 TO 09\" ~ as.integer(3),\n    storey_range == \"10 TO 12\" ~ as.integer(4),\n    storey_range == \"13 TO 15\" ~ as.integer(5),\n    storey_range == \"16 TO 18\" ~ as.integer(6),\n    storey_range == \"19 TO 21\" ~ as.integer(7),\n    storey_range == \"22 TO 24\" ~ as.integer(8),\n    storey_range == \"25 TO 27\" ~ as.integer(9),\n    storey_range == \"28 TO 30\" ~ as.integer(10),\n    storey_range == \"31 TO 33\" ~ as.integer(11),\n    storey_range == \"34 TO 36\" ~ as.integer(12),\n    storey_range == \"37 TO 39\" ~ as.integer(13),\n    storey_range == \"40 TO 42\" ~ as.integer(14),\n    storey_range == \"43 TO 45\" ~ as.integer(15),\n    storey_range == \"46 TO 48\" ~ as.integer(16),\n    storey_range == \"49 TO 51\" ~ as.integer(17)))\n\nNow with the basic resale data sort, we would next need to find the geographical location of each of these units, to do this I would first need to get the list of address of these units, using the code chunk below\n\nadd_list &lt;- sort(unique(resale_tidy$address))\n\nThis function below is used to make and API call to onemap API to extract the coordinate of these units based on its address, this would later be used based on names of shopping malls as well which I will mention.\n\nget_coords &lt;- function(add_list){\n  \n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (i in add_list){\n    #print(i)\n\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each address\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(address= i, \n                            postal = postal, \n                            latitude = lat, \n                            longitude = lng)\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found &gt; 1){\n      # Remove those with NIL as postal\n      res_sub &lt;- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row &lt;- data.frame(address= i, \n                                postal = NA, \n                                latitude = NA, \n                                longitude = NA)\n      }\n      \n      else{\n        top1 &lt;- head(res_sub, n = 1)\n        postal &lt;- top1$POSTAL \n        lat &lt;- top1$LATITUDE\n        lng &lt;- top1$LONGITUDE\n        new_row &lt;- data.frame(address= i, \n                              postal = postal, \n                              latitude = lat, \n                              longitude = lng)\n      }\n    }\n\n    else {\n      new_row &lt;- data.frame(address= i, \n                            postal = NA, \n                            latitude = NA, \n                            longitude = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  return(postal_coords)\n}\n\nOnce the function is loaded and the unit address list is created, the below code chunk is run to get all the geo coordinates of these units\n\ncoords &lt;- get_coords(add_list)\n\nJust in case I will write these coords to a rds file for later usage.\n\nwrite_rds(coords, \"data/rds/coords.rds\")\n\n\ncoords &lt;- read_rds(\"data/rds/coords.rds\")\n\nThese coordinates is then joined back to the resale_tidy\n\nresale_tidy &lt;- resale_tidy %&gt;% \n  left_join(coords)\n\nSince the coords would appear as longitude and latitude which is not sf type and would be hard for later analysis, the code chunk below would use the st_as_sf() to convert it to a POINT geometric instead, then to make sure the crs is in correct format of 3414 I will also use st_transform(). This code chunk below would create resale_tidy_final data\n\nresale_tidy_final &lt;- resale_tidy %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nJust in case I will write this data to a rds file for later usage.\n\nwrite_rds(resale_tidy_final, \"data/rds/resale_tidy_final.rds\")\n\n\nresale_tidy_final &lt;- read_rds(\"data/rds/resale_tidy_final.rds\")\n\nNow, since the study would be focusing on either 3 rooms, 4 rooms or 5 rooms units using 2023 data to predict July-September 2024 data. In this case I would be focusing on 3 rooms analysis and did one more wrangling to filter data to only include 2023 - Sep 2024 and for 3 room units.\nThe code chunk below is for this wrangling process\n\nresale_final &lt;- resale_tidy_final %&gt;%\n  filter(flat_type == '3 ROOM') %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\n\nCBD data\nCBD data is using the Master Plan 2014 Subzone Boundary (Web) which I would load and then filter out only the CBD region which includes ‘DOWNTOWN CORE’, ‘MARINA EAST’, ‘MARINA SOUTH’, ‘MUSEUM’, ‘NEWTON’, ‘ORCHARD’, ‘OUTRAM’, ‘RIVER VALLEY’, ‘ROCHOR’, ‘SINGAPORE RIVER’, ‘STRAITS VIEW’, st_transform() would also be used just in case in the code chunk below creating the CBD data\n\nCBD &lt;- st_read(dsn = \"data/geo\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %&gt;%\n  filter(PLN_AREA_N %in% c('DOWNTOWN CORE', 'MARINA EAST', 'MARINA SOUTH',\n                           'MUSEUM', 'NEWTON', 'ORCHARD', 'OUTRAM',\n                           'RIVER VALLEY', 'ROCHOR', 'SINGAPORE RIVER',\n                           'STRAITS VIEW'))%&gt;%\n  st_transform(crs = 3414)\n\n\n\nMall list\nSince the data is extracted from Wikipedia and only include the mall names, I would need to somehow get the coordinates for these malls. But first the mall list is loaded in creating mall_list\n\nmall_list &lt;- read_csv(\"data/non-geo/mall_list.csv\")\n\nSimilarly to the resale data I wil once again get the unique list of name instead of address this time with the code chunk below\n\nmall_name &lt;- sort(unique(mall_list$name))\n\nThen this name list would be feed into the get_coords() function creating a new list of coordinations that has the name of the malls and its coords as longitude and latitude which is not sf type and would be hard for later analysis, the code chunk below would use the st_as_sf() to convert it to a POINT geometric instead and st_transform() used to make sure the crs is in correct format. All of this would be done in the code chunk below\n\nmall_list_coords &lt;- get_coords(mall_name) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nJust in case I will write this data to a rds file for later usage.\n\nwrite_rds(mall_list_coords, \"data/rds/mall_list_coords.rds\")\n\n\nmall &lt;- read_rds(\"data/rds/mall_list_coords.rds\")\n\n\n\nPrimary school list\nFirstly since the data Generalinformationofschools.csv file include the list of all schools I would need to extract data to get the necessary information such as name and address. This is done using the code chunk below\n\nschool_list &lt;- read_csv(\"data/non-geo/Generalinformationofschools.csv\") %&gt;%\n  filter(mainlevel_code == 'PRIMARY') %&gt;%\n  select(1,3)\n\nNext similarly to the resale data or the mall data this address list would be feed into the get_coords() function creating a new list of coordinations as longitude and latitude which is not sf type and would be hard for later analysis, the code chunk below would use the st_as_sf() to convert it to a POINT geometric instead and st_transform() used to make sure the crs is in correct format. All of this would be done in the code chunk below\n\nschool_list_address &lt;- sort(unique(school_list$address))\nschool_list_coords &lt;- get_coords(school_list_address) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nJust in case I will also write this data to a rds file for later usage.\n\nwrite_rds(school_list_coords, \"data/rds/school_list_coords.rds\")\n\n\nprimary_school &lt;- read_rds(\"data/rds/school_list_coords.rds\")\n\n\n\nMRT\nSince MRT data is already in geographical points I just need to load it in using the code chunk below\n\nMRT &lt;- st_read(dsn = \"data/geo/LTAMRTStationExitGEOJSON.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nPreschool location\nPreschool data is already in geographical points I just need to load it in using the code chunk below\n\npreschoolslocation &lt;- st_read(\"data/geo/PreSchoolsLocation.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nKindergartens\nKindergartens data is already in geographical points I just need to load it in using the code chunk below\n\nkindergartens &lt;- st_read(dsn = \"data/geo/Kindergartens.kml\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nSupermarkets\nSupermarkets data is already in geographical points I just need to load it in using the code chunk below\n\nsupermarkets &lt;- st_read(dsn = \"data/geo/SupermarketsKML.kml\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nEldercare center\nElder care center data is already in geographical points I just need to load it in using the code chunk below\n\neldercare &lt;- st_read(dsn = \"data/geo\",\n                     layer = \"ELDERCARE\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nChildcare center\nChildcare center data is already in geographical points I just need to load it in using the code chunk below\n\nchildcare &lt;- st_read(dsn = \"data/geo\",\n                     layer = \"CHILDCARE\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nBus Stops\nBus Stops data is already in geographical points I just need to load it in using the code chunk below. However I do notice during analysis that some of the Bus stops especially ‘46239’, ‘46609’, ‘47701’, ‘46211’, ‘46219’ are located outside of Singapore hence I would remove them from this analysis\n\nbusstop &lt;- st_read(dsn = \"data/geo\",\n                     layer = \"BusStop\") %&gt;%\n  filter(!BUS_STOP_N %in% c('46239','46609','47701','46211','46219')) %&gt;%\n  st_transform(crs = 3414)\n\n\n\nCHAS clinics\nCHAS clinics data is already in geographical points I just need to load it in using the code chunk below. However I do notice during analysis that one of the clinic ‘kml_271’ is somehow located outside of Singapore hence I would remove them from this analysis\n\nCHAS &lt;- st_read(dsn = \"data/geo/CHASClinics.kml\") %&gt;%\n  filter(Name != 'kml_271')%&gt;%\n  st_transform(crs = 3414)\n\n\n\nMarket and foodcentres\nMarket and foodcentres data is already in geographical points I just need to load it in using the code chunk below\n\nmarket_foodcentre &lt;- st_read(dsn = \"data/geo/NEAMarketandFoodCentre.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nParks\nParks data is already in geographical points I just need to load it in using the code chunk below\n\npark &lt;- st_read(dsn = \"data/geo/ParkFacilitiesGEOJSON.geojson\") %&gt;%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#second-phase-of-data-preparation-and-wrangling",
    "href": "R-GAA/Project03/Project03.html#second-phase-of-data-preparation-and-wrangling",
    "title": "Project 3",
    "section": "3.2 Second phase of data preparation and wrangling",
    "text": "3.2 Second phase of data preparation and wrangling\nOnce all the data are loaded in I will move on to the next step of calculating the geographical proximity and the number of facilities within a radius of HDB units.\nFirst I will create 2 buffer zone data for these unit at 1000 m or 1 km and 350 m separately. The code chunk below will be for this purpose\n\nbuffer_1km_HDB  &lt;- st_buffer(resale_final,\n                             dist = 1000)\n\n\nbuffer_350m_HDB  &lt;- st_buffer(resale_final,\n                             dist = 350)\n\nOnce the buffer zones are created, new columns are created for the resale_final and they each represent the number of facilities within a radius of HDB units either 350 m or 1 km. The function to calculate this number would be based on the lengths(st_intersects(bufferzone, facility)).\n\nresale_final$within_350m_kindergartens &lt;- lengths(st_intersects(buffer_350m_HDB, kindergartens))\nresale_final$within_350m_childcare &lt;- lengths(st_intersects(buffer_350m_HDB, childcare))\nresale_final$within_350mm_busstop &lt;- lengths(st_intersects(buffer_350m_HDB, busstop))\nresale_final$within_350mm_preschoolslocation &lt;- lengths(st_intersects(buffer_350m_HDB, preschoolslocation))\nresale_final$within_1km_chas &lt;- lengths(st_intersects(buffer_1km_HDB, CHAS))\nresale_final$within_1km_primary_school &lt;- lengths(st_intersects(buffer_1km_HDB, primary_school))\n\nNext new columns are created for the resale_final and they each represent the minimum distance from a unit to another region (CBD) or to another facility. This calculation is based on the min(st_distance(HDB, location)))/1000 or is in kilometer shortest distance\n\nresale_final &lt;- resale_final %&gt;%\n  rowwise() %&gt;%\n  mutate(prox_CBD =  as.numeric(min(st_distance(geometry, CBD)))/1000) %&gt;%\n  mutate(prox_eldercare =  as.numeric(min(st_distance(geometry, eldercare)))/1000) %&gt;%\n  mutate(prox_market_foodcentre =  as.numeric(min(st_distance(geometry, market_foodcentre)))/1000) %&gt;%\n  mutate(prox_MRT =  as.numeric(min(st_distance(geometry, MRT)))/1000) %&gt;%\n  mutate(prox_park =  as.numeric(min(st_distance(geometry, park)))/1000) %&gt;%\n  mutate(prox_mall =  as.numeric(min(st_distance(geometry, mall)))/1000) %&gt;%\n  mutate(prox_supermarkets =  as.numeric(min(st_distance(geometry, supermarkets)))/1000)\n\nOnce all this caluclation is done I will write this data to a rds file for later usage.\n\nwrite_rds(resale_final, \"data/rds/resale_final.rds\")"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#third-phase-of-data-preparation-and-wrangling",
    "href": "R-GAA/Project03/Project03.html#third-phase-of-data-preparation-and-wrangling",
    "title": "Project 3",
    "section": "3.3 Third phase of data preparation and wrangling",
    "text": "3.3 Third phase of data preparation and wrangling\nThis will be the final phase to get all the dat needed for the analysis\nFirstly, I will be selecting only the columns that is needed for the analysis using the code chunk below\n\nresale_final &lt;- read_rds(\"data/rds/resale_final.rds\") %&gt;%\n  select(month, resale_price, floor_area_sqm, storey_level, remaining_lease_time,\n         prox_CBD, prox_eldercare, prox_market_foodcentre, prox_MRT,\n         prox_park, prox_mall, prox_supermarkets, within_350m_kindergartens,\n         within_350m_childcare, within_350mm_busstop, \n         within_350mm_preschoolslocation, within_1km_chas, \n         within_1km_primary_school)\n\nNext I will check for the duplicated point using the sum of multiplicity or sum(multiplicity()), multiplicity() is part of spatstat package to count the number of duplicates for each point in a spatial point pattern\n\nsum(multiplicity(resale_final) &gt; 1)\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code would return a results of 12 duplicated units points, this has been cut off from running since it taking a long time to run. This indicates that there are units that could be in the same building block or very unlikely, sold multiple time during the study period.\n\n\nTo resolve this issue I will be using st_jitter() to which techincally moving points within a short distance to address overlapping points issue. In this case I will move them within a 5 meter radius. The code chunk below is used to do this.\n\nresale_final &lt;- st_jitter(resale_final, amount = 5)\n\nOnce this is done we could no longer see any duplicate point by rerunning the previous code/\n\nsum(multiplicity(resale_final) &gt; 1)\n\n[1] 0\n\n\nNow, since the task specifically specify that I would be using HDB resale transaction records in 2023 to predict HDB resale prices between July-September 2024. I will split them into 2 part call resale_main and resale_check filtered by the specific period.\n\nresale_main &lt;- resale_final %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2023-12\")\n\nresale_check &lt;- resale_final %&gt;%\n  filter(month &gt;= \"2024-07\" & month &lt;= \"2024-09\")\n\nNext, they would be turn into the data that would be used for training and data for testing and prediction specifically call train_data and test_data. The code chunk below will be doing the above and I will write this data to a rds file for later usage.\n\nset.seed(1234)\n\ntrain_data &lt;- resale_main\ncoords_train &lt;- st_coordinates(resale_main)\n\ntrain_data &lt;- write_rds(train_data, \"data/rds/train_data.rds\")\ncoords_train &lt;- write_rds(coords_train, \"data/rds/coords_train.rds\" )\n\ntest_data &lt;- resale_check\ncoords_test &lt;- st_coordinates(resale_check)\n\ntest_data &lt;- write_rds(test_data, \"data/rds/test_data.rds\")\ncoords_test &lt;- write_rds(coords_test, \"data/rds/coords_test.rds\" )\n\nNext I would want to check how is the data is doing and see if there was any issue with Collinearity. To do this I would first create a new data set without its geometry using the code chunk below\n\nresale_main_nogeo &lt;- resale_main %&gt;%\n  st_drop_geometry()\n\nThis data would then be checked for Collinearity using the corrplot() of corrplot package in the code chunk below\n\ncorrplot::corrplot(cor(resale_main_nogeo[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince none of the correlation is higher/lower than +- 0.7, I will be keeping all the variables for this study\n\n\n\ntrain_data &lt;- read_rds(\"data/rds/train_data.rds\")\ntest_data &lt;- read_rds(\"data/rds/test_data.rds\")\ncoords_train &lt;- read_rds(\"data/rds/coords_train.rds\" )\ncoords_test &lt;- read_rds(\"data/rds/coords_test.rds\" )"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#non-spatial-multiple-linear-regression",
    "href": "R-GAA/Project03/Project03.html#non-spatial-multiple-linear-regression",
    "title": "Project 3",
    "section": "4.1 Non-spatial multiple linear regression",
    "text": "4.1 Non-spatial multiple linear regression\nThe code chunk below will build the multiple linear regression using the lm() of stats package to fit linear multivariate models, all the previously mentioned predictors and variables are included to build this model. Then we would use the ols_regress() of olsrr to perform the Ordinary least squares regression\n\nset.seed(1234)\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm + storey_level + \n                  remaining_lease_time + prox_CBD + prox_eldercare + \n                  prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                  prox_supermarkets + within_350m_kindergartens +\n                  within_350m_childcare + within_350mm_busstop + \n                  within_350mm_preschoolslocation + within_1km_chas +\n                  within_1km_primary_school,\n                data=train_data)\nolsrr::ols_regress(price_mlr)\n\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.875       RMSE                    42719.098 \nR-Squared                   0.766       MSE                1829816217.009 \nAdj. R-Squared              0.766       Coef. Var                  10.386 \nPred R-Squared              0.764       AIC                    153589.830 \nMAE                     31029.976       SBC                    153711.456 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                      \n-------------------------------------------------------------------------------\n                    Sum of                                                     \n                   Squares          DF       Mean Square       F          Sig. \n-------------------------------------------------------------------------------\nRegression    3.800424e+13          16      2.375265e+12    1298.089    0.0000 \nResidual      1.159738e+13        6338    1829816217.009                       \nTotal         4.960161e+13        6354                                         \n-------------------------------------------------------------------------------\n\n                                                    Parameter Estimates                                                      \n----------------------------------------------------------------------------------------------------------------------------\n                          model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n----------------------------------------------------------------------------------------------------------------------------\n                    (Intercept)    -159037.708      7560.767                 -21.035    0.000    -173859.370    -144216.045 \n                 floor_area_sqm       5268.250        97.413        0.338     54.082    0.000       5077.287       5459.213 \n                   storey_level       9668.810       325.058        0.200     29.745    0.000       9031.587      10306.033 \n           remaining_lease_time        373.707         3.791        0.849     98.581    0.000        366.276        381.139 \n                       prox_CBD      -9842.643       183.872       -0.457    -53.530    0.000     -10203.095      -9482.191 \n                 prox_eldercare       3837.286      1118.404        0.024      3.431    0.001       1644.835       6029.736 \n         prox_market_foodcentre     -15096.642       603.961       -0.235    -24.996    0.000     -16280.609     -13912.674 \n                       prox_MRT     -30707.664      1653.726       -0.126    -18.569    0.000     -33949.527     -27465.801 \n                      prox_park     -17357.348      2233.627       -0.050     -7.771    0.000     -21736.013     -12978.683 \n                      prox_mall     -16171.291      1665.034       -0.066     -9.712    0.000     -19435.321     -12907.260 \n              prox_supermarkets      25146.697      3034.004        0.055      8.288    0.000      19199.023      31094.371 \n      within_350m_kindergartens       5560.911       897.766        0.056      6.194    0.000       3800.986       7320.837 \n          within_350m_childcare        959.410       217.318        0.032      4.415    0.000        533.392       1385.427 \n           within_350mm_busstop        572.749       213.877        0.017      2.678    0.007        153.477        992.020 \nwithin_350mm_preschoolslocation      -2078.563       355.906       -0.059     -5.840    0.000      -2776.259      -1380.866 \n                within_1km_chas       -358.466        88.349       -0.033     -4.057    0.000       -531.660       -185.272 \n      within_1km_primary_school       4499.554       496.240        0.071      9.067    0.000       3526.756       5472.353 \n----------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe model has an Adj. R-Squared of 0.766 which is not bad but also not great and I believe we could do better than this, ANOVA results show that the differences between variables are statistically significant and are unlikely to be due to chance\nIts AIC (Akaike Information Criteria) 153589.830 would be kept in mind for later comparison of other models\n\n\nNext we will calculating the variance inflation factor (VIF) using the check_collinearity() of the performance package, and then explore its results within the table created by the kable() from either kableExtra or knitr package\n\nvif &lt;- check_collinearity(price_mlr)\nkable(vif, \n      caption = \"Variance Inflation Factor (VIF) Results\") %&gt;%\n  kable_styling(font_size = 18) \n\n\nVariance Inflation Factor (VIF) Results\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nfloor_area_sqm\n1.057888\n1.036092\n1.092846\n1.028537\n0.9452798\n0.9150417\n0.9651653\n\n\nstorey_level\n1.219417\n1.186839\n1.257676\n1.104272\n0.8200639\n0.7951171\n0.8425746\n\n\nremaining_lease_time\n2.012568\n1.941069\n2.089498\n1.418650\n0.4968776\n0.4785837\n0.5151799\n\n\nprox_CBD\n1.978201\n1.908356\n2.053416\n1.406485\n0.5055098\n0.4869934\n0.5240111\n\n\nprox_eldercare\n1.314164\n1.276684\n1.356721\n1.146370\n0.7609401\n0.7370710\n0.7832795\n\n\nprox_market_foodcentre\n2.400750\n2.310604\n2.497097\n1.549435\n0.4165364\n0.4004650\n0.4327872\n\n\nprox_MRT\n1.255822\n1.221328\n1.295691\n1.120634\n0.7962914\n0.7717892\n0.8187806\n\n\nprox_park\n1.102899\n1.077209\n1.137135\n1.050190\n0.9067016\n0.8794028\n0.9283247\n\n\nprox_mall\n1.260166\n1.225447\n1.300231\n1.122571\n0.7935462\n0.7690939\n0.8160285\n\n\nprox_supermarkets\n1.198851\n1.167381\n1.236238\n1.094920\n0.8341322\n0.8089060\n0.8566185\n\n\nwithin_350m_kindergartens\n2.210474\n2.129463\n2.297297\n1.486766\n0.4523916\n0.4352942\n0.4696020\n\n\nwithin_350m_childcare\n1.442154\n1.398298\n1.490838\n1.200897\n0.6934074\n0.6707638\n0.7151550\n\n\nwithin_350mm_busstop\n1.106823\n1.080858\n1.141125\n1.052056\n0.9034869\n0.8763283\n0.9251906\n\n\nwithin_350mm_preschoolslocation\n2.740045\n2.633628\n2.853395\n1.655308\n0.3649575\n0.3504598\n0.3797043\n\n\nwithin_1km_chas\n1.783973\n1.723491\n1.849511\n1.335655\n0.5605466\n0.5406835\n0.5802176\n\n\nwithin_1km_primary_school\n1.681112\n1.625605\n1.741544\n1.296577\n0.5948444\n0.5742032\n0.6151557\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVIF itself is below 5 and tolerance is within the 0.25 to 4 hence indicating that there is unlikely any issue with multicollinearity with this regression model\n\n\nThe plot below is to better visualize the VIF\n\nplot(vif) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#geographically-weighted-regression-with-gwr-method",
    "href": "R-GAA/Project03/Project03.html#geographically-weighted-regression-with-gwr-method",
    "title": "Project 3",
    "section": "4.2 Geographically Weighted Regression with gwr method",
    "text": "4.2 Geographically Weighted Regression with gwr method\nNext I would try out the Geographically Weighted Regression or gwr method using GWmodel package, however just running the model and then calibrating them would take a lot of time hence I would be calibrating the model at the same time.\nThe first step is to calibrate the model by calculating the adaptive bandwidth based on the training data or train_data. To do this bw.gwr() would be used in the code chunk below creating the bw_adaptive\n\nset.seed(1234)\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm + storey_level + \n                        remaining_lease_time + prox_CBD + prox_eldercare + \n                        prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                        prox_supermarkets + within_350m_kindergartens +\n                        within_350m_childcare + within_350mm_busstop + \n                        within_350mm_preschoolslocation + within_1km_chas +\n                        within_1km_primary_school,\n                      data=train_data,\n                      approach=\"CV\",\n                      kernel=\"gaussian\",\n                      adaptive=TRUE,\n                      longlat=FALSE)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(bw_adaptive, \"data/rds/bw_adaptive.rds\")\n\n\nbw_adaptive &lt;- read_rds(\"data/rds/bw_adaptive.rds\")\nbw_adaptive\n\n[1] 38\n\n\nThe results is that the calibrated adaptive bandwidth for the model should be 38\nThe next step is\n\nset.seed(1234)\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~ floor_area_sqm + storey_level + \n                            remaining_lease_time + prox_CBD + prox_eldercare + \n                            prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                            within_350m_childcare + within_350mm_busstop + \n                            within_350mm_preschoolslocation + within_1km_chas +\n                            within_1km_primary_school,\n                          data=train_data,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(gwr_adaptive, \"data/rds/gwr_adaptive.rds\")\n\n\ngwr_adaptive &lt;- read_rds(\"data/rds/gwr_adaptive.rds\")\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-09 21:02:57.55229 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + storey_level + \n    remaining_lease_time + prox_CBD + prox_eldercare + prox_market_foodcentre + \n    prox_MRT + prox_park + prox_mall + within_350m_childcare + \n    within_350mm_busstop + within_350mm_preschoolslocation + \n    within_1km_chas + within_1km_primary_school, data = train_data, \n    bw = bw_adaptive, kernel = \"gaussian\", adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_level remaining_lease_time prox_CBD prox_eldercare prox_market_foodcentre prox_MRT prox_park prox_mall within_350m_childcare within_350mm_busstop within_350mm_preschoolslocation within_1km_chas within_1km_primary_school\n   Number of data points: 6355\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-345595  -26786   -3503   21956  535104 \n\n   Coefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)                     -1.510e+05  7.585e+03 -19.904  &lt; 2e-16 ***\n   floor_area_sqm                   5.279e+03  9.815e+01  53.785  &lt; 2e-16 ***\n   storey_level                     9.902e+03  3.270e+02  30.278  &lt; 2e-16 ***\n   remaining_lease_time             3.674e+02  3.765e+00  97.587  &lt; 2e-16 ***\n   prox_CBD                        -9.436e+03  1.805e+02 -52.279  &lt; 2e-16 ***\n   prox_eldercare                   4.321e+03  1.127e+03   3.835 0.000127 ***\n   prox_market_foodcentre          -1.617e+04  5.960e+02 -27.126  &lt; 2e-16 ***\n   prox_MRT                        -2.974e+04  1.663e+03 -17.888  &lt; 2e-16 ***\n   prox_park                       -1.602e+04  2.242e+03  -7.145 9.97e-13 ***\n   prox_mall                       -1.416e+04  1.664e+03  -8.507  &lt; 2e-16 ***\n   within_350m_childcare            8.335e+02  2.177e+02   3.830 0.000130 ***\n   within_350mm_busstop             5.856e+02  2.157e+02   2.715 0.006637 ** \n   within_350mm_preschoolslocation -8.841e+02  2.610e+02  -3.387 0.000711 ***\n   within_1km_chas                 -4.614e+02  8.846e+01  -5.216 1.89e-07 ***\n   within_1km_primary_school        4.307e+03  4.971e+02   8.664  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 43140 on 6340 degrees of freedom\n   Multiple R-squared: 0.7622\n   Adjusted R-squared: 0.7616 \n   F-statistic:  1451 on 14 and 6340 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 1.17974e+13\n   Sigma(hat): 43092.71\n   AIC:  153694.5\n   AICc:  153694.6\n   BIC:  147587.7\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 38 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                          Min.     1st Qu.      Median\n   Intercept                       -4.0054e+07 -2.9240e+05 -7.0469e+04\n   floor_area_sqm                  -5.9340e+04  2.5181e+03  3.3501e+03\n   storey_level                    -1.5804e+03  5.6834e+03  7.5921e+03\n   remaining_lease_time            -1.9320e+03  1.8958e+02  2.9800e+02\n   prox_CBD                        -1.3404e+06 -2.7368e+04 -5.5153e+03\n   prox_eldercare                  -7.1923e+05 -5.2017e+03  1.2601e+04\n   prox_market_foodcentre          -4.4488e+06 -2.4613e+04  3.9812e+03\n   prox_MRT                        -1.1081e+06 -7.2842e+04 -3.3150e+04\n   prox_park                       -5.8831e+05 -3.0059e+04  2.7048e+02\n   prox_mall                       -1.8786e+06 -4.2443e+04 -6.8576e+03\n   within_350m_childcare           -5.3707e+04 -2.7396e+03 -1.3046e+02\n   within_350mm_busstop            -1.9138e+04 -1.1816e+03  1.9434e+02\n   within_350mm_preschoolslocation -3.4975e+04 -2.7861e+03 -2.1528e+02\n   within_1km_chas                 -1.2560e+04 -1.2234e+03  1.3256e+02\n   within_1km_primary_school       -3.5193e+05 -7.9389e+03 -1.0362e+02\n                                       3rd Qu.       Max.\n   Intercept                        1.8794e+05 6319227.82\n   floor_area_sqm                   4.4999e+03  127881.24\n   storey_level                     9.2998e+03   19478.85\n   remaining_lease_time             4.3956e+02     794.34\n   prox_CBD                         1.8830e+04 5131948.02\n   prox_eldercare                   4.3843e+04  877257.18\n   prox_market_foodcentre           3.7015e+04  625951.00\n   prox_MRT                        -6.2888e+02  901122.86\n   prox_park                        2.7799e+04 1112226.80\n   prox_mall                        2.2010e+04  666309.80\n   within_350m_childcare            2.7012e+03   31247.77\n   within_350mm_busstop             1.6702e+03   13729.73\n   within_350mm_preschoolslocation  1.7522e+03   29700.89\n   within_1km_chas                  1.7448e+03   27523.38\n   within_1km_primary_school        4.9770e+03  456192.00\n   ************************Diagnostic information*************************\n   Number of data points: 6355 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1168.128 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 5186.872 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 146751.4 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 145452.5 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 146518.1 \n   Residual sum of squares: 2.788363e+12 \n   R-square value:  0.9437848 \n   Adjusted R-square value:  0.9311223 \n\n   ***********************************************************************\n   Program stops at: 2024-11-09 21:03:19.81276 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInterestingly this gwr.basic() also include the results of another linear regression hence we could quickly compare this result of the GWR model with the previously ran linear model. In this case the Adjusted R-square value of the GWR model is 0.9311223 sinificantly better than 0.76 of the Multilinear regression model. In addition to this its AIC is also at 146751.4 lower than 153694.5 in this model ore the previously recorded 153589.830.\nOverall this Geographically Weighted Regression model seems to perform significantly better than the Multilinear regression model\n\n\nNext I’ll calculate the calibration bandwidth for the testing data or test_data\n\nset.seed(1234)\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm + storey_level + \n                                 remaining_lease_time + prox_CBD + prox_eldercare + \n                                 prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                                 prox_supermarkets + within_350m_kindergartens +\n                                 within_350m_childcare + within_350mm_busstop +\n                                 within_350mm_preschoolslocation + within_1km_chas +\n                                 within_1km_primary_school,\n                               data=test_data,\n                               approach=\"CV\",\n                               kernel=\"gaussian\",\n                               adaptive=TRUE,\n                               longlat=FALSE)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(gwr_bw_test_adaptive, \"data/rds/gwr_bw_test_adaptive.rds\")\n\n\ngwr_bw_test_adaptive &lt;- read_rds(\"data/rds/gwr_bw_test_adaptive.rds\")\ngwr_bw_test_adaptive\n\n[1] 287\n\n\nThe results is that the calibrated adaptive bandwidth for the test data should be 38\nNext I would attempt to predict the data based on the train_data and test_data to have something for comparison\n\nset.seed(1234)\ngwr_pred &lt;- gwr.predict(resale_price ~ floor_area_sqm + storey_level +\n                          remaining_lease_time + prox_CBD + prox_eldercare +\n                          prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                          prox_supermarkets + within_350m_kindergartens +\n                          within_350m_childcare + within_350mm_busstop +\n                          within_350mm_preschoolslocation + within_1km_chas +\n                          within_1km_primary_school,\n                        data=train_data,\n                        predictdata = test_data,\n                        bw=287,\n                        kernel = 'gaussian',\n                        adaptive=TRUE,\n                        longlat = FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nUnfortunately, I was not able to overcame the “no regression point is fixed” error for this gwr.predict() function and unable to showcase them here"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#geographically-weighted-random-forest-method-of-spatialml-package",
    "href": "R-GAA/Project03/Project03.html#geographically-weighted-random-forest-method-of-spatialml-package",
    "title": "Project 3",
    "section": "4.3 Geographically Weighted Random Forest method of SpatialML package",
    "text": "4.3 Geographically Weighted Random Forest method of SpatialML package\nFirst let drop the geometry column of training data sets\n\ntrain_data &lt;- train_data %&gt;% \n  st_drop_geometry()\n\nThen we would run an inital Random Forest model with default setting to check how well the model would turn out using the ranger() of ranger package. In addition, I will reduce the number of tree down to 53 instead of 500 since 500 trees would take a lot of time to run for later calibration\n\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_level + \n               remaining_lease_time + prox_CBD + prox_eldercare + \n               prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n               prox_supermarkets + within_350m_kindergartens +\n               within_350m_childcare + within_350mm_busstop + \n               within_350mm_preschoolslocation + within_1km_chas +\n               within_1km_primary_school,\n             num.trees = 53,\n             mtry = 5,\n             importance = \"impurity\",\n             data=train_data)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(rf, \"data/rds/rf.rds\")\n\n\nrf &lt;- read_rds(\"data/rds/rf.rds\")\nrf\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_level + remaining_lease_time +      prox_CBD + prox_eldercare + prox_market_foodcentre + prox_MRT +      prox_park + prox_mall + prox_supermarkets + within_350m_kindergartens +      within_350m_childcare + within_350mm_busstop + within_350mm_preschoolslocation +      within_1km_chas + within_1km_primary_school, num.trees = 53,      mtry = 5, importance = \"impurity\", data = train_data) \n\nType:                             Regression \nNumber of trees:                  53 \nSample size:                      6355 \nNumber of independent variables:  16 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       633848167 \nR squared (OOB):                  0.9188036 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis model return a OOB prediction error (MSE) of 636284892 and R squared (OOB) of 0.9184915, based on R squared alone this is a pretty good model.\n\n\nNext I will attempt to calibrate this model using the bw_adaptive of 38 calculated previously and using the grf() of SpacialML package. This would fit a local version of the Random Forest algorithm, accounting for spatial non-stationarity\nThe code chunk below show the code for this.\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_level + \n                       remaining_lease_time + prox_CBD + prox_eldercare + \n                       prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                       prox_supermarkets + within_350m_kindergartens +\n                       within_350m_childcare + within_350mm_busstop + \n                       within_350mm_preschoolslocation + within_1km_chas +\n                       within_1km_primary_school,\n                     dframe=train_data,\n                     ntree = 53,\n                     bw=38,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(gwRF_adaptive, \"data/rds/gwRF_adaptive.rds\")\n\n\ngwRF_adaptive &lt;- read_rds(\"data/rds/gwRF_adaptive.rds\")\n\n\n\ngwRF_adaptive$LocalModelSummary\n\n$l.VariableImportance\n                                      Min          Max        Mean         StD\nfloor_area_sqm                          0 1.444374e+12 12036908744 38731166250\nstorey_level                     44363034 2.112563e+11  6706363630 11869802696\nremaining_lease_time            312420137 4.938088e+11 19019149378 43649567043\nprox_CBD                                0 3.672406e+11  6845136742 20900812241\nprox_eldercare                    8062395 1.036893e+12  7365166244 30251391477\nprox_market_foodcentre            1815629 4.008882e+11  5927098746 17506021731\nprox_MRT                          1871799 9.538612e+11  7094318007 27526763708\nprox_park                         3383980 1.442077e+12  6911451236 30460063518\nprox_mall                         5138662 6.983204e+11  6454574985 26122120904\nprox_supermarkets                       0 1.749969e+12  7368199448 34438841202\nwithin_350m_kindergartens               0 1.154855e+11   987896884  5095969294\nwithin_350m_childcare                   0 1.871588e+11  2809696103 10499177446\nwithin_350mm_busstop                    0 3.120124e+11  3792415215 14101062371\nwithin_350mm_preschoolslocation         0 2.646240e+11  2349973922  7794490999\nwithin_1km_chas                         0 2.410287e+11  4166481778 14685127753\nwithin_1km_primary_school               0 3.213113e+11  2408800082 15656650488\n\n$l.MSE.OOB\n[1] 858973085\n\n$l.r.OOB\n[1] 0.8899476\n\n$l.MSE.Pred\n[1] 82225526\n\n$l.r.Pred\n[1] 0.9894652\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe results show both Global ML and Local model with R-squared (Not OOB) at 97.845 and R-squared predicted (Not OOB) at 98.947 respectively, which is quite good. In addition to this all AIC metrics (Not OOB) is at 120401.135 and and 115853.725. This seems to indicate that the local of calibrated version of the Geographically Weighted Random Forest seems to be a better model compared to the model ran by default.\nOverall, the Machine Learning method of Geographically Weighted Random Forest is performing the best compared to the Geographically Weighted Regression or the Non-spatial multiple linear regression\nThe overall trade of is time to run a actually calibrate the model could take a while, for my calibrated Geographically Weighted Random Forest it tooks around 4 hours to finish. Geographically Weighted Regression or the Non-spatial multiple linear regression are performing much better in this regards.\n\n\nNow that the model is inplace, I’ll try to actually predict the data for the July to Sep 2024. Since this the best model, it would be best to use this model to predict data itself to show case its accuracy.\nFirst step is to drop the geometry column of test data sets test_data\n\ntest_data &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\nThen the data would be predicted using the predict.grf() method of SpatialML package\n\nset.seed(1234)\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                         test_data, \n                         x.var.name=\"X\",\n                         y.var.name=\"Y\", \n                         local.w=1,\n                         global.w=0)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nGRF_pred &lt;- write_rds(gwRF_pred, \"data/rds/GRF_pred.rds\")\n\nNow with the newly created prediction data, they would be mapped to the original test_data using cbind() of base R code\n\nGRF_pred &lt;- read_rds(\"data/rds/GRF_pred.rds\")\nGRF_pred_df &lt;- as.data.frame(GRF_pred)\ntest_data_p &lt;- cbind(test_data, GRF_pred_df)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(test_data_p, \"data/rds/test_data_p.rds\")\n\n\ntest_data_p &lt;- read_rds(\"data/rds/test_data_p.rds\")\n\nLet’s quickly check the root Root Mean Squared Error between the actual resale price and the predicted data\n\nMetrics::rmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\n[1] 69067.91\n\n\nI would now use the ggplot() of ggplot2 package to plot out this grapoh showing prediction data compared to actual data\n\ntheme_set(theme_light())\nggplot(data = test_data_p,\n       aes(x = GRF_pred/1000,\n           y = resale_price/1000)) +\n  geom_point() +\n  ggtitle(\"Model prediction graph\") +\n  xlab(\"Resale price (predicted) thousands $SGD\") +\n  ylab(\"Resale price (actual) thousands $SGD\") +\n  geom_abline(color = \"blue4\", size = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe graph has an diagonal line and the closer the dots are to the line the more accurate the results were. From initial observation it seems that the model has done ok in term of predicting the resale price of HDB units.\nHowever it seems there maybe more or less variables that could be added in or removed to calibrate the model further make its prediction event more accurate. There seems to be more points on the left of the line rather than the right, this signify that the predicted price are often lower than the actual resale prices. This does make sense as all our data actually did not include any economics indicators such as inflation, tax rate raise (2024 in Singapore), etc. Including these metrics in to the model building would greatly improve the model.\nIn addition, the appearance of outliers, points more on the top left and top rights has also impacted the model itself and its prediction capability. The exclusion of these outliers could potentially be beneficial for the building of a better model as well."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html",
    "href": "R-GAA/Project02/Project02.html",
    "title": "Project 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion USD from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion USD in 2020. However, it is important to note that the tourism economy of Thailand are not evenly distributed.\nWe are interested to discover:\n\nIf the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above.\n\n\n\n\n\npacman::p_load(sf, sfdep, spdep, tmap, plotly, tidyverse, Kendall)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#objectives",
    "href": "R-GAA/Project02/Project02.html#objectives",
    "title": "Project 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion USD from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion USD in 2020. However, it is important to note that the tourism economy of Thailand are not evenly distributed.\nWe are interested to discover:\n\nIf the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#the-tasks",
    "href": "R-GAA/Project02/Project02.html#the-tasks",
    "title": "Project 2",
    "section": "",
    "text": "The specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#the-packages",
    "href": "R-GAA/Project02/Project02.html#the-packages",
    "title": "Project 2",
    "section": "",
    "text": "pacman::p_load(sf, sfdep, spdep, tmap, plotly, tidyverse, Kendall)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#importing-the-raw-data",
    "href": "R-GAA/Project02/Project02.html#importing-the-raw-data",
    "title": "Project 2",
    "section": "2.1 Importing the raw data",
    "text": "2.1 Importing the raw data\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Domestic Tourism Statistics at Kaggle. We are required to use version 2 of the data set.\nThailand - Subnational Administrative Boundaries at HDX. We are required to use the province boundary data set.\n\nThe code chunk below is used to import Thailand - Subnational Administrative Boundaries as well as filtering out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create THSAB_sf\n\nTHSAB_sf &lt;- st_read(dsn = \"data/geo\", \n                         layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\Portfolio\\R-GAA\\Project02\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThis code chunk is to import Thailand Domestic Tourism Statistics data and create tourism.\nIn the below code I will also create 2 new columns for the Month and Year separately for the purpose of using them for later analysis as well as converting the province_thai column name to ADM1_TH for the purpose of left-joining with the GEO data later and removing the date column as the 2 new Month and Year column are already created.\n\ntourism &lt;- read_csv(\"data/non-geo/thailand_domestic_tourism_2019_2023_ver2.csv\") %&gt;%\n  mutate(`month` = as.numeric(format(as.Date(`date`), \"%m\"))) %&gt;%\n  mutate(`year` = as.numeric(format(as.Date(`date`), \"%Y\"))) %&gt;%\n  rename(`ADM1_TH` = `province_thai`) %&gt;%\n  select(2:9)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#visualising-foreign-revenue-indicator",
    "href": "R-GAA/Project02/Project02.html#visualising-foreign-revenue-indicator",
    "title": "Project 2",
    "section": "3.1 Visualising Foreign Revenue Indicator",
    "text": "3.1 Visualising Foreign Revenue Indicator\nFirst I want to merge the yearly foreign revenue table revenue_foreign_year with the GEO data THSAB_sf for easier analysis later. This is done using the code below\n\nrevenue_foreign &lt;- revenue_foreign_year %&gt;%\n  left_join(THSAB_sf) %&gt;%\n  select(1:2,4, 7, 20)\n\nBefore we start the analysis let create a spactime data revenue_foreign_st using revenue_foreign for the purpose of study later\n\nrevenue_foreign_st &lt;- spacetime(revenue_foreign,\n                                THSAB_sf,\n                                .loc_col = \"ADM1_EN\",\n                                .time_col = \"year\")\n\nFor the basic visualization I would still want to see if there are any potential cluster and I want to see the changes of Foreign Revenue cluster over the year. Hence for this purpose I would plot 4 graph using the data extract from revenue_foreign_2019. Therefore I will be using bclust style which is a good combination between kmeans and hclust to fill the data\n\nrevenue_foreign_2019 &lt;- revenue_foreign %&gt;%\n  filter(year == 2019)\nrevenue_foreign_2020 &lt;- revenue_foreign %&gt;%\n  filter(year == 2020)\nrevenue_foreign_2021 &lt;- revenue_foreign %&gt;%\n  filter(year == 2021)\nrevenue_foreign_2022 &lt;- revenue_foreign %&gt;%\n  filter(year == 2022)\n\nRF2019 &lt;- tm_shape(st_as_sf(revenue_foreign_2019)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2019\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2019\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2020 &lt;- tm_shape(st_as_sf(revenue_foreign_2020)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2020\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2020\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2021 &lt;- tm_shape(st_as_sf(revenue_foreign_2021)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2021\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2021\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2022 &lt;- tm_shape(st_as_sf(revenue_foreign_2022)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2022\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2022\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\n\ntmap_arrange(RF2019, RF2020, RF2021, RF2022, asp=1, ncol=4)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#global-measures-of-spatial-autocorrelation",
    "href": "R-GAA/Project02/Project02.html#global-measures-of-spatial-autocorrelation",
    "title": "Project 2",
    "section": "3.2 Global Measures of Spatial Autocorrelation",
    "text": "3.2 Global Measures of Spatial Autocorrelation\nI’ve previously created the Queen contiguity weight matrix thai_wm_q with snap = 400. Next we need to create Row-standardised weights matrix using the code below\n\nthai_rswm_q &lt;- nb2listw(thai_wm_q,\n                        style=\"W\",\n                        zero.policy = TRUE)\nthai_rswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 354 \nPercentage nonzero weights: 5.970653 \nAverage number of links: 4.597403 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1      S2\nW 77 5929 77 37.54724 320.752\n\n\n\n3.2.1 Computing Global Carlo Moran’s I\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep. from 2019 to 2022\n\nwm_q_2019 &lt;- revenue_foreign_2019 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2020 &lt;- revenue_foreign_2020 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2021 &lt;- revenue_foreign_2021 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2022 &lt;- revenue_foreign_2022 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n2019\n\nglobal_moran_perm(wm_q_2019$sum_rev,\n                  wm_q_2019$nb,\n                  wm_q_2019$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.0065291, observed rank = 751, p-value = 0.498\nalternative hypothesis: two.sided\n\n\n2020\n\nglobal_moran_perm(wm_q_2020$sum_rev,\n                  wm_q_2020$nb,\n                  wm_q_2020$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.0044982, observed rank = 788, p-value = 0.424\nalternative hypothesis: two.sided\n\n\n2021\n\nglobal_moran_perm(wm_q_2021$sum_rev,\n                  wm_q_2021$nb,\n                  wm_q_2021$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.011613, observed rank = 942, p-value = 0.116\nalternative hypothesis: two.sided\n\n\n2022\n\nglobal_moran_perm(wm_q_2022$sum_rev,\n                  wm_q_2022$nb,\n                  wm_q_2022$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.020738, observed rank = 691, p-value = 0.618\nalternative hypothesis: two.sided\n\n\nAnother way to do this is using the below test method code chunk since we already have the listw of thai_rswm_q\n\nset.seed(1234)\nbperm_2019 = moran.mc(revenue_foreign_2019$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2019\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2019$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.024516, observed rank = 862, p-value = 0.138\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2020 = moran.mc(revenue_foreign_2020$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2020\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2020$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.027343, observed rank = 879, p-value = 0.121\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2021 = moran.mc(revenue_foreign_2021$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2021\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2021$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.014846, observed rank = 855, p-value = 0.145\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2022 = moran.mc(revenue_foreign_2022$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2022\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2022$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.015294, observed rank = 799, p-value = 0.201\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll the test over the year from 2019-2022 indicate that p-value &gt; 0.05 hence the null hypothesis are not rejected\n\n\n\n\n3.2.2 Visualising Global Moran’s I\nThe code chunk below is used to plot a histogram of Simulated Moran’s I\n\n2019202020212022\n\n\n\nhist(bperm_2019$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2019 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2020$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2020 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2021$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2021 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2022$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2022 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\n\nMI_corr_2019 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2019$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2019)\n\n\n\n\n\n\n\n\n\nMI_corr_2020 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2020$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2020)\n\n\n\n\n\n\n\n\n\nMI_corr_2021 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2021$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2021)\n\n\n\n\n\n\n\n\n\nMI_corr_2022 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2022$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2022)\n\n\n\n\n\n\n\n\n\n\n3.2.3 Computing local Moran’s I\nUsing the above created wm_q data, we could create the LISA Map and visualizaing the local Moran’s I. The below code is used to create the lisa mapping for Local Moran’s I of Foreign revenue at Province level by using local_moran() of sfdep package.\n\nlisa_2019 &lt;- wm_q_2019 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2020 &lt;- wm_q_2020 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2021 &lt;- wm_q_2021 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2022 &lt;- wm_q_2022 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n3.2.4 Visualising local Moran’s I\nVisualising local Moran’s I and p-value for each year\n\n2019202020212022\n\n\n\ntmap_mode(\"plot\")\n\nmap2019_1&lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2019\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2019_2 &lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2019_1, map2019_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2020_1&lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2020\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2020_2 &lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2020_1, map2020_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2021_1&lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2021\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2021_2 &lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2021_1, map2021_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2022_1&lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2022\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2022_2 &lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2022_1, map2022_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Plotting LISA map\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\nlisa_sig_2019 &lt;- lisa_2019  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2019 &lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2019\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2019)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2020 &lt;- lisa_2020  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2020 &lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2020\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2020)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2021 &lt;- lisa_2021  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2021 &lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2021\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2021)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2022 &lt;- lisa_2022  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2022 &lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2022\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2022)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_mode(\"plot\")\ntmap_arrange(lisa_map_2019, lisa_map_2020, lisa_map_2021, lisa_map_2022, ncol = 4)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "R-GAA/Project02/Project02.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "Project 2",
    "section": "3.3 Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "3.3 Hot Spot and Cold Spot Area Analysis (HCSA)\n\n3.3.1 Computing local Gi* statistics\n\nwm_idw_2019 &lt;- revenue_foreign_2019 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2020 &lt;- revenue_foreign_2020 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2021 &lt;- revenue_foreign_2021 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2022 &lt;- revenue_foreign_2022 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nHCSA_2019 &lt;- wm_idw_2019 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2020 &lt;- wm_idw_2020 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2021 &lt;- wm_idw_2021 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2022 &lt;- wm_idw_2022 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_sig_2019 &lt;- HCSA_2019  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2020 &lt;- HCSA_2020  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2021 &lt;- HCSA_2021  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2022 &lt;- HCSA_2022  %&gt;%\n  filter(p_sim &lt; 0.05)\n\n\ntmap_mode(\"plot\")\n\nHCSA_map_2019 &lt;- tm_shape(st_as_sf(HCSA_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2019\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2019)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2020 &lt;- tm_shape(st_as_sf(HCSA_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2020\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2020)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2021 &lt;- tm_shape(st_as_sf(HCSA_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2021\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2021)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2022 &lt;- tm_shape(st_as_sf(HCSA_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2022\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2022)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\ntmap_arrange(HCSA_map_2019, HCSA_map_2020, HCSA_map_2021, HCSA_map_2022, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFigure above reveals the changes of hotspot and cold spot over the year."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#emerging-hotspot-analysis",
    "href": "R-GAA/Project02/Project02.html#emerging-hotspot-analysis",
    "title": "Project 2",
    "section": "3.4 Emerging Hotspot Analysis",
    "text": "3.4 Emerging Hotspot Analysis\nWe previously already create Spacetime revenue_foreign_st that included the foreign revenue data from 2019-2022\n\nis_spacetime_cube(revenue_foreign_st)\n\n[1] TRUE\n\n\n\n3.4.1 Computing Gi*\nDeriving the spatial weights\n\nrevenue_foreign_nb &lt;- revenue_foreign_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wt = st_inverse_distance(nb,\n                                  geometry,\n                                  scale = 1,\n                                  alpha = 1),\n  .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\ngi_stars &lt;- revenue_foreign_nb %&gt;%\n  group_by(year) %&gt;%\n  mutate(gi_star = local_gstar_perm(sum_rev,\n                                    nb,\n                                    wt)) %&gt;%\n  unnest(gi_star)\n\nMann-Kendall test data.frame We can replicate this for each location by using group_by() of dplyr package.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(ADM1_EN) %&gt;%\n  summarise(mk = list(unclass(\n      Kendall::MannKendall(gi_star)\n    )\n  )) %&gt;%\n  unnest_wider(mk)\n\n\n\n3.4.2 Mann-Kendall Test on Gi*\nWith these Gi* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Bankok county.\n\ncbg &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(ADM1_EN == \"Bangkok\") %&gt;%\n  select(ADM1_EN, year, gi_star)\n\nInteractive Mann-Kendall Plot\n\nggplotly(ggplot(data = cbg, \n       aes(x = year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light())\n\n\n\n\n\n\n\n3.4.3 Performing Emerging Hotspot Analysis\nUsing ehsa We can also sort to show significant emerging hot/cold spots\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:10)\nhead(emerging)\n\n# A tibble: 6 × 6\n  ADM1_EN          tau    sl     S     D  varS\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bangkok       -0.667 0.308    -4  6.00  8.67\n2 Chachoengsao  -0.667 0.308    -4  6.00  8.67\n3 Chanthaburi   -0.667 0.308    -4  6.00  8.67\n4 Chon Buri     -0.667 0.308    -4  6.00  8.67\n5 Lampang        0.667 0.308     4  6.00  8.67\n6 Nakhon Pathom -0.667 0.308    -4  6.00  8.67\n\n\n\nset.seed(1234)\nrevenue_ehsa &lt;- emerging_hotspot_analysis(\n  x = revenue_foreign_nb,\n  .var = \"sum_rev\",\n  k = 1,\n  nsim = 199,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\n\n\nrevenue_foreign_ehsa &lt;- THSAB_sf %&gt;%\n  left_join(revenue_ehsa,\n            by = join_by(ADM1_EN == location))\n\nVisualising the distribution of EHSA classes\n\nggplot(data = revenue_foreign_ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWe could see majority of location does not has any pattern\n\ntmap_mode('plot')\nrevenue_foreign_ehsa_sig &lt;- revenue_foreign_ehsa  %&gt;%\n  filter(p_value &lt; 1)\n\ntm_shape(st_as_sf(revenue_foreign_ehsa)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_text(\"ADM1_EN\", size=0.5) +\n  tm_shape(st_as_sf(revenue_foreign_ehsa_sig)) +\n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html",
    "href": "R-VAA/Project03/Project03.html",
    "title": "Project 3",
    "section": "",
    "text": "The Oceanus business ecosystem is dynamic in nature, marked by the continual emergence of startups, mergers, acquisitions, and investments. FishEye International serves as a vigilant overseer of this landscape, meticulously monitoring the activities of commercial fishing operators to uphold the integrity of the region’s marine ecosystem. Through comprehensive analysis of business records, FishEye endeavors to uncover ownership structures, shareholder dynamics, transactional histories, and the core offerings of each entity, culminating in the creation of CatchNet: the Oceanus Knowledge Graph, achieved through a blend of automated processes and manual review.\nRecent events have cast a shadow over Oceanus’s commercial fishing sector, following the discovery of illegal fishing practices by SouthSeafood Express Corp. In response, FishEye has initiated an in-depth exploration to discern the temporal implications of this occurrence on Oceanus’s fishing marketplace. The competitive landscape may witness a variety of reactions, ranging from aggressive maneuvers by industry players seeking to capitalize on voids left by SouthSeafood Express Corp, to a heightened awareness within the industry that unlawful activities will be met with diligent scrutiny and consequential action. This ongoing investigation underscores the significance of FishEye’s role in maintaining the ethical and legal standards of Oceanus’s commercial endeavors.\n\n\nWith reference to the Mini-Challenge 3 of VAST Challenge 2024.\nA key element in stopping illegal fishing is holding the people who own nefarious companies accountable. Thus, FishEye is keenly interested in developing visualization tools that work with CatchNet to identify the people who hold influence over business networks. That is especially difficult with varied and changing shareholder and ownership relationships. My main focus for this portion would be on:\nCreate a visual analytics approach that analysts can use to highlight temporal patterns and changes in corporate structures. Examine the most active people and businesses using visual analytics.\nBelow is a further clarification from organizer\n\n\n\n\n\n\n1. What level of corporate change is of interest?\n\n\n\nQuestion\nDoes the concept of ‘changes in corporate structures over time’ mean the changes in one corporation,like the changes of board of the company, or the changes happening in the whole society, like the ratio of some kind of companies?\nClarification\nFishEye is more interested in the ways in which the structures of individual corporations change over time, rather than macro-economic or industry-level changes in the Oceanus marketplace. In some cases the corporate structure of several organizations may be intertwined (such as when one company owns another) and the relationships between them would therefore be relevant. Systematic trends in the ways individual companies re-structure over time could also be interesting. However, any large-scale changes in the structure of the economy are not the intention.\n\n\nSo it looks like being to show the changes over time is important\n\n\n\nThe data used for this part would be the mc3.json file download from the VAST MC3 website"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#the-task",
    "href": "R-VAA/Project03/Project03.html#the-task",
    "title": "Project 3",
    "section": "",
    "text": "With reference to the Mini-Challenge 3 of VAST Challenge 2024.\nA key element in stopping illegal fishing is holding the people who own nefarious companies accountable. Thus, FishEye is keenly interested in developing visualization tools that work with CatchNet to identify the people who hold influence over business networks. That is especially difficult with varied and changing shareholder and ownership relationships. My main focus for this portion would be on:\nCreate a visual analytics approach that analysts can use to highlight temporal patterns and changes in corporate structures. Examine the most active people and businesses using visual analytics.\nBelow is a further clarification from organizer\n\n\n\n\n\n\n1. What level of corporate change is of interest?\n\n\n\nQuestion\nDoes the concept of ‘changes in corporate structures over time’ mean the changes in one corporation,like the changes of board of the company, or the changes happening in the whole society, like the ratio of some kind of companies?\nClarification\nFishEye is more interested in the ways in which the structures of individual corporations change over time, rather than macro-economic or industry-level changes in the Oceanus marketplace. In some cases the corporate structure of several organizations may be intertwined (such as when one company owns another) and the relationships between them would therefore be relevant. Systematic trends in the ways individual companies re-structure over time could also be interesting. However, any large-scale changes in the structure of the economy are not the intention.\n\n\nSo it looks like being to show the changes over time is important"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#the-data",
    "href": "R-VAA/Project03/Project03.html#the-data",
    "title": "Project 3",
    "section": "",
    "text": "The data used for this part would be the mc3.json file download from the VAST MC3 website"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#loading-and-launching-of-r-packages",
    "href": "R-VAA/Project03/Project03.html#loading-and-launching-of-r-packages",
    "title": "Project 3",
    "section": "1. Loading and launching of R Packages",
    "text": "1. Loading and launching of R Packages\nBelow is a list of R Packages I am planning to use for this portion and for exploration\n\nplotly for creating interactive web-based graphs via the open source JavaScript.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\njsonlite JSON parser and generator optimized for statistical data and the web.\nigraph for creating and manipulating graphs and analyzing networks.\ntidygraph provides a tidy framework for all things relational (networks/graphs, trees, etc.)\nggraph an extension of the ggplot2 API tailored to graph visualizations and provides the same flexible approach to building up plots layer by layer.\nvisNetwork for network visualization.\nggforce collection of mainly new stats and geoms for composing specialised plots\nskimr provides summary statistics about variables in data frames, tibbles, data tables and vectors.\ntidyverse an opinionated collection of R packages designed for data science.\n\n\npacman::p_load(plotly, DT, jsonlite, igraph, \n               tidygraph, ggraph, visNetwork,\n               ggforce, skimr, tidyverse)"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#importing-the-data",
    "href": "R-VAA/Project03/Project03.html#importing-the-data",
    "title": "Project 3",
    "section": "2. Importing the data",
    "text": "2. Importing the data\nI import the data from mc3.json file using the fromJSON() function\n\nmc3_data &lt;- fromJSON(\"data/mc3.json\")"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#quick-look-at-the-mc3-data",
    "href": "R-VAA/Project03/Project03.html#quick-look-at-the-mc3-data",
    "title": "Project 3",
    "section": "1. Quick look at the mc3 data",
    "text": "1. Quick look at the mc3 data\n\nglimpse(mc3_data)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 60520 obs. of  15 variables:\n  ..$ type             : chr [1:60520] \"Entity.Organization.Company\" \"Entity.Organization.Company\" \"Entity.Organization.Company\" \"Entity.Organization.Company\" ...\n  ..$ country          : chr [1:60520] \"Uziland\" \"Mawalara\" \"Uzifrica\" \"Islavaragon\" ...\n  ..$ ProductServices  : chr [1:60520] \"Unknown\" \"Furniture and home accessories\" \"Food products\" \"Unknown\" ...\n  ..$ PointOfContact   : chr [1:60520] \"Rebecca Lewis\" \"Michael Lopez\" \"Steven Robertson\" \"Anthony Wyatt\" ...\n  ..$ HeadOfOrg        : chr [1:60520] \"Émilie-Susan Benoit\" \"Honoré Lemoine\" \"Jules Labbé\" \"Dr. Víctor Hurtado\" ...\n  ..$ founding_date    : chr [1:60520] \"1954-04-24T00:00:00\" \"2009-06-12T00:00:00\" \"2029-12-15T00:00:00\" \"1972-02-16T00:00:00\" ...\n  ..$ revenue          : num [1:60520] 5995 71767 0 0 4747 ...\n  ..$ TradeDescription : chr [1:60520] \"Unknown\" \"Abbott-Gomez is a leading manufacturer and supplier of high-quality furniture and home accessories, catering to\"| __truncated__ \"Abbott-Harrison is a leading manufacturer of high-quality food products, including baked goods, snacks, and bev\"| __truncated__ \"Unknown\" ...\n  ..$ _last_edited_by  : chr [1:60520] \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" ...\n  ..$ _last_edited_date: chr [1:60520] \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _date_added      : chr [1:60520] \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _raw_source      : chr [1:60520] \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" ...\n  ..$ _algorithm       : chr [1:60520] \"Automatic Import\" \"Automatic Import\" \"Automatic Import\" \"Automatic Import\" ...\n  ..$ id               : chr [1:60520] \"Abbott, Mcbride and Edwards\" \"Abbott-Gomez\" \"Abbott-Harrison\" \"Abbott-Ibarra\" ...\n  ..$ dob              : chr [1:60520] NA NA NA NA ...\n $ links     :'data.frame': 75817 obs. of  11 variables:\n  ..$ start_date       : chr [1:75817] \"2016-10-29T00:00:00\" \"2035-06-03T00:00:00\" \"2028-11-20T00:00:00\" \"2024-09-04T00:00:00\" ...\n  ..$ type             : chr [1:75817] \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" ...\n  ..$ _last_edited_by  : chr [1:75817] \"Pelagia Alethea Mordoch\" \"Niklaus Oberon\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" ...\n  ..$ _last_edited_date: chr [1:75817] \"2035-01-01T00:00:00\" \"2035-07-15T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _date_added      : chr [1:75817] \"2035-01-01T00:00:00\" \"2035-07-15T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _raw_source      : chr [1:75817] \"Existing Corporate Structure Data\" \"Oceanus Corporations Monthly - Jun '35\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" ...\n  ..$ _algorithm       : chr [1:75817] \"Automatic Import\" \"Manual Entry\" \"Automatic Import\" \"Automatic Import\" ...\n  ..$ source           : chr [1:75817] \"Avery Inc\" \"Berger-Hayes\" \"Bowers Group\" \"Bowman-Howe\" ...\n  ..$ target           : chr [1:75817] \"Allen, Nichols and Thompson\" \"Jensen, Morris and Downs\" \"Barnett Inc\" \"Bennett Ltd\" ...\n  ..$ key              : int [1:75817] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ end_date         : chr [1:75817] NA NA NA NA ...\n\n\nThe main data in this mc3.json file includes 2 data frame nodes and links, hence I will further breakdown and review the data in these 2 data frames."
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#dataframe-nodes",
    "href": "R-VAA/Project03/Project03.html#dataframe-nodes",
    "title": "Project 3",
    "section": "2. Dataframe nodes",
    "text": "2. Dataframe nodes\nFirst I import this nodes data using as_tibble() function and have a quick glimpse() at the data itself, let call this data mc3_nodes_raw\nNodes\n\n\nShow the code\nmc3_nodes_raw &lt;- as_tibble(mc3_data$nodes) %&gt;%\n  distinct()\n\nglimpse(mc3_nodes_raw)\n\n\nRows: 60,520\nColumns: 15\n$ type                &lt;chr&gt; \"Entity.Organization.Company\", \"Entity.Organizatio…\n$ country             &lt;chr&gt; \"Uziland\", \"Mawalara\", \"Uzifrica\", \"Islavaragon\", …\n$ ProductServices     &lt;chr&gt; \"Unknown\", \"Furniture and home accessories\", \"Food…\n$ PointOfContact      &lt;chr&gt; \"Rebecca Lewis\", \"Michael Lopez\", \"Steven Robertso…\n$ HeadOfOrg           &lt;chr&gt; \"Émilie-Susan Benoit\", \"Honoré Lemoine\", \"Jules La…\n$ founding_date       &lt;chr&gt; \"1954-04-24T00:00:00\", \"2009-06-12T00:00:00\", \"202…\n$ revenue             &lt;dbl&gt; 5994.73, 71766.67, 0.00, 0.00, 4746.67, 46566.67, …\n$ TradeDescription    &lt;chr&gt; \"Unknown\", \"Abbott-Gomez is a leading manufacturer…\n$ `_last_edited_by`   &lt;chr&gt; \"Pelagia Alethea Mordoch\", \"Pelagia Alethea Mordoc…\n$ `_last_edited_date` &lt;chr&gt; \"2035-01-01T00:00:00\", \"2035-01-01T00:00:00\", \"203…\n$ `_date_added`       &lt;chr&gt; \"2035-01-01T00:00:00\", \"2035-01-01T00:00:00\", \"203…\n$ `_raw_source`       &lt;chr&gt; \"Existing Corporate Structure Data\", \"Existing Cor…\n$ `_algorithm`        &lt;chr&gt; \"Automatic Import\", \"Automatic Import\", \"Automatic…\n$ id                  &lt;chr&gt; \"Abbott, Mcbride and Edwards\", \"Abbott-Gomez\", \"Ab…\n$ dob                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nmc3_nodes_raw have 15 columns but many of them seems to be of no use from a data analysis perspective of this portion such as TradeDescription, _last_edited_by, _last_edited_date, _date_added, _raw_source, _algorithm, dob. In addition, some data column seems to be in the wrong format such as founding_date which supposed to be in datetime instead of character\nTherefore, I will select from the raw file, columns that I think maybe of use for the data analysis and fix the issue with wrong data format using the code below and call the new data mc3_nodes,I also rename the type to nodes_type instead since both the nodes and links dataframes seems to have type as one of the column\n\n\nShow the code\nmc3_nodes &lt;- mc3_nodes_raw %&gt;%\n  mutate(founding_date = as.Date(founding_date),\n         country = as.character(country),\n         id = as.character(id),\n         ProductServices = as.character(ProductServices),\n         revenue = as.numeric(as.character(revenue)),\n         type = as.character(type),\n         HeadOfOrg = as.character(HeadOfOrg),\n         PointOfContact = as.character(PointOfContact)) %&gt;%\n  select(id, \n         founding_date, \n         country, \n         type, \n         revenue, \n         ProductServices, \n         HeadOfOrg,\n         PointOfContact) %&gt;%\n  rename(nodes_type = type)\n\nglimpse(mc3_nodes)\n\n\nRows: 60,520\nColumns: 8\n$ id              &lt;chr&gt; \"Abbott, Mcbride and Edwards\", \"Abbott-Gomez\", \"Abbott…\n$ founding_date   &lt;date&gt; 1954-04-24, 2009-06-12, 2029-12-15, 1972-02-16, 1954-…\n$ country         &lt;chr&gt; \"Uziland\", \"Mawalara\", \"Uzifrica\", \"Islavaragon\", \"Oce…\n$ nodes_type      &lt;chr&gt; \"Entity.Organization.Company\", \"Entity.Organization.Co…\n$ revenue         &lt;dbl&gt; 5994.73, 71766.67, 0.00, 0.00, 4746.67, 46566.67, 1696…\n$ ProductServices &lt;chr&gt; \"Unknown\", \"Furniture and home accessories\", \"Food pro…\n$ HeadOfOrg       &lt;chr&gt; \"Émilie-Susan Benoit\", \"Honoré Lemoine\", \"Jules Labbé\"…\n$ PointOfContact  &lt;chr&gt; \"Rebecca Lewis\", \"Michael Lopez\", \"Steven Robertson\", …\n\n\nThe founding_date is now in the correct format"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#dataframe-links",
    "href": "R-VAA/Project03/Project03.html#dataframe-links",
    "title": "Project 3",
    "section": "3. Dataframe links",
    "text": "3. Dataframe links\nFirst I import this nodes data using as_tibble() function and have a quick glimpse() at the data itself, let call this data mc3_edges_raw\n\n\nShow the code\nmc3_edges_raw &lt;- as_tibble(mc3_data$links) %&gt;%\n  distinct()\n\nglimpse(mc3_edges_raw)\n\n\nRows: 75,817\nColumns: 11\n$ start_date          &lt;chr&gt; \"2016-10-29T00:00:00\", \"2035-06-03T00:00:00\", \"202…\n$ type                &lt;chr&gt; \"Event.Owns.Shareholdership\", \"Event.Owns.Sharehol…\n$ `_last_edited_by`   &lt;chr&gt; \"Pelagia Alethea Mordoch\", \"Niklaus Oberon\", \"Pela…\n$ `_last_edited_date` &lt;chr&gt; \"2035-01-01T00:00:00\", \"2035-07-15T00:00:00\", \"203…\n$ `_date_added`       &lt;chr&gt; \"2035-01-01T00:00:00\", \"2035-07-15T00:00:00\", \"203…\n$ `_raw_source`       &lt;chr&gt; \"Existing Corporate Structure Data\", \"Oceanus Corp…\n$ `_algorithm`        &lt;chr&gt; \"Automatic Import\", \"Manual Entry\", \"Automatic Imp…\n$ source              &lt;chr&gt; \"Avery Inc\", \"Berger-Hayes\", \"Bowers Group\", \"Bowm…\n$ target              &lt;chr&gt; \"Allen, Nichols and Thompson\", \"Jensen, Morris and…\n$ key                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ end_date            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nmc3_edges_raw is also having the same prolem as mc3_nodes_raw of having 1 columns but many of them seems to be of no use from a data analysis perspective of this portion such as _last_edited_by, _last_edited_date, _date_added, _raw_source, _algorithm, key. In addition, some data column seems to be in the wrong format such as start_date and end_date which supposed to be in datetime instead of character\nTherefore, I will select from the raw file, columns that I think maybe of use for the data analysis and fix the issue with wrong data format using the code below and call the new data mc3_edges\n\n\nShow the code\nmc3_edges &lt;- mc3_edges_raw %&gt;%\n  select(source, \n         target, \n         type, \n         start_date, \n         end_date) %&gt;%\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type),\n         start_date = as.Date(start_date),\n         end_date = as.Date(end_date)) \n\nglimpse(mc3_edges)\n\n\nRows: 75,817\nColumns: 5\n$ source     &lt;chr&gt; \"Avery Inc\", \"Berger-Hayes\", \"Bowers Group\", \"Bowman-Howe\",…\n$ target     &lt;chr&gt; \"Allen, Nichols and Thompson\", \"Jensen, Morris and Downs\", …\n$ type       &lt;chr&gt; \"Event.Owns.Shareholdership\", \"Event.Owns.Shareholdership\",…\n$ start_date &lt;date&gt; 2016-10-29, 2035-06-03, 2028-11-20, 2024-09-04, 2034-11-12…\n$ end_date   &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nThe start_date and end_date are now in the correct format"
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#number-of-organization-over-the-year.",
    "href": "R-VAA/Project03/Project03.html#number-of-organization-over-the-year.",
    "title": "Project 3",
    "section": "1. Number of Organization over the year.",
    "text": "1. Number of Organization over the year.\nFor this Analysis and visualization I want to create a time series line graph of how many organization were founded each year over the years, to find if there any trend or suspicious changes in number of Organization over the years.\nLet us look at the different type that the mc3_nodes data have\n\n\nShow the code\nunique(mc3_nodes$nodes_type)\n\n\n[1] \"Entity.Organization.Company\"         \n[2] \"Entity.Organization.LogisticsCompany\"\n[3] \"Entity.Organization.FishingCompany\"  \n[4] \"Entity.Organization.FinancialCompany\"\n[5] \"Entity.Organization.NewsCompany\"     \n[6] \"Entity.Organization.NGO\"             \n[7] \"Entity.Person\"                       \n[8] \"Entity.Person.CEO\"                   \n\n\nThere seems to be multiple type including Organization and Person, for the purpose of this analysis, we will be focusing on Organization. Therefore, I will filter the data to Organization. In addition, since the duration of founding_date is between 1945 to 2035 (70 years of data), I will create another column called founding_year to breakdown the date to year instead\n\n\nShow the code\nmc3_nodes_Organization &lt;- mc3_nodes %&gt;%\n  mutate(founding_year = format(founding_date, format=\"%Y\")) %&gt;%\n  filter(str_like(nodes_type, \"%Entity.Organization%\"))\nglimpse(mc3_nodes_Organization)\n\n\nRows: 8,871\nColumns: 9\n$ id              &lt;chr&gt; \"Abbott, Mcbride and Edwards\", \"Abbott-Gomez\", \"Abbott…\n$ founding_date   &lt;date&gt; 1954-04-24, 2009-06-12, 2029-12-15, 1972-02-16, 1954-…\n$ country         &lt;chr&gt; \"Uziland\", \"Mawalara\", \"Uzifrica\", \"Islavaragon\", \"Oce…\n$ nodes_type      &lt;chr&gt; \"Entity.Organization.Company\", \"Entity.Organization.Co…\n$ revenue         &lt;dbl&gt; 5994.73, 71766.67, 0.00, 0.00, 4746.67, 46566.67, 1696…\n$ ProductServices &lt;chr&gt; \"Unknown\", \"Furniture and home accessories\", \"Food pro…\n$ HeadOfOrg       &lt;chr&gt; \"Émilie-Susan Benoit\", \"Honoré Lemoine\", \"Jules Labbé\"…\n$ PointOfContact  &lt;chr&gt; \"Rebecca Lewis\", \"Michael Lopez\", \"Steven Robertson\", …\n$ founding_year   &lt;chr&gt; \"1954\", \"2009\", \"2029\", \"1972\", \"1954\", \"2031\", \"2007\"…\n\n\nNext I would want to count the number of company founded per year using the code below under Organization_historical_year\n\n\nShow the code\nOrganization_historical_year &lt;- mc3_nodes_Organization %&gt;%\n  group_by(`founding_year`, `nodes_type`) %&gt;%\n  summarise(count = n())\n\n\nNext I will pivot_wider() the data so each different Organization counts would be in its own columns, this is for the purpose of plotting the graph later since I want to show them in the same plot as well as being able to select the specific Organization viewers want to choose. In addition, since I want to have a data table later showing the raw data I will also renaming the column to a more user friendly name and then sort the full table starting with the earliest year. This would create Organization_historical_year_select\n\n\nShow the code\nOrganization_historical_year_select &lt;- Organization_historical_year %&gt;%\n  select(founding_year, nodes_type, count) %&gt;%\n  pivot_wider(names_from = nodes_type, values_from = count) %&gt;%\n  rename(`Founding Year` = founding_year,\n         `Company` = Entity.Organization.Company,\n         `Fishing Company` = Entity.Organization.FishingCompany,\n         `Logistics Company` = Entity.Organization.LogisticsCompany,\n         `Financial Company` = Entity.Organization.FinancialCompany,\n         `News Company` = Entity.Organization.NewsCompany,\n         `NGO` = Entity.Organization.NGO)\n\nOrganization_historical_year_select &lt;- \n  Organization_historical_year_select[\n    order(Organization_historical_year_select$`Founding Year`),]\n\n\nWith that done, I would showcase final data table and plot the interactive time series graph with a time slider\n\n\nShow the code\ndatatable(Organization_historical_year_select, \n              filter = 'top', \n              options = list(pageLength = 10, \n                             autoWidth = TRUE))\n\n\n\n\n\n\nShow the code\nplot_ly(as.data.frame(Organization_historical_year_select),\n          x = ~`Founding Year`,\n          y = ~`Company`,\n          name = \"Company\",\n          type = 'scatter',\n          mode = 'lines+markers',\n          text = ~paste(\"Year: \", `Founding Year`, \n                        \"&lt;br&gt;Founded: \", Company),\n          hoverinfo = 'text') %&gt;%\n  add_trace(y = ~`Fishing Company`, \n            name = 'Fishing Company', \n            mode = 'lines+markers',\n            text = ~paste(\"Year: \", `Founding Year`,\n                          \"&lt;br&gt;Founded: \", `Fishing Company`),\n            hoverinfo = 'text') %&gt;%\n  add_trace(y = ~`Logistics Company`, \n            name = 'Logistics Company', \n            mode = 'lines+markers',\n            text = ~paste(\"Year: \", `Founding Year`,\n                          \"&lt;br&gt;Founded: \", `Logistics Company`),\n            hoverinfo = 'text') %&gt;%\n  add_trace(y = ~`Financial Company`, \n            name = 'Financial Company', \n            mode = 'lines+markers',\n            text = ~paste(\"Year: \", `Founding Year`,\n                          \"&lt;br&gt;Founded: \", `Financial Company`),\n            hoverinfo = 'text') %&gt;%\n  add_trace(y = ~`News Company`, \n            name = 'News Company', \n            mode = 'lines+markers',\n            text = ~paste(\"Year: \", `Founding Year`,\n                          \"&lt;br&gt;Founded: \", `News Company`),\n            hoverinfo = 'text') %&gt;%\n  add_trace(y = ~`NGO`, \n            name = 'NGO', \n            mode = 'lines+markers',\n            text = ~paste(\"Year: \", `Founding Year`,\n                          \"&lt;br&gt;Founded: \", `NGO`),\n            hoverinfo = 'text') %&gt;%\n  layout(legend = list(orientation = 'h'),\n         xaxis = list(title = \"Founding Year\", \n                      rangeslider = list(visible = TRUE, \n                                         thickness = 0.03)),\n         yaxis = list(title = \"Count\"))\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nThere seems to be a spike of number of Company founded between 2034 and 2035, the number of Fishing Company has a spike in 2031 but has been since on a steady decline ever since."
  },
  {
    "objectID": "R-VAA/Project03/Project03.html#beneficial-ownership-data-analysis-and-visualization",
    "href": "R-VAA/Project03/Project03.html#beneficial-ownership-data-analysis-and-visualization",
    "title": "Project 3",
    "section": "2. Beneficial Ownership data analysis and Visualization",
    "text": "2. Beneficial Ownership data analysis and Visualization\nLet looks at the different of relationship type that the mc3_edges table has\n\n\nShow the code\nunique(mc3_edges$type)\n\n\n[1] \"Event.Owns.Shareholdership\"      \"Event.Owns.BeneficialOwnership\" \n[3] \"Event.WorksFor\"                  \"Relationship.FamilyRelationship\"\n\n\nThere seems to be 4 type of relationship, for this part I would be focusing more on the Beneficial Ownership relationship. First thing first, as previously seen there seems to be 2 columns that represent the entity relation either source or target. I am curious to see what are the type of the entity for each of these source and target hence I would use the mc3_nodes to join with mc3_edges table to find out the nature of these source or target\n\n\nShow the code\nnodes_type &lt;- mc3_nodes %&gt;%\n  select(id, nodes_type)\n\nmc3_edges &lt;- mc3_edges %&gt;%\n  left_join(nodes_type, by = c(\"source\" = \"id\")) %&gt;%\n  rename(nodes_type_source = nodes_type) %&gt;%\n  left_join(nodes_type, by = c(\"target\" = \"id\")) %&gt;%\n  rename(nodes_type_target = nodes_type)\n\n\nLet us check the data generated\n\n\nShow the code\nmc3_edges %&gt;%\n  filter(type == \"Event.Owns.BeneficialOwnership\") %&gt;%\n  head()\n\n\n# A tibble: 6 × 7\n  source  target type  start_date end_date nodes_type_source nodes_type_target  \n  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;date&gt;     &lt;date&gt;   &lt;chr&gt;             &lt;chr&gt;              \n1 Laura … Brigg… Even… 2018-05-10 NA       Entity.Person     Entity.Organizatio…\n2 Jillia… Brigg… Even… 2013-11-30 NA       Entity.Person     Entity.Organizatio…\n3 Anna B… Brigg… Even… 2012-05-04 NA       Entity.Person     Entity.Organizatio…\n4 Dawn K… Brigg… Even… 2007-03-16 NA       Entity.Person     Entity.Organizatio…\n5 Dawn K… Flemi… Even… 2016-09-28 NA       Entity.Person     Entity.Organizatio…\n6 Elizab… Brigg… Even… 2011-11-28 NA       Entity.Person     Entity.Organizatio…\n\n\nInterestingly, it seems like source are the entity that has the type relationship with the target, on the Beneficial Ownership context, source are the owners (specifically person or individual) and target is the one being owned (specifically Organization).\nWith the understanding above I want find out want to know how many in total a source own a target and how many source (owners) a target have over the year. Let us start with the first part\n\nHow many in total a source own a target over the year\nFor this part I will first filter the data to Event.Owns.BeneficialOwnership then do a group_by() of start_date and source then count the number of row creating BO_indv_count. Afteward, I will group_by() again using source and sum the BO_indv_count creating the cummulative column BO_indv_total\n\n\nShow the code\nedges_BO_indv_count &lt;- mc3_edges %&gt;%\n  filter(type == \"Event.Owns.BeneficialOwnership\") %&gt;%\n  group_by(start_date, source) %&gt;%  \n  summarise(BO_indv_count = n())%&gt;%\n  group_by(source) %&gt;%\n  mutate(BO_indv_total = cumsum(BO_indv_count)) %&gt;%\n  ungroup()\n\n\nLet see how many unique source there is in the table\n\nn_distinct(edges_BO_indv_count$source)\n\n[1] 16231\n\n\n16231 is quite a large number, hence I will reduce this number by create a list of Owner that own 10 or more target using the code below\n\n\nShow the code\nOwner_list &lt;- \n  edges_BO_indv_count[\n    order(edges_BO_indv_count$BO_indv_total,\n          decreasing = T),] %&gt;%\n  filter(BO_indv_total&gt;=10) %&gt;%\n  select(source) %&gt;%\n  distinct()\n\n\nWhat left now is to plot the graph showing the Individual total Beneficial Ownership over time\n\n\nShow the code\nBO_indv_count_table &lt;- edges_BO_indv_count %&gt;%\n  rename(`Start Date` = start_date,\n         `Individual` = source,\n         `Ownership at curent date` = BO_indv_count,\n         `Total Ownership at curent date` = BO_indv_total)\n  \ndatatable(BO_indv_count_table, \n              filter = 'top', \n              options = list(pageLength = 10, \n                             autoWidth = TRUE))\n\n\n\n\n\n\nShow the code\nfig &lt;- edges_BO_indv_count %&gt;%\n  select(start_date, source, BO_indv_total) %&gt;%\n  filter(source %in% Owner_list$source)%&gt;%\n  plot_ly(x = ~start_date,\n          y = ~BO_indv_total,\n          type = 'scatter',\n          mode = 'lines+markers',\n          text = ~paste(\"Day: \", start_date, \n                        \"&lt;br&gt;Own: \", BO_indv_total),\n          hoverinfo = 'text',\n          fill = 'tozeroy',\n          transforms = list(\n            list(\n              type = 'filter',\n              target = ~source,\n              operation = '=',\n              value = unique(Owner_list$source)[1]))) %&gt;%\n  layout(title = 'Individual total Beneficial Ownership over time',\n         xaxis = list(title = \"Time\",\n                      rangeslider = list(visible = TRUE,\n                                         thickness = 0.03)),\n         yaxis = list(title = \"Count\"),\n         updatemenus = list(\n           list(type = 'dropdown',\n                active = 0,\n                buttons = list(\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[1]),\n                       label = unique(Owner_list$source)[1]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[2]),\n                       label = unique(Owner_list$source)[2]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[3]),\n                       label = unique(Owner_list$source)[3]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[4]),\n                       label = unique(Owner_list$source)[4]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[5]),\n                       label = unique(Owner_list$source)[5]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[6]),\n                       label = unique(Owner_list$source)[6]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[7]),\n                       label = unique(Owner_list$source)[7]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[8]),\n                       label = unique(Owner_list$source)[8]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[9]),\n                       label = unique(Owner_list$source)[9]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[10]),\n                       label = unique(Owner_list$source)[10]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[11]),\n                       label = unique(Owner_list$source)[11]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[12]),\n                       label = unique(Owner_list$source)[12]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[13]),\n                       label = unique(Owner_list$source)[13]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[14]),\n                       label = unique(Owner_list$source)[14]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[15]),\n                       label = unique(Owner_list$source)[15]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[16]),\n                       label = unique(Owner_list$source)[16]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[17]),\n                       label = unique(Owner_list$source)[17]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[18]),\n                       label = unique(Owner_list$source)[18]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[19]),\n                       label = unique(Owner_list$source)[19]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[20]),\n                       label = unique(Owner_list$source)[20]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[21]),\n                       label = unique(Owner_list$source)[21]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[22]),\n                       label = unique(Owner_list$source)[22]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[23]),\n                       label = unique(Owner_list$source)[23]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[24]),\n                       label = unique(Owner_list$source)[24]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[25]),\n                       label = unique(Owner_list$source)[25]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[26]),\n                       label = unique(Owner_list$source)[26]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[27]),\n                       label = unique(Owner_list$source)[27]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[28]),\n                       label = unique(Owner_list$source)[28]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[29]),\n                       label = unique(Owner_list$source)[29]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[30]),\n                       label = unique(Owner_list$source)[30]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[31]),\n                       label = unique(Owner_list$source)[31]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[32]),\n                       label = unique(Owner_list$source)[32]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[33]),\n                       label = unique(Owner_list$source)[33]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[34]),\n                       label = unique(Owner_list$source)[34]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[35]),\n                       label = unique(Owner_list$source)[35]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[36]),\n                       label = unique(Owner_list$source)[36]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Owner_list$source)[37]),\n                       label = unique(Owner_list$source)[37])\n                  )\n                )\n              )\n            )\n\nfig\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nThere seems to be a few individual of interest such as Zachary Taylor or Breanna Price who suddenly start owning a significant amount of entities between 2033 and 2034 but then slow down their activities in 2035\n\n\n\n\nHow many in total target owned by source over the year\nSame as before I will go through the data creation steps\n\n\nShow the code\nBO_owners_count &lt;- mc3_edges %&gt;%\n  filter(type == \"Event.Owns.BeneficialOwnership\") %&gt;%\n  group_by(start_date, target) %&gt;%  \n  summarise(BeneficialOwnership_count = n())%&gt;%\n  group_by(target) %&gt;%\n  mutate(BeneficialOwnership_total = cumsum(BeneficialOwnership_count)) %&gt;%\n  ungroup()\n\n\nCompany that has 35 or more source using the code below\n\n\nShow the code\nCompany_list &lt;- \n  BO_owners_count[\n    order(BO_owners_count$BeneficialOwnership_total,\n          decreasing = T),] %&gt;%\n  filter(BeneficialOwnership_total&gt;=35) %&gt;%\n  select(target) %&gt;%\n  distinct()\n\n\nAnd finally plotting\n\n\nShow the code\nBO_owners_count_table &lt;- BO_owners_count %&gt;%\n  rename(`Start Date` = start_date,\n         `Organization` = target,\n         `New owners at curent date` = BeneficialOwnership_count,\n         `Total owners at curent date` = BeneficialOwnership_total)\n  \ndatatable(BO_owners_count_table, \n              filter = 'top', \n              options = list(pageLength = 10, \n                             autoWidth = TRUE))\n\n\n\n\n\n\nShow the code\nfig1 &lt;- BO_owners_count %&gt;%\n  select(start_date, target, BeneficialOwnership_total) %&gt;%\n  filter(target %in% Company_list$target)%&gt;%\n  plot_ly(x = ~start_date,\n          y = ~BeneficialOwnership_total,\n          type = 'scatter',\n          mode = 'lines+markers',\n          text = ~paste(\"Day: \", start_date, \n                        \"&lt;br&gt;Owner: \", BeneficialOwnership_total),\n          hoverinfo = 'text',\n          fill = 'tozeroy',\n          transforms = list(\n            list(\n              type = 'filter',\n              target = ~target,\n              operation = '=',\n              value = unique(Company_list$target)[1]))) %&gt;%\n  layout(title = 'Company total number of Beneficial Owners over time',\n         xaxis = list(title = \"Time\",\n                      rangeslider = list(visible = TRUE,\n                                         thickness = 0.03)),\n         yaxis = list(title = \"Count\"),\n         updatemenus = list(\n           list(type = 'dropdown',\n                active = 0,\n                buttons = list(\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[1]),\n                       label = unique(Company_list$target)[1]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[2]),\n                       label = unique(Company_list$target)[2]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[3]),\n                       label = unique(Company_list$target)[3]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[4]),\n                       label = unique(Company_list$target)[4]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[5]),\n                       label = unique(Company_list$target)[5]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[6]),\n                       label = unique(Company_list$target)[6]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[7]),\n                       label = unique(Company_list$target)[7]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[8]),\n                       label = unique(Company_list$target)[8]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[9]),\n                       label = unique(Company_list$target)[9]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[10]),\n                       label = unique(Company_list$target)[10]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[11]),\n                       label = unique(Company_list$target)[11]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[12]),\n                       label = unique(Company_list$target)[12]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[13]),\n                       label = unique(Company_list$target)[13]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[14]),\n                       label = unique(Company_list$target)[14]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[15]),\n                       label = unique(Company_list$target)[15]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[16]),\n                       label = unique(Company_list$target)[16]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[17]),\n                       label = unique(Company_list$target)[17]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[18]),\n                       label = unique(Company_list$target)[18]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[19]),\n                       label = unique(Company_list$target)[19]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[20]),\n                       label = unique(Company_list$target)[20]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[21]),\n                       label = unique(Company_list$target)[21]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[22]),\n                       label = unique(Company_list$target)[22]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[23]),\n                       label = unique(Company_list$target)[23]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[24]),\n                       label = unique(Company_list$target)[24]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[25]),\n                       label = unique(Company_list$target)[25]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[26]),\n                       label = unique(Company_list$target)[26]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[27]),\n                       label = unique(Company_list$target)[27]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[28]),\n                       label = unique(Company_list$target)[28]),\n                  list(method = \"restyle\",\n                       args = list(\"transforms[0].value\",\n                                   unique(Company_list$target)[29]),\n                       label = unique(Company_list$target)[29])\n                  )\n                )\n              )\n            )\nfig1\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nThere seems to be a few company of interest such as Downs Group who suddenly start having their business under owner ship at a significant rate between 2034 and 2035"
  },
  {
    "objectID": "R-VAA/Project01/Project01.html",
    "href": "R-VAA/Project01/Project01.html",
    "title": "Project 1",
    "section": "",
    "text": "Assuming the role of a graphical editor of a median company, the purpose of this data visualization exercise is to prepare minimum two and maximum three data visualizations to reveal interesting insights on the private residential market and sub-markets of Singapore for the 1st quarter of 2024 and prior periods.\n\n\n\nFor this exercise and to accomplish the task, transaction data of REALIS will be used, which provides comprehensive and up-to-date statistics on the property market in Singapore.\nA complete set of the private residential property transaction data from 1st January 2023 to 31st March 2024"
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#the-task",
    "href": "R-VAA/Project01/Project01.html#the-task",
    "title": "Project 1",
    "section": "",
    "text": "Assuming the role of a graphical editor of a median company, the purpose of this data visualization exercise is to prepare minimum two and maximum three data visualizations to reveal interesting insights on the private residential market and sub-markets of Singapore for the 1st quarter of 2024 and prior periods."
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#the-data",
    "href": "R-VAA/Project01/Project01.html#the-data",
    "title": "Project 1",
    "section": "",
    "text": "For this exercise and to accomplish the task, transaction data of REALIS will be used, which provides comprehensive and up-to-date statistics on the property market in Singapore.\nA complete set of the private residential property transaction data from 1st January 2023 to 31st March 2024"
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#loading-the-packages",
    "href": "R-VAA/Project01/Project01.html#loading-the-packages",
    "title": "Project 1",
    "section": "2.1 Loading the packages",
    "text": "2.1 Loading the packages\nFor this Take-home exercise 1, I am planning to use some of the libraries below:\ntidyverse: The tidyverse is an opinionated collection of R packages designed for data science.\npatchwork: a package to make it simple to combine separate ggplots into the same graphic\nggrepel: a package to provide geoms for ggplot2 to repel overlapping text labels\nggthemes: a package to provide some extra themes, geoms, and scales for ‘ggplot2’.\nggridges: a package for Ridgeline plots, which are partially overlapping line plots that create the impression of a mountain range.\nggdist: an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualizing distributions and uncertainty which will assist with ggridges package\n\nThe Code:\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse, ggridges, ggdist)"
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#importing-data-and-preparation",
    "href": "R-VAA/Project01/Project01.html#importing-data-and-preparation",
    "title": "Project 1",
    "section": "2.2 Importing data and preparation",
    "text": "2.2 Importing data and preparation\nSince the data is a list of csv files containing quarterly data from the first quarter of 2023 to the first quarter of 2024, for the purpose of the study I will be creating and using both the full data set (Realis) and a smaller data set containing only data for the first quarter of 2024 (first_quarter_2024) from ResidentialTransaction20240414220633.csv file\n\nThe Code:\n\nfirst_quarter_2024 &lt;- read_csv(\"data/ResidentialTransaction20240414220633.csv\")\n\nlist_of_files &lt;- list.files(path = \"data\",\n                            recursive = TRUE,\n                            pattern = \"\\\\.csv$\",\n                            full.names = TRUE)\nRealis &lt;- read_csv(list_of_files)"
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#data-overview",
    "href": "R-VAA/Project01/Project01.html#data-overview",
    "title": "Project 1",
    "section": "3.1 Data overview",
    "text": "3.1 Data overview\n\nThe Code:\n\nglimpse(Realis)\n\nRows: 26,806\nColumns: 21\n$ `Project Name`                &lt;chr&gt; \"THE REEF AT KING'S DOCK\", \"URBAN TREASU…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2317000, 1823500, 1421112, 1258112, 1280…\n$ `Area (SQFT)`                 &lt;dbl&gt; 882.65, 882.65, 1076.40, 1033.34, 871.88…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2625, 2066, 1320, 1218, 1468, 1767, 1095…\n$ `Sale Date`                   &lt;chr&gt; \"01 Jan 2023\", \"02 Jan 2023\", \"02 Jan 20…\n$ Address                       &lt;chr&gt; \"12 HARBOURFRONT AVENUE #05-32\", \"205 JA…\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Area (SQM)`                  &lt;dbl&gt; 82.0, 82.0, 100.0, 96.0, 81.0, 308.7, 42…\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 28256, 22238, 14211, 13105, 15802, 19015…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Condominium\", \"Executive…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 12/01/2021\", \"Freehold\", \"9…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"HDB\", \"Private\", \"HDB\", \"HDB\", \"HDB\", \"…\n$ `Postal Code`                 &lt;chr&gt; \"097996\", \"419535\", \"269343\", \"269294\", …\n$ `Postal District`             &lt;chr&gt; \"04\", \"14\", \"27\", \"27\", \"28\", \"19\", \"10\"…\n$ `Postal Sector`               &lt;chr&gt; \"09\", \"41\", \"26\", \"26\", \"79\", \"54\", \"27\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"East Region\", \"North …\n$ `Planning Area`               &lt;chr&gt; \"Bukit Merah\", \"Bedok\", \"Yishun\", \"Yishu…\n\nsummary(Realis)\n\n Project Name       Transacted Price ($)  Area (SQFT)       Unit Price ($ PSF)\n Length:26806       Min.   :   440000    Min.   :   322.9   Min.   : 138      \n Class :character   1st Qu.:  1280000    1st Qu.:   721.2   1st Qu.:1384      \n Mode  :character   Median :  1660000    Median :   990.3   Median :1762      \n                    Mean   :  2143286    Mean   :  1191.6   Mean   :1852      \n                    3rd Qu.:  2320000    3rd Qu.:  1302.4   3rd Qu.:2260      \n                    Max.   :392180000    Max.   :144883.4   Max.   :5756      \n  Sale Date           Address          Type of Sale       Type of Area      \n Length:26806       Length:26806       Length:26806       Length:26806      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   Area (SQM)      Unit Price ($ PSM) Nett Price($)      Property Type     \n Min.   :   30.0   Min.   : 1484      Length:26806       Length:26806      \n 1st Qu.:   67.0   1st Qu.:14893      Class :character   Class :character  \n Median :   92.0   Median :18966      Mode  :character   Mode  :character  \n Mean   :  110.7   Mean   :19930                                           \n 3rd Qu.:  121.0   3rd Qu.:24327                                           \n Max.   :13460.0   Max.   :61962                                           \n Number of Units     Tenure          Completion Date   \n Min.   : 1.000   Length:26806       Length:26806      \n 1st Qu.: 1.000   Class :character   Class :character  \n Median : 1.000   Mode  :character   Mode  :character  \n Mean   : 1.005                                        \n 3rd Qu.: 1.000                                        \n Max.   :60.000                                        \n Purchaser Address Indicator Postal Code        Postal District   \n Length:26806                Length:26806       Length:26806      \n Class :character            Class :character   Class :character  \n Mode  :character            Mode  :character   Mode  :character  \n                                                                  \n                                                                  \n                                                                  \n Postal Sector      Planning Region    Planning Area     \n Length:26806       Length:26806       Length:26806      \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\ncolSums(is.na(Realis))\n\n               Project Name        Transacted Price ($) \n                          0                           0 \n                Area (SQFT)          Unit Price ($ PSF) \n                          0                           0 \n                  Sale Date                     Address \n                          0                           0 \n               Type of Sale                Type of Area \n                          0                           0 \n                 Area (SQM)          Unit Price ($ PSM) \n                          0                           0 \n              Nett Price($)               Property Type \n                          0                           0 \n            Number of Units                      Tenure \n                          0                           0 \n            Completion Date Purchaser Address Indicator \n                          0                           0 \n                Postal Code             Postal District \n                          0                           0 \n              Postal Sector             Planning Region \n                          0                           0 \n              Planning Area \n                          0 \n\n\nA quick look at the data shown that there are 21 different columns, there are categorical variables that could be of interest such as Project Name, Property Type, Planning Region, Planning Area as well as continuous variables such as Transacted Price ($), Area (SQFT), Area (SQM), Unit Price ($ PSF), Unit Price ($ PSM), Number of Units.\nThere is also no missing data which is good and mean that we do not have to perform data wrangling for missing data.\nFor the purpose of this study, I would be focusing on these variable below:\nProperty Type, Planning Region, Transacted Price ($), Area (SQM), Unit Price ($ PSM), Sale Date"
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#data-wrangling",
    "href": "R-VAA/Project01/Project01.html#data-wrangling",
    "title": "Project 1",
    "section": "3.2 Data wrangling",
    "text": "3.2 Data wrangling\nInterestingly, Sale Date is supposed to be a kind of continuous variable or discrete variable; however, it is in ‘character’ type instead of ‘datetime’ or rather it is just in ‘string’, even though, the format seems correct. In addition, I also want to use a sort of monthly data visualization and the time seems to be in Date format.\nTherefore some data transformation would need to be performed to create 2 new column called Sale Date asDate which is properly in Datetime format, and Sale Month, which show the month Sale happened instead of date.\nTo do this I would be using parse_date_time part of the lubridate in tidyverse package to correctly pasring the ‘string’ into datetime format then use as.Date to finally turn it into proper ‘datetime’, this would also create the Sale Date asDate column.\nAfter above steps, I will be using format to create the Sale Month colum\n\nThe Code:\n\nRealis &lt;- Realis %&gt;% \n  mutate(`Sale Date asDate` = as.Date((parse_date_time(`Sale Date`, \n        orders = c(\"%d %b %Y\")))))\nRealis &lt;- Realis %&gt;% \n  mutate(`Sale Month` = format(as.Date((parse_date_time(`Sale Date`, \n        orders = c(\"%d %b %Y\")))), \"%b %Y\"))\n\n\n\nfirst_quarter_2024 &lt;- first_quarter_2024 %&gt;% \n  mutate(`Sale Date asDate` = as.Date((parse_date_time(`Sale Date`, \n        orders = c(\"%d %b %Y\")))))\n\nfirst_quarter_2024 &lt;- first_quarter_2024 %&gt;% \n  mutate(`Sale Month` = format(as.Date((parse_date_time(`Sale Date`, \n        orders = c(\"%d %b %Y\")))), \"%b %Y\"))"
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#data-visualization-for-the-first-quarter-of-2024",
    "href": "R-VAA/Project01/Project01.html#data-visualization-for-the-first-quarter-of-2024",
    "title": "Project 1",
    "section": "4.1 Data visualization for the first quarter of 2024",
    "text": "4.1 Data visualization for the first quarter of 2024\n\n4.1.1 Main part\nThis part of the first quarter of 2024 data visualization contain 3 plots:\n\nOn the left is the bar plot counting the number of sales breaking down by different Planning Region\nOn the right is a ridgeline plot showing the Unit Price ($ PSM) distribution breakdown by 4 quartiles by different Planning Region\nBottom plot is box plot showing a further breakdown from the ridgeline plot going into different Property Type\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot1 &lt;-ggplot(data = first_quarter_2024, aes(x = `Planning Region`)) +\n    geom_bar(color = \"grey10\", aes(fill = `Property Type`)) +\n    ggtitle(\"Sales by Property Type by Planning Region\") +\n    theme_economist() +\n    theme(axis.text=element_text(size = 20), \n          axis.title = element_text(size = 25), \n          title = element_text(size = 25, margin = margin(b = 15)), \n          axis.title.y = element_text(margin = margin(r = 15)),\n          axis.title.x = element_text(margin = margin(t = 15)))\n\nplot2 &lt;- ggplot(data = first_quarter_2024, aes(x = `Planning Region`, y = `Unit Price ($ PSM)`)) +\n    geom_boxplot(aes(color=`Property Type`)) + \n    theme(axis.text=element_text(size=12)) +\n    ggtitle(\"Distribution of Unit Price ($ PSM) by Planning Region by Property Type\")+\n    theme_economist() +\n    theme(axis.text=element_text(size = 20), \n          axis.title = element_text(size = 25), \n          title = element_text(size = 25, margin = margin(b = 15)), \n          axis.title.y = element_text(margin = margin(r = 15)),\n          axis.title.x = element_text(margin = margin(t = 15)))\n\nplot3 &lt;- ggplot(data = first_quarter_2024, \n       aes(y = `Planning Region`, x = `Unit Price ($ PSM)`, \n            fill = factor(after_stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_economist()+\n  ggtitle(\"Distribution of Unit Price ($ PSM) by Planning Region\")+\n  theme(axis.text=element_text(size = 20), \n        axis.title = element_text(size = 25), \n        title = element_text(size = 25, margin = margin(b = 15)), \n        axis.title.y = element_text(margin = margin(r = 15)),\n        axis.title.x = element_text(margin = margin(t = 15)))\n\n(plot1 + plot3 ) / plot2\n\n\n\n\n\n\n\n\n\n\nObservations and insights\n\n\n\nFrom the data visualizations above, here are my observations and insights on date from first quarter 2024:\n\nThe Central region seems to have the highest number of sales followed by the North East region with main Property Type being Apartment and Condominium. North Region has the lowest number of sales, but their main Property Type sale is Executive Condominium; similarly West Region main Property Type sale is also Executive Condominium\nThe Unit Price ($ PSM) seems to be on average higher in the Central and North East region with some of the highest being in Central region\nFurther breakdown of Unit Price ($ PSM) by Property Type show an interesting trend of Apartment being more expensive than other types in Central and North East region. Meanwhile, in North and West region where their main Property Type sale is Executive Condominium the Unit Price ($ PSM) is lowest.\n\n\n\n\n\n4.1.1 Sub part\nFor this sub part, I am showing a scatterplot of Area (SQM) vs Transacted Price ($), with a fit line using Generalized Linear Models, each data point is label under their Planning Region and different colors is for different Property Type.\nThe purpose of this plot is to see if there is a relationship between property size and its price as well as to see which property type from which region is likely to be of bigger size and more expensive overall.\nHowever, since the proper size and its transacted price could be quite high (144883.4 SQM for size and 392,180,000 SGD for price), for this study, I have limited the data to be within 30 - 600 SQM for size and 450,000 - 10,000,000 SGD for transacted price\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = first_quarter_2024, aes(y = `Area (SQM)`,\n                                      x = `Transacted Price ($)`)) +\n  geom_point()+\n  geom_smooth(method = glm, linewidth = 1) +\n  coord_cartesian(ylim = c(30,600), xlim=c(450000,10000000)) +\n  geom_label_repel(aes(label = `Planning Region`, color = `Property Type`), \n                   fontface = \"bold\", max.overlaps = 12) +\n  ggtitle(\"Size (SQM) vs Transacted Price ($) in the first quarter of 2024\") +\n  theme_economist()+\n  theme(axis.text=element_text(size = 15), \n        axis.title = element_text(size = 20), \n        title = element_text(size = 20, margin = margin(b = 15)), \n        axis.title.y = element_text(margin = margin(r = 15)),\n        axis.title.x = element_text(margin = margin(t = 15)))\n\n\n\n\n\n\n\n\n\n\nObservations and insights\n\n\n\nFrom the data visualizations above, here are my observations\n\nThere seems to be a clear linear correlation between Area (SQM) and Transacted Price ($) based on the Generalized Linear Models fit line.\nCentral region shows a high concentration of high value properties with some of the most expensive properties and these properties are mainly Condominium and Executive Condominium.\nDetached House seems to be generally larger than other types of property."
  },
  {
    "objectID": "R-VAA/Project01/Project01.html#time-series-data-visualization-from-2023-to-first-quarter-of-2024",
    "href": "R-VAA/Project01/Project01.html#time-series-data-visualization-from-2023-to-first-quarter-of-2024",
    "title": "Project 1",
    "section": "4.2 Time series data visualization from 2023 to first quarter of 2024",
    "text": "4.2 Time series data visualization from 2023 to first quarter of 2024\nThis part of the data visualization contain 3 plots:\n\nOn the top is a bar plot, counting the number of monthly sales breaking down by different Property Type\nIn the middle is a box plot showing the monthly distribution of Unit Price ($ PSM) by different Property Type\nBottom plot is a line plot as further breakdown from the middle box plot, showing the movement of Mean Unit Price ($ PSM) by different Property Type\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot4 &lt;- ggplot(data = Realis, aes(x = `Sale Month`)) +\n    geom_bar(color = \"grey10\", aes(fill = `Property Type`)) +\n    scale_x_discrete(name =\"Sale Month\", \n                     limits = c(\"Jan 2023\", \"Feb 2023\", \"Mar 2023\", \n                        \"Apr 2023\",\"May 2023\",\"Jun 2023\", \n                        \"Jul 2023\", \"Aug 2023\", \"Sep 2023\",\n                        \"Oct 2023\", \"Nov 2023\", \"Dec 2023\", \n                        \"Jan 2024\", \"Feb 2024\", \"Mar 2024\")) +\n  \n    ggtitle(\"Monthly number of sales by Property Type\") +\n    theme_economist() +\n    theme(axis.text=element_text(size = 15), \n          axis.title = element_text(size = 18)\n          , title = element_text(size = 20), axis)\n\nplot5 &lt;- ggplot(data = Realis, aes(x = `Sale Month`, y = `Unit Price ($ PSM)`)) +\n    scale_x_discrete(name =\"Sale Month\",\n                   limits = c(\"Jan 2023\", \"Feb 2023\", \"Mar 2023\",\n                      \"Apr 2023\",\"May 2023\",\"Jun 2023\",\n                      \"Jul 2023\", \"Aug 2023\", \"Sep 2023\",\n                      \"Oct 2023\", \"Nov 2023\", \"Dec 2023\",\n                      \"Jan 2024\", \"Feb 2024\", \"Mar 2024\")) +\n    \n    geom_boxplot(aes(color=`Property Type`)) + \n    theme(axis.text=element_text(size=12)) +\n    ggtitle(\"Monthly changes in Unit Price ($PSM)\")+\n    theme_economist() +\n    theme(axis.text=element_text(size = 15), \n          axis.title = element_text(size = 18)\n          , title = element_text(size = 20))\n\nby_month &lt;- Realis %&gt;%\n  group_by(`Property Type`, `Sale Month`) %&gt;%\n  summarise(`Mean Unit Price ($ PSM)` = mean(`Unit Price ($ PSM)`))\n\nby_month$`Sale Month` &lt;- factor(by_month$`Sale Month`, \n                                levels = c(\"Jan 2023\", \"Feb 2023\", \"Mar 2023\",\n                                            \"Apr 2023\",\"May 2023\",\"Jun 2023\",\n                                            \"Jul 2023\", \"Aug 2023\", \"Sep 2023\",\n                                            \"Oct 2023\", \"Nov 2023\", \"Dec 2023\",\n                                            \"Jan 2024\", \"Feb 2024\", \"Mar 2024\"))\n\nby_month &lt;- by_month[order(by_month$`Sale Month`),]\n\n\nplot6 &lt;- ggplot(data = by_month, aes(x = `Sale Month`, y = `Mean Unit Price ($ PSM)`, \n                            group = `Property Type`)) +\n    geom_path(aes(color = `Property Type`), size = 1) +\n    geom_point(size = 1.5) +\n    theme(axis.text=element_text(size=12))+\n    ggtitle(\"Monthly changes in Mean Unit Price ($PSM) 2023 to first quarter 2024\")+\n    theme_economist() +\n    theme(axis.text=element_text(size = 15), \n        axis.title = element_text(size = 18)\n        , title = element_text(size = 20))\n\n(plot4 / plot5 / plot6)\n\n\n\n\n\n\n\n\n\n\nObservations and insights\n\n\n\nFrom the data visualizations above, here are my observations:\n\nThe main property types for sales from Jan 2023 to Mar 2024 seem to be Apartment and Condominium followed by Executive Condominium. The highest sales amount for this period occurred in July 2023\nThe overall Unit Price ($ PSM) for different property type during the period seems stable except for Detached House which has a dip in July 2023 and Executive Condominium with overall downward trend and a dip in Jun 2023. For Apartment type, there seems to be an overall downward trend for Mean Unit Price ($ PSM) but seems to be picking up again\nSurprisingly, contrary to what being observed in the scatterplot where some of the most expensive properties are Executive Condominium, their overall Mean Unit Price ($ PSM) has been the lowest and quite stable, albeit there is a small increase starting 2024"
  },
  {
    "objectID": "R-VAA/Project02/Project02.html",
    "href": "R-VAA/Project02/Project02.html",
    "title": "Project 2",
    "section": "",
    "text": "Assuming the role of a graphical editor of a median company, the purpose of this data visualization exercise is to prepare minimum two and maximum three data visualizations to reveal interesting insights on the private residential market and sub-markets of Singapore for the 1st quarter of 2024 and prior periods.\nTake-home Exercise 1\n\n\n\nFor this exercise and to accomplish the task, transaction data of REALIS will be used, which provides comprehensive and up-to-date statistics on the property market in Singapore.\nA complete set of the private residential property transaction data from 1st January 2023 to 31st March 2024"
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#the-task",
    "href": "R-VAA/Project02/Project02.html#the-task",
    "title": "Project 2",
    "section": "",
    "text": "Assuming the role of a graphical editor of a median company, the purpose of this data visualization exercise is to prepare minimum two and maximum three data visualizations to reveal interesting insights on the private residential market and sub-markets of Singapore for the 1st quarter of 2024 and prior periods.\nTake-home Exercise 1"
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#the-data",
    "href": "R-VAA/Project02/Project02.html#the-data",
    "title": "Project 2",
    "section": "",
    "text": "For this exercise and to accomplish the task, transaction data of REALIS will be used, which provides comprehensive and up-to-date statistics on the property market in Singapore.\nA complete set of the private residential property transaction data from 1st January 2023 to 31st March 2024"
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#loading-the-packages",
    "href": "R-VAA/Project02/Project02.html#loading-the-packages",
    "title": "Project 2",
    "section": "2.1 Loading the packages",
    "text": "2.1 Loading the packages\nFor this Take-home exercise 2, I am planning to use libraries that my classmate has put into their code below:\ntidyverse: The tidyverse is an opinionated collection of R packages designed for data science.\npatchwork: a package to make it simple to combine separate ggplots into the same graphic\nggrepel: a package to provide geoms for ggplot2 to repel overlapping text labels\nggthemes: a package to provide some extra themes, geoms, and scales for ‘ggplot2’.\nggridges: a package for Ridgeline plots, which are partially overlapping line plots that create the impression of a mountain range.\nggdist: an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualizing distributions and uncertainty which will assist with ggridges package\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualizations.\n\nThe Code:\n\npacman::p_load(ggrepel, patchwork, ggthemes, hrbrthemes,\n               tidyverse, ggridges, ggdist, colorspace)"
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#loading-the-data",
    "href": "R-VAA/Project02/Project02.html#loading-the-data",
    "title": "Project 2",
    "section": "2.2 Loading the data",
    "text": "2.2 Loading the data\nFor the take-home exercise purpose I will follow me classmate data steps:\n\nrealis_data_2023Q1 &lt;- read_csv(\"data/ResidentialTransaction20240308160536.csv\")\nrealis_data_2023Q1 &lt;- mutate(realis_data_2023Q1, Quarter='2023-Q1')\nrealis_data_2023Q2 &lt;- read_csv(\"data/ResidentialTransaction20240308160736.csv\")\nrealis_data_2023Q2 &lt;- mutate(realis_data_2023Q2, Quarter='2023-Q2')\nrealis_data_2023Q3 &lt;- read_csv(\"data/ResidentialTransaction20240308161009.csv\")\nrealis_data_2023Q3 &lt;- mutate(realis_data_2023Q3, Quarter='2023-Q3')\nrealis_data_2023Q4 &lt;- read_csv(\"data/ResidentialTransaction20240308161109.csv\")\nrealis_data_2023Q4 &lt;- mutate(realis_data_2023Q4, Quarter='2023-Q4')\nrealis_data_2024Q1 &lt;- read_csv(\"data/ResidentialTransaction20240414220633.csv\")\nrealis_data_2024Q1 &lt;- mutate(realis_data_2024Q1, Quarter='2024-Q1')\n\nrealis_data &lt;- rbind(realis_data_2023Q1, realis_data_2023Q2, realis_data_2023Q3, realis_data_2023Q4, realis_data_2024Q1)\nrealis_data$Month &lt;- substr(realis_data$`Sale Date`,3,6)"
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#the-original-design",
    "href": "R-VAA/Project02/Project02.html#the-original-design",
    "title": "Project 2",
    "section": "3.1 The original design",
    "text": "3.1 The original design\nThis original design was taken from my classmate submission Take-home Exercise 1\n\nggplot(realis_data, aes(x = Quarter, fill =`Type of Sale`)) + \n  geom_bar() +  \n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  \n  facet_wrap(~`Property Type`,scales = \"free_y\") + \n  ggtitle(label = \"Number of properties sold for each type per quarter\") + ylab(\"Number of properties sold\") + xlab(\"Quarter\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the data visualizations above, here are my starting observations\n\nThis graph purpose seems to be to showcase the number of properties sold for each Type of Sale per Property Type per Quarter.\nThe Property Type is separated into 2 rows grids\nDifferent color theme representing different Type of Sale."
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#critique-of-the-original-design",
    "href": "R-VAA/Project02/Project02.html#critique-of-the-original-design",
    "title": "Project 2",
    "section": "3.2 Critique of the original design",
    "text": "3.2 Critique of the original design\n\n\n\nFigure 1. Classmate plot\n\n\n\nPlot size: the plot size is a bit small.\nScale of measurement: since the different Property Type grid are put together this create an illusion that their scale should be the same. However, in actual fact, the plot scale is all over the place and create a misleading visualization of the actual sales for each Property Type. It would also be nice if we could have y axes on one side of the plot only to show that they are of the same scale\n2 rows grids: The Property Type separated into 2 rows grids in itself is not really a problem. However, for me personally, I think it would be nice if we could put them on the same row for easier comparison and also since there are only 6 Property Type, we could manipulate the plot to make it fit somehow\nThe label: Number of properties sold for each type per quarter is missing the Type of Sale part of the graph.\nGrid separation: it woule be nice if I could have a line or box to separate the grid so viewer could separate between which Property Type they are looking at\nThe legends: somehow the legend is taking a huge portion of the plot (approximately 1/4), this feels like such a waste of space and make it visually unattractive in a sense. It could be due to my classmate using the theme_minimal(), using a different theme and adjusting the legend position to make more space for the actual plot may make a big different here."
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#make-over-of-the-original-design.",
    "href": "R-VAA/Project02/Project02.html#make-over-of-the-original-design.",
    "title": "Project 2",
    "section": "3.3 Make over of the original design.",
    "text": "3.3 Make over of the original design.\nWith the above 3 critiques made, I will go into details below on what I plan to do with the original design to address each point. Then, I will show the final code and the alternative design\n\nPlot size:\nI will be adjusting the size of the plot by putting some adjustment to the R code chunk using {r fig.height = 8, fig.width = 18} instead of {r}\n\n\nScale of measurement:\nThis is actually quite an easy fix, by using the Cartesian coordinates via coord_cartesian() we could adjust the limits for the y axes and preventing the plot. With the knowledge of the original plot that the limit of y fall somewhere between 0 and 2600, I could just add in coord_cartesian(ylim=c(0,2600))\n\n\n2 rows grids:\nThe original code for creating the multifaceted facet_wrap(~Property Type,scales = “free_y”). However, as mentioned, for comparison purpose, I prefer if they could be on the same row with the same scale. Therefore, I will be replacing the above line of code with facet_wrap(~Property Type, nrow = 1) instead\n\n\nThe label:\nI will be renaming the plot to Number of properties sold for each Type of Sale per Property Type per Quarter.\n\n\nGrid separation:\nAnother easy change we just need to adjust the panel using panel.border argument of the gglot2 theme() here. For this purpose, I will be using\ntheme(panel.border = element_rect(color = “grey10”, linetype = “solid”, linewidth = 1)\n\n\nThe legends\nSo for this part, there is a lot of things to make adjustment here since I will be using a different theme, theme_economist() to be specific. With this theme changes, I will be adjust a multitude of arguments of the of the gglot2 theme() to adjust both the text size as well as the margin between the text and the plot. These arguments include axis.text, axis.title, title, strip.text, axis.title.y, axis.title.x\n\n\nFinal Result\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(realis_data, aes(x = Quarter, \n                        fill =`Type of Sale`)) +\n  geom_bar() +\n  \n  coord_cartesian(ylim=c(0,2600)) +\n  \n  facet_wrap(~`Property Type`, nrow = 1) +\n  \n  theme_economist() +\n  ggtitle(label = \"Number of properties sold for each Type of Sale per Property Type per Quarter\") +\n  ylab(\"Number of properties sold\") +\n  xlab(\"Quarter\") + \n  theme(axis.text=element_text(size = 9), \n        axis.title = element_text(size = 15), \n        title = element_text(size = 15, \n                               margin = margin(b = 15,)),\n        strip.text = element_text(margin = margin(b = 15)),\n        axis.title.y = element_text(margin = margin(r = 15)),\n        axis.title.x = element_text(margin = margin(t = 15)),\n        panel.border = element_rect(color = \"grey10\", \n                                    linetype = \"solid\", \n                                    linewidth = 1))\n\n\n\n\n\n\nClarity Improvements\n\nPlot size and font size and other margin are adjusted to mae sure no overlapping of texts\nConsistent scale of y-axis allows fair comparisons of properties sales and patterns across different Property Type, which is especially important since the sales for different Property Type differ greatly.\nThe title has been change to reflect correctly the visualization being made\nLegend is adjusted to the top, creating more space for the actual plot.\nFor readability sake, I have adjust the x-axes text to be horizontal instead of slanted like the original design"
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#critique-of-the-make-over-of-the-original-design",
    "href": "R-VAA/Project02/Project02.html#critique-of-the-make-over-of-the-original-design",
    "title": "Project 2",
    "section": "3.4 Critique of the make over of the original design",
    "text": "3.4 Critique of the make over of the original design\nSo the resulting plot of the make over of the original design seems to address the critiques made originally. However, I think this design could be improve event further. One critique I want to make here is that since the scale is 0 to 2600, it maybe too big to show the “Sub Sale” and “New Sale” of some of the Property Type. In addition, since this is a stacked plot, it is a bit difficult if viewer want to see the changes of the different Type of Sale over time.\nTherefore, I thought to myself what if I could separate the Type of Sale as well and hence reduce the scale of count so the missing “Sub Sale” and “New Sale” could be shown as well."
  },
  {
    "objectID": "R-VAA/Project02/Project02.html#the-further-make-over",
    "href": "R-VAA/Project02/Project02.html#the-further-make-over",
    "title": "Project 2",
    "section": "3.5 The further make over",
    "text": "3.5 The further make over\nIt is actually quite simple to separate the Type of Sale. However, instead of using the facet_wrap(), I will switch to using facet_grid().\nSimply replacing facet_wrap(~Property Type, nrow = 1) with facet_grid(cols = vars(Property Type), rows = vars(Type of Sale)), and also removing fill =Type of Sale from ggplot since we will not really need the color to show different Type of Sale\n\nFinal Result\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(realis_data, aes(x = Quarter)) + \n  geom_bar(fill = \"lightblue4\") +  \n  coord_cartesian(expand = TRUE, \n                  ylim = c(0,1680)) +\n\n  facet_grid(cols = vars(`Property Type`),\n             rows = vars(`Type of Sale`)) +\n  \n\n  theme_economist() +\n  ggtitle(label = \"Number of properties sold for each Type of Sale per Property Type per Quarter\") +    \n  ylab(\"Number of properties sold\") + \n  xlab(\"Quarter\") + \n  theme(axis.text=element_text(size = 9), \n        axis.title = element_text(size = 15), \n        title = element_text(size = 15,\n                             margin = margin(b = 15,)),\n        strip.text = element_text(margin = margin(b = 10,\n                                                  l = 10)),\n        strip.text.y.right = element_text(angle = 0),\n        axis.title.y = element_text(margin = margin(r = 15)),\n        axis.title.x = element_text(margin = margin(t = 15)),\n        panel.border = element_rect(color = \"grey10\", \n                                    linetype = \"solid\", \n                                    linewidth = 1))\n\n\n\n\nThe resulting plot seems to be doing better. However, some smaller count property type sale is still nowhere to be seen and is negligible."
  }
]