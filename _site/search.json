[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "R-GAA/Project01/Project01.html",
    "href": "R-GAA/Project01/Project01.html",
    "title": "Project 1",
    "section": "",
    "text": "Previous studies have demonstrated the significant potential of Spatial Point Patterns Analysis (SPPA) in exploring and identifying factors influencing road traffic accidents. However, these studies often focus solely on either behavioral or environmental factors, with limited consideration of temporal factors such as season, day of the week, or time of day.\n\nIn view of this, I am tasked to discover factors affecting road traffic accidents in the Bangkok Metropolitan Region BMR by employing both spatial spatio-temporal point patterns analysis methods.\n\nThe specific objectives of this take-home exercise are as follows:\n\nTo visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\nTo conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\nTo conduct detailed spatio-temporal analysis of road traffic accidents using appropriate Temporal Network Spatial Point Patterns Analysis methods."
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#importing-and-wrangling-the-data",
    "href": "R-GAA/Project01/Project01.html#importing-and-wrangling-the-data",
    "title": "Project 1",
    "section": "Importing and wrangling the data",
    "text": "Importing and wrangling the data\nThe below code would import the Thailand Road Accident (2019-2022), make changes to the coordinates by filtering out the ones with empty or NA coordinates then filter out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647. The operation would also create 2 new columns that has the Month and the Year of the date accidents occured. This step would create accident_data_sf\n\naccident_data_sf &lt;- read_csv(\"data/nongeo/thai_road_accident_2019_2022.csv\") %&gt;%\n  filter(!is.na(longitude) & longitude != \"\",\n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), \n           crs = 4326) %&gt;%\n  filter(province_en %in% list(\"Bangkok\", \"Nonthaburi\", \n                                           \"Nakhon Pathom\", \"Pathum Thani\", \n                                           \"Samut Prakan\", \"Samut Sakhon\")) %&gt;%\n  mutate(`incident_monthyear` = format(as.Date(`incident_datetime`), \"%Y/%m\")) %&gt;%\n  mutate(`incident_year` = format(as.Date(`incident_datetime`), \"%Y\")) %&gt;%\n  st_transform(crs = 32647)\n\n\nstr(accident_data_sf)\n\nsf [12,986 × 19] (S3: sf/tbl_df/tbl/data.frame)\n $ acc_code                   : num [1:12986] 571882 600001 605043 629691 571887 ...\n $ incident_datetime          : POSIXct[1:12986], format: \"2019-01-01 02:25:00\" \"2019-01-01 03:00:00\" ...\n $ report_datetime            : POSIXct[1:12986], format: \"2019-01-02 17:32:00\" \"2019-01-05 10:33:00\" ...\n $ province_th                : chr [1:12986] \"นครปฐม\" \"นนทบุรี\" \"สมุทรปราการ\" \"กรุงเทพมหานคร\" ...\n $ province_en                : chr [1:12986] \"Nakhon Pathom\" \"Nonthaburi\" \"Samut Prakan\" \"Bangkok\" ...\n $ agency                     : chr [1:12986] \"department of rural roads\" \"department of highways\" \"department of highways\" \"expressway authority of thailand\" ...\n $ route                      : chr [1:12986] \"แยกทางหลวงหมายเลข 4 (กม.ที่ 51+920) - บ้านวัดละมุด\" \"คลองวัดแดง - บางบัวทอง\" \"ราษฎร์บูรณะ - พระสมุทรเจดีย์\" \"บางพลี-สุขสวัสดิ์\" ...\n $ vehicle_type               : chr [1:12986] \"motorcycle\" \"private/passenger car\" \"private/passenger car\" \"other\" ...\n $ presumed_cause             : chr [1:12986] \"speeding\" \"speeding\" \"running red lights/traffic signals\" \"other\" ...\n $ accident_type              : chr [1:12986] \"rollover/fallen on straight road\" \"rollover/fallen on straight road\" \"collision at intersection corner\" \"other\" ...\n $ number_of_vehicles_involved: num [1:12986] 1 1 2 1 1 1 1 1 1 1 ...\n $ number_of_fatalities       : num [1:12986] 0 0 0 0 0 1 1 0 0 0 ...\n $ number_of_injuries         : num [1:12986] 2 1 0 1 1 0 0 0 0 1 ...\n $ weather_condition          : chr [1:12986] \"clear\" \"clear\" \"clear\" \"clear\" ...\n $ road_description           : chr [1:12986] \"straight road\" \"straight road\" \"other\" \"other\" ...\n $ slope_description          : chr [1:12986] \"no slope\" \"no slope\" \"other\" \"other\" ...\n $ geometry                   :sfc_POINT of length 12986; first list element:  'XY' num [1:2] 627012 1533381\n $ incident_monthyear         : chr [1:12986] \"2019/01\" \"2019/01\" \"2019/01\" \"2019/01\" ...\n $ incident_year              : chr [1:12986] \"2019\" \"2019\" \"2019\" \"2019\" ...\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:18] \"acc_code\" \"incident_datetime\" \"report_datetime\" \"province_th\" ...\n\n\nThis part is to import Thailand Roads (OpenStreetMap Export) and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create THR_sf\n\nTHR_sf &lt;- st_read(dsn = \"data/geo\", \n                         layer = \"hotosm_tha_roads_lines_shp\")\n\nReading layer `hotosm_tha_roads_lines_shp' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\Portfolio\\R-GAA\\Project01\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2792590 features and 14 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA\n\nTHR_sf &lt;-  st_set_crs(THR_sf, 32647)\n\nThis part is to import Thailand - Subnational Administrative Boundaries as well as filtering out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create THSAB_sf\n\nTHSAB_sf &lt;- st_read(dsn = \"data/geo\", \n                         layer = \"tha_admbnda_adm2_rtsd_20220121\") %&gt;%\n  filter(ADM1_EN %in% list(\"Bangkok\", \"Nonthaburi\", \"Nakhon Pathom\",\n                      \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\")) %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm2_rtsd_20220121' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\Portfolio\\R-GAA\\Project01\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 928 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#quick-analysis",
    "href": "R-GAA/Project01/Project01.html#quick-analysis",
    "title": "Project 1",
    "section": "4.1 Quick analysis",
    "text": "4.1 Quick analysis\nNow let’s take a quick look at temporal distribution of accidents data. The idea of this is to identify whether there is any identifiable temporal patterns in the distribution of accidents.\nTo to this I will be plotting a bar chart showing the counts of accidents months on end from 2019 to 2022.\n\nggplot(accident_data_sf) +\n  geom_bar(aes(x = incident_monthyear), \n                 bin = 100, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  xlab(\"Time\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above graph review that accident tends to peak around January April, July, October and December. Interestingly, these period seems to coincide with holidays seasons in Thailand.\n\n\nBut what about the year by year number of cases ?\n\nggplot(accident_data_sf) +\n  geom_bar(aes(x = incident_year),\n                 color=\"black\", \n                 fill=\"light blue\") +\n  xlab(\"Province\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above graph review that number of accidents are around the same over the year and does not seem to be getting better.\n\n\nBut how about the distribution of cases of the different region within the review region?\n\nggplot(accident_data_sf) +\n  geom_bar(aes(x = province_en),\n                 color=\"black\", \n                 fill=\"light blue\") +\n  xlab(\"Province\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above results shown that majority of accidents cases from 2019 to 2022 is actually from Bangkok province itself which does make sense since the concentration of population are probably in this region as well which leads to higher traffics\n\n\nThe graphs below are showcasing the Distribution of accidents in different scale from overall to year by year and to month by year\n\ntmap_mode('plot')\ntm_shape(THSAB_sf) +\n  tm_polygons() +\ntm_shape(accident_data_sf) +\n  tm_dots(size = 0.01, \"red\") +\ntm_layout(main.title = \"Distribution of accidents\")\n\n\n\n\n\n\n\n# tm_shape(THR_sf) +\n#   tm_lines()\n\n\ntm_shape(THSAB_sf) +\n  tm_polygons() +\ntm_shape(accident_data_sf) +\n  tm_dots(size = 0.01, \"red\") +\ntm_facets(by = \"incident_year\",\n          free.coords = FALSE,\n          free.scales = FALSE,\n          drop.units = TRUE) +\ntm_layout(main.title = \"Distribution of accidents by year\")\n\n\n\n\n\n\n\n\n\ntm_shape(THSAB_sf) +\n  tm_polygons() +\ntm_shape(accident_data_sf) +\n  tm_dots(size = 0.1, \"red\") +\ntm_facets(by = \"incident_monthyear\",\n          free.coords = FALSE,\n          free.scales = FALSE,\n          drop.units = TRUE,\n          ncol = 12,\n          nrow = 4)+\ntm_layout(main.title = \"Distribution of accidents by month by year\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe most comprehensive one are probably the month by year distribution which showcase the changes of distribution of accidents depending on the period and different seasons"
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#first-order-spatial-point-patterns-analysis",
    "href": "R-GAA/Project01/Project01.html#first-order-spatial-point-patterns-analysis",
    "title": "Project 1",
    "section": "4.2 First-order Spatial Point Patterns Analysis",
    "text": "4.2 First-order Spatial Point Patterns Analysis\nKernel Density Estimation\n\nGeospatial Data wrangling\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\naccident_data &lt;- as_Spatial(accident_data_sf)\nTHSAB &lt;- as_Spatial(THSAB_sf)\n\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\naccident_data_sp &lt;- as(accident_data, \"SpatialPoints\")\nTHSAB_sp &lt;- as(THSAB, \"SpatialPolygons\")\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\naccident_data_ppp &lt;- as.ppp(accident_data_sf)\nany(duplicated(accident_data_ppp))\n\n[1] FALSE\n\n\nSince the results return false there’s no duplicated points in the data\n\nplot(accident_data_ppp)\n\n\n\n\n\n\n\n\n\n\nCreating owin object\nThe code chunk below is used to covert the SpatialPolygon object into owin object of spatstat\n\nTHSAB_owin &lt;- as.owin(THSAB_sf)\n\n\n\nRescalling KDE values\nwe will extract childcare events that are located within the review region and re-scale the unit of measurement from meter to kilometer by using the code chunk below.\n\naccident_data_owin_ppp &lt;- accident_data_ppp[THSAB_owin]\naccident_data_owin_ppp.km &lt;- rescale.ppp(accident_data_owin_ppp, 1000, \"km\")\n\nNow, we can run density() using the resale data set and plot the output kde map. In this case I’ll be using bw.diggle() since it seems to be working best\n\nkde_accident_data_bw &lt;- density(accident_data_owin_ppp.km,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                              kernel=\"gaussian\") \nplot(kde_accident_data_bw, main = \"Fixed bandwidth\")\n\n\n\n\n\n\n\n\nTo compared the above method I’ll also be running another density using adaptive method\n\nkde_accident_data_adaptive &lt;- adaptive.density(accident_data_owin_ppp.km, method=\"kernel\")\nplot(kde_accident_data_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth graph produce roughly the same results which highlight some of the hot zone of accidents that could happen throughout the period. The KDE shown that there are hotspots in the roads between Bangkok - Samut Prakan, Bangkok - Pathum Thani and with a lower but still significant hotspot between Bangkok - Nakhon Pathom and Bangkok - Samut Sakhon\n\n\n\n\nTesting spatial point patterns using Clark and Evans Test\n\nclarkevans.test(accident_data_ppp,\n                correction=\"none\",\n                clipregion=\"THSAB_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  accident_data_ppp\nR = 0.16207, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the test results, we rejected the null hypothesis that the point patterns are randomly distributed."
  },
  {
    "objectID": "R-GAA/Project01/Project01.html#second-order-spatial-point-patterns-analysis",
    "href": "R-GAA/Project01/Project01.html#second-order-spatial-point-patterns-analysis",
    "title": "Project 1",
    "section": "4.3 Second-order Spatial Point Patterns Analysis",
    "text": "4.3 Second-order Spatial Point Patterns Analysis\n\nAnalysing Spatial Point Process Using G-Function\n\nG_CK = Gest(accident_data_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,2000))\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test Using G-Function\n\nG_CK.csr &lt;- envelope(accident_data_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the test results it seems likely that the null hypothesis is rejected and distribution of accidents at Bangkok Metropolitan Region BMR is not randomly distributed. The estimated G(r) lies above the upper envelope, the estimated G(r) is statistically significant\n\n\n\n\nAnalysing Spatial Point Process Using F-Function\n\nF_CK = Fest(accident_data_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test Using F-Function\n\nF_CK.csr &lt;- envelope(accident_data_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the test results it seems likely that the null hypothesis is rejected and distribution of accidents at Bangkok Metropolitan Region BMR is not randomly distributed"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html",
    "href": "R-GAA/Project03/Project03.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "In this take-home exercise, I am required to calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023. For the purpose of this take-home exercise, HDB Resale Flat Prices provided by Data.gov.sg should be used as the core data set. The study should focus on either three-room, four-room or five-room flat.\nhttps://isss626-ay2024-25aug.netlify.app/take-home_ex03b\nThe below packages are used and loaded in using the p_load() function of pacman package\n\npacman::p_load(tidyverse, sf, httr, jsonlite, tmap, SpatialAcc, \n               spdep, GWmodel, SpatialML, rsample, Metrics, kableExtra,\n               knitr, ggstatsplot, spatstat, see, performance)"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#first-phase-of-data-preparation-and-wrangling",
    "href": "R-GAA/Project03/Project03.html#first-phase-of-data-preparation-and-wrangling",
    "title": "Take-home Exercise 3",
    "section": "3.1 First phase of data preparation and wrangling",
    "text": "3.1 First phase of data preparation and wrangling\n\nResale data\nFirst the resale data will be loaded into data call resale using the read_cvs()\n\nresale &lt;- read_csv(\"data/non-geo/resale.csv\") %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\nhead(resale)\n\n# A tibble: 6 × 11\n  month town  flat_type block street_name storey_range floor_area_sqm flat_model\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;     \n1 2023… ANG … 2 ROOM    406   ANG MO KIO… 01 TO 03                 44 Improved  \n2 2023… ANG … 2 ROOM    323   ANG MO KIO… 04 TO 06                 49 Improved  \n3 2023… ANG … 2 ROOM    314   ANG MO KIO… 04 TO 06                 44 Improved  \n4 2023… ANG … 2 ROOM    314   ANG MO KIO… 07 TO 09                 44 Improved  \n5 2023… ANG … 2 ROOM    170   ANG MO KIO… 01 TO 03                 45 Improved  \n6 2023… ANG … 3 ROOM    225   ANG MO KIO… 04 TO 06                 67 New Gener…\n# ℹ 3 more variables: lease_commence_date &lt;dbl&gt;, remaining_lease &lt;chr&gt;,\n#   resale_price &lt;dbl&gt;\n\n\nFirst look at the data we could see that there is a range of story under storey_range and remaining_lease are actully a string instead of numeric data that need to be convert to a better data for better modelling later. Other data such as floor_area_sqm and resale_price seems to be in appropriate\nQuickly checking unique data for storey_range\n\nunique(resale$storey_range)\n\n [1] \"01 TO 03\" \"04 TO 06\" \"07 TO 09\" \"25 TO 27\" \"10 TO 12\" \"13 TO 15\"\n [7] \"16 TO 18\" \"22 TO 24\" \"19 TO 21\" \"34 TO 36\" \"28 TO 30\" \"37 TO 39\"\n[13] \"31 TO 33\" \"40 TO 42\" \"43 TO 45\" \"46 TO 48\" \"49 TO 51\"\n\n\nThere is 17 unique data which I’ll convert to numeric from 1-17 separately..\nThe remaining_lease would go through being separate into remaining_lease_yr column and remaining_lease_mth separated then recalculate under remaining_lease_time with function remaining_lease_yr*12 + remaining_lease_mth.\nAll of the above steps would be done with the code below creating resale_tidy data\n\nresale_tidy &lt;- resale %&gt;%\n  mutate(address = paste(block,street_name)) %&gt;%\n  mutate(remaining_lease_yr = as.integer(\n    str_sub(remaining_lease, 0, 2)))%&gt;%\n  mutate(remaining_lease_mth = as.integer(\n    str_sub(remaining_lease, 9, 11))) %&gt;%\n  mutate_if(is.numeric , replace_na, replace = 0) %&gt;%\n  mutate(remaining_lease_time = remaining_lease_yr*12 + remaining_lease_mth) %&gt;%\n  mutate(storey_level =  case_when(\n    storey_range == \"01 TO 03\" ~ as.integer(1),\n    storey_range == \"04 TO 06\" ~ as.integer(2),\n    storey_range == \"07 TO 09\" ~ as.integer(3),\n    storey_range == \"10 TO 12\" ~ as.integer(4),\n    storey_range == \"13 TO 15\" ~ as.integer(5),\n    storey_range == \"16 TO 18\" ~ as.integer(6),\n    storey_range == \"19 TO 21\" ~ as.integer(7),\n    storey_range == \"22 TO 24\" ~ as.integer(8),\n    storey_range == \"25 TO 27\" ~ as.integer(9),\n    storey_range == \"28 TO 30\" ~ as.integer(10),\n    storey_range == \"31 TO 33\" ~ as.integer(11),\n    storey_range == \"34 TO 36\" ~ as.integer(12),\n    storey_range == \"37 TO 39\" ~ as.integer(13),\n    storey_range == \"40 TO 42\" ~ as.integer(14),\n    storey_range == \"43 TO 45\" ~ as.integer(15),\n    storey_range == \"46 TO 48\" ~ as.integer(16),\n    storey_range == \"49 TO 51\" ~ as.integer(17)))\n\nNow with the basic resale data sort, we would next need to find the geographical location of each of these units, to do this I would first need to get the list of address of these units, using the code chunk below\n\nadd_list &lt;- sort(unique(resale_tidy$address))\n\nThis function below is used to make and API call to onemap API to extract the coordinate of these units based on its address, this would later be used based on names of shopping malls as well which I will mention.\n\nget_coords &lt;- function(add_list){\n  \n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (i in add_list){\n    #print(i)\n\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each address\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(address= i, \n                            postal = postal, \n                            latitude = lat, \n                            longitude = lng)\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found &gt; 1){\n      # Remove those with NIL as postal\n      res_sub &lt;- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row &lt;- data.frame(address= i, \n                                postal = NA, \n                                latitude = NA, \n                                longitude = NA)\n      }\n      \n      else{\n        top1 &lt;- head(res_sub, n = 1)\n        postal &lt;- top1$POSTAL \n        lat &lt;- top1$LATITUDE\n        lng &lt;- top1$LONGITUDE\n        new_row &lt;- data.frame(address= i, \n                              postal = postal, \n                              latitude = lat, \n                              longitude = lng)\n      }\n    }\n\n    else {\n      new_row &lt;- data.frame(address= i, \n                            postal = NA, \n                            latitude = NA, \n                            longitude = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  return(postal_coords)\n}\n\nOnce the function is loaded and the unit address list is created, the below code chunk is run to get all the geo coordinates of these units\n\ncoords &lt;- get_coords(add_list)\n\nJust in case I will write these coords to a rds file for later usage.\n\nwrite_rds(coords, \"data/rds/coords.rds\")\n\n\ncoords &lt;- read_rds(\"data/rds/coords.rds\")\n\nThese coordinates is then joined back to the resale_tidy\n\nresale_tidy &lt;- resale_tidy %&gt;% \n  left_join(coords)\n\nSince the coords would appear as longitude and latitude which is not sf type and would be hard for later analysis, the code chunk below would use the st_as_sf() to convert it to a POINT geometric instead, then to make sure the crs is in correct format of 3414 I will also use st_transform(). This code chunk below would create resale_tidy_final data\n\nresale_tidy_final &lt;- resale_tidy %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nJust in case I will write this data to a rds file for later usage.\n\nwrite_rds(resale_tidy_final, \"data/rds/resale_tidy_final.rds\")\n\n\nresale_tidy_final &lt;- read_rds(\"data/rds/resale_tidy_final.rds\")\n\nNow, since the study would be focusing on either 3 rooms, 4 rooms or 5 rooms units using 2023 data to predict July-September 2024 data. In this case I would be focusing on 3 rooms analysis and did one more wrangling to filter data to only include 2023 - Sep 2024 and for 3 room units.\nThe code chunk below is for this wrangling process\n\nresale_final &lt;- resale_tidy_final %&gt;%\n  filter(flat_type == '3 ROOM') %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2024-09\")\n\n\n\nCBD data\nCBD data is using the Master Plan 2014 Subzone Boundary (Web) which I would load and then filter out only the CBD region which includes ‘DOWNTOWN CORE’, ‘MARINA EAST’, ‘MARINA SOUTH’, ‘MUSEUM’, ‘NEWTON’, ‘ORCHARD’, ‘OUTRAM’, ‘RIVER VALLEY’, ‘ROCHOR’, ‘SINGAPORE RIVER’, ‘STRAITS VIEW’, st_transform() would also be used just in case in the code chunk below creating the CBD data\n\nCBD &lt;- st_read(dsn = \"data/geo\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %&gt;%\n  filter(PLN_AREA_N %in% c('DOWNTOWN CORE', 'MARINA EAST', 'MARINA SOUTH',\n                           'MUSEUM', 'NEWTON', 'ORCHARD', 'OUTRAM',\n                           'RIVER VALLEY', 'ROCHOR', 'SINGAPORE RIVER',\n                           'STRAITS VIEW'))%&gt;%\n  st_transform(crs = 3414)\n\n\n\nMall list\nSince the data is extracted from Wikipedia and only include the mall names, I would need to somehow get the coordinates for these malls. But first the mall list is loaded in creating mall_list\n\nmall_list &lt;- read_csv(\"data/non-geo/mall_list.csv\")\n\nSimilarly to the resale data I wil once again get the unique list of name instead of address this time with the code chunk below\n\nmall_name &lt;- sort(unique(mall_list$name))\n\nThen this name list would be feed into the get_coords() function creating a new list of coordinations that has the name of the malls and its coords as longitude and latitude which is not sf type and would be hard for later analysis, the code chunk below would use the st_as_sf() to convert it to a POINT geometric instead and st_transform() used to make sure the crs is in correct format. All of this would be done in the code chunk below\n\nmall_list_coords &lt;- get_coords(mall_name) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nJust in case I will write this data to a rds file for later usage.\n\nwrite_rds(mall_list_coords, \"data/rds/mall_list_coords.rds\")\n\n\nmall &lt;- read_rds(\"data/rds/mall_list_coords.rds\")\n\n\n\nPrimary school list\nFirstly since the data Generalinformationofschools.csv file include the list of all schools I would need to extract data to get the necessary information such as name and address. This is done using the code chunk below\n\nschool_list &lt;- read_csv(\"data/non-geo/Generalinformationofschools.csv\") %&gt;%\n  filter(mainlevel_code == 'PRIMARY') %&gt;%\n  select(1,3)\n\nNext similarly to the resale data or the mall data this address list would be feed into the get_coords() function creating a new list of coordinations as longitude and latitude which is not sf type and would be hard for later analysis, the code chunk below would use the st_as_sf() to convert it to a POINT geometric instead and st_transform() used to make sure the crs is in correct format. All of this would be done in the code chunk below\n\nschool_list_address &lt;- sort(unique(school_list$address))\nschool_list_coords &lt;- get_coords(school_list_address) %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\nJust in case I will also write this data to a rds file for later usage.\n\nwrite_rds(school_list_coords, \"data/rds/school_list_coords.rds\")\n\n\nprimary_school &lt;- read_rds(\"data/rds/school_list_coords.rds\")\n\n\n\nMRT\nSince MRT data is already in geographical points I just need to load it in using the code chunk below\n\nMRT &lt;- st_read(dsn = \"data/geo/LTAMRTStationExitGEOJSON.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nPreschool location\nPreschool data is already in geographical points I just need to load it in using the code chunk below\n\npreschoolslocation &lt;- st_read(\"data/geo/PreSchoolsLocation.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nKindergartens\nKindergartens data is already in geographical points I just need to load it in using the code chunk below\n\nkindergartens &lt;- st_read(dsn = \"data/geo/Kindergartens.kml\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nSupermarkets\nSupermarkets data is already in geographical points I just need to load it in using the code chunk below\n\nsupermarkets &lt;- st_read(dsn = \"data/geo/SupermarketsKML.kml\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nEldercare center\nElder care center data is already in geographical points I just need to load it in using the code chunk below\n\neldercare &lt;- st_read(dsn = \"data/geo\",\n                     layer = \"ELDERCARE\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nChildcare center\nChildcare center data is already in geographical points I just need to load it in using the code chunk below\n\nchildcare &lt;- st_read(dsn = \"data/geo\",\n                     layer = \"CHILDCARE\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nBus Stops\nBus Stops data is already in geographical points I just need to load it in using the code chunk below. However I do notice during analysis that some of the Bus stops especially ‘46239’, ‘46609’, ‘47701’, ‘46211’, ‘46219’ are located outside of Singapore hence I would remove them from this analysis\n\nbusstop &lt;- st_read(dsn = \"data/geo\",\n                     layer = \"BusStop\") %&gt;%\n  filter(!BUS_STOP_N %in% c('46239','46609','47701','46211','46219')) %&gt;%\n  st_transform(crs = 3414)\n\n\n\nCHAS clinics\nCHAS clinics data is already in geographical points I just need to load it in using the code chunk below. However I do notice during analysis that one of the clinic ‘kml_271’ is somehow located outside of Singapore hence I would remove them from this analysis\n\nCHAS &lt;- st_read(dsn = \"data/geo/CHASClinics.kml\") %&gt;%\n  filter(Name != 'kml_271')%&gt;%\n  st_transform(crs = 3414)\n\n\n\nMarket and foodcentres\nMarket and foodcentres data is already in geographical points I just need to load it in using the code chunk below\n\nmarket_foodcentre &lt;- st_read(dsn = \"data/geo/NEAMarketandFoodCentre.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\nParks\nParks data is already in geographical points I just need to load it in using the code chunk below\n\npark &lt;- st_read(dsn = \"data/geo/ParkFacilitiesGEOJSON.geojson\") %&gt;%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#second-phase-of-data-preparation-and-wrangling",
    "href": "R-GAA/Project03/Project03.html#second-phase-of-data-preparation-and-wrangling",
    "title": "Take-home Exercise 3",
    "section": "3.2 Second phase of data preparation and wrangling",
    "text": "3.2 Second phase of data preparation and wrangling\nOnce all the data are loaded in I will move on to the next step of calculating the geographical proximity and the number of facilities within a radius of HDB units.\nFirst I will create 2 buffer zone data for these unit at 1000 m or 1 km and 350 m separately. The code chunk below will be for this purpose\n\nbuffer_1km_HDB  &lt;- st_buffer(resale_final,\n                             dist = 1000)\n\n\nbuffer_350m_HDB  &lt;- st_buffer(resale_final,\n                             dist = 350)\n\nOnce the buffer zones are created, new columns are created for the resale_final and they each represent the number of facilities within a radius of HDB units either 350 m or 1 km. The function to calculate this number would be based on the lengths(st_intersects(bufferzone, facility)).\n\nresale_final$within_350m_kindergartens &lt;- lengths(st_intersects(buffer_350m_HDB, kindergartens))\nresale_final$within_350m_childcare &lt;- lengths(st_intersects(buffer_350m_HDB, childcare))\nresale_final$within_350mm_busstop &lt;- lengths(st_intersects(buffer_350m_HDB, busstop))\nresale_final$within_350mm_preschoolslocation &lt;- lengths(st_intersects(buffer_350m_HDB, preschoolslocation))\nresale_final$within_1km_chas &lt;- lengths(st_intersects(buffer_1km_HDB, CHAS))\nresale_final$within_1km_primary_school &lt;- lengths(st_intersects(buffer_1km_HDB, primary_school))\n\nNext new columns are created for the resale_final and they each represent the minimum distance from a unit to another region (CBD) or to another facility. This calculation is based on the min(st_distance(HDB, location)))/1000 or is in kilometer shortest distance\n\nresale_final &lt;- resale_final %&gt;%\n  rowwise() %&gt;%\n  mutate(prox_CBD =  as.numeric(min(st_distance(geometry, CBD)))/1000) %&gt;%\n  mutate(prox_eldercare =  as.numeric(min(st_distance(geometry, eldercare)))/1000) %&gt;%\n  mutate(prox_market_foodcentre =  as.numeric(min(st_distance(geometry, market_foodcentre)))/1000) %&gt;%\n  mutate(prox_MRT =  as.numeric(min(st_distance(geometry, MRT)))/1000) %&gt;%\n  mutate(prox_park =  as.numeric(min(st_distance(geometry, park)))/1000) %&gt;%\n  mutate(prox_mall =  as.numeric(min(st_distance(geometry, mall)))/1000) %&gt;%\n  mutate(prox_supermarkets =  as.numeric(min(st_distance(geometry, supermarkets)))/1000)\n\nOnce all this caluclation is done I will write this data to a rds file for later usage.\n\nwrite_rds(resale_final, \"data/rds/resale_final.rds\")"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#third-phase-of-data-preparation-and-wrangling",
    "href": "R-GAA/Project03/Project03.html#third-phase-of-data-preparation-and-wrangling",
    "title": "Take-home Exercise 3",
    "section": "3.3 Third phase of data preparation and wrangling",
    "text": "3.3 Third phase of data preparation and wrangling\nThis will be the final phase to get all the dat needed for the analysis\nFirstly, I will be selecting only the columns that is needed for the analysis using the code chunk below\n\nresale_final &lt;- read_rds(\"data/rds/resale_final.rds\") %&gt;%\n  select(month, resale_price, floor_area_sqm, storey_level, remaining_lease_time,\n         prox_CBD, prox_eldercare, prox_market_foodcentre, prox_MRT,\n         prox_park, prox_mall, prox_supermarkets, within_350m_kindergartens,\n         within_350m_childcare, within_350mm_busstop, \n         within_350mm_preschoolslocation, within_1km_chas, \n         within_1km_primary_school)\n\nNext I will check for the duplicated point using the sum of multiplicity or sum(multiplicity()), multiplicity() is part of spatstat package to count the number of duplicates for each point in a spatial point pattern\n\nsum(multiplicity(resale_final) &gt; 1)\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code would return a results of 12 duplicated units points, this has been cut off from running since it taking a long time to run. This indicates that there are units that could be in the same building block or very unlikely, sold multiple time during the study period.\n\n\nTo resolve this issue I will be using st_jitter() to which techincally moving points within a short distance to address overlapping points issue. In this case I will move them within a 5 meter radius. The code chunk below is used to do this.\n\nresale_final &lt;- st_jitter(resale_final, amount = 5)\n\nOnce this is done we could no longer see any duplicate point by rerunning the previous code/\n\nsum(multiplicity(resale_final) &gt; 1)\n\n[1] 0\n\n\nNow, since the task specifically specify that I would be using HDB resale transaction records in 2023 to predict HDB resale prices between July-September 2024. I will split them into 2 part call resale_main and resale_check filtered by the specific period.\n\nresale_main &lt;- resale_final %&gt;%\n  filter(month &gt;= \"2023-01\" & month &lt;= \"2023-12\")\n\nresale_check &lt;- resale_final %&gt;%\n  filter(month &gt;= \"2024-07\" & month &lt;= \"2024-09\")\n\nNext, they would be turn into the data that would be used for training and data for testing and prediction specifically call train_data and test_data. The code chunk below will be doing the above and I will write this data to a rds file for later usage.\n\nset.seed(1234)\n\ntrain_data &lt;- resale_main\ncoords_train &lt;- st_coordinates(resale_main)\n\ntrain_data &lt;- write_rds(train_data, \"data/rds/train_data.rds\")\ncoords_train &lt;- write_rds(coords_train, \"data/rds/coords_train.rds\" )\n\ntest_data &lt;- resale_check\ncoords_test &lt;- st_coordinates(resale_check)\n\ntest_data &lt;- write_rds(test_data, \"data/rds/test_data.rds\")\ncoords_test &lt;- write_rds(coords_test, \"data/rds/coords_test.rds\" )\n\nNext I would want to check how is the data is doing and see if there was any issue with Collinearity. To do this I would first create a new data set without its geometry using the code chunk below\n\nresale_main_nogeo &lt;- resale_main %&gt;%\n  st_drop_geometry()\n\nThis data would then be checked for Collinearity using the corrplot() of corrplot package in the code chunk below\n\ncorrplot::corrplot(cor(resale_main_nogeo[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince none of the correlation is higher/lower than +- 0.7, I will be keeping all the variables for this study\n\n\n\ntrain_data &lt;- read_rds(\"data/rds/train_data.rds\")\ntest_data &lt;- read_rds(\"data/rds/test_data.rds\")\ncoords_train &lt;- read_rds(\"data/rds/coords_train.rds\" )\ncoords_test &lt;- read_rds(\"data/rds/coords_test.rds\" )"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#non-spatial-multiple-linear-regression",
    "href": "R-GAA/Project03/Project03.html#non-spatial-multiple-linear-regression",
    "title": "Take-home Exercise 3",
    "section": "4.1 Non-spatial multiple linear regression",
    "text": "4.1 Non-spatial multiple linear regression\nThe code chunk below will build the multiple linear regression using the lm() of stats package to fit linear multivariate models, all the previously mentioned predictors and variables are included to build this model. Then we would use the ols_regress() of olsrr to perform the Ordinary least squares regression\n\nset.seed(1234)\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm + storey_level + \n                  remaining_lease_time + prox_CBD + prox_eldercare + \n                  prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                  prox_supermarkets + within_350m_kindergartens +\n                  within_350m_childcare + within_350mm_busstop + \n                  within_350mm_preschoolslocation + within_1km_chas +\n                  within_1km_primary_school,\n                data=train_data)\nolsrr::ols_regress(price_mlr)\n\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.875       RMSE                    42719.098 \nR-Squared                   0.766       MSE                1829816217.009 \nAdj. R-Squared              0.766       Coef. Var                  10.386 \nPred R-Squared              0.764       AIC                    153589.830 \nMAE                     31029.976       SBC                    153711.456 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                      \n-------------------------------------------------------------------------------\n                    Sum of                                                     \n                   Squares          DF       Mean Square       F          Sig. \n-------------------------------------------------------------------------------\nRegression    3.800424e+13          16      2.375265e+12    1298.089    0.0000 \nResidual      1.159738e+13        6338    1829816217.009                       \nTotal         4.960161e+13        6354                                         \n-------------------------------------------------------------------------------\n\n                                                    Parameter Estimates                                                      \n----------------------------------------------------------------------------------------------------------------------------\n                          model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n----------------------------------------------------------------------------------------------------------------------------\n                    (Intercept)    -159037.708      7560.767                 -21.035    0.000    -173859.370    -144216.045 \n                 floor_area_sqm       5268.250        97.413        0.338     54.082    0.000       5077.287       5459.213 \n                   storey_level       9668.810       325.058        0.200     29.745    0.000       9031.587      10306.033 \n           remaining_lease_time        373.707         3.791        0.849     98.581    0.000        366.276        381.139 \n                       prox_CBD      -9842.643       183.872       -0.457    -53.530    0.000     -10203.095      -9482.191 \n                 prox_eldercare       3837.286      1118.404        0.024      3.431    0.001       1644.835       6029.736 \n         prox_market_foodcentre     -15096.642       603.961       -0.235    -24.996    0.000     -16280.609     -13912.674 \n                       prox_MRT     -30707.664      1653.726       -0.126    -18.569    0.000     -33949.527     -27465.801 \n                      prox_park     -17357.348      2233.627       -0.050     -7.771    0.000     -21736.013     -12978.683 \n                      prox_mall     -16171.291      1665.034       -0.066     -9.712    0.000     -19435.321     -12907.260 \n              prox_supermarkets      25146.697      3034.004        0.055      8.288    0.000      19199.023      31094.371 \n      within_350m_kindergartens       5560.911       897.766        0.056      6.194    0.000       3800.986       7320.837 \n          within_350m_childcare        959.410       217.318        0.032      4.415    0.000        533.392       1385.427 \n           within_350mm_busstop        572.749       213.877        0.017      2.678    0.007        153.477        992.020 \nwithin_350mm_preschoolslocation      -2078.563       355.906       -0.059     -5.840    0.000      -2776.259      -1380.866 \n                within_1km_chas       -358.466        88.349       -0.033     -4.057    0.000       -531.660       -185.272 \n      within_1km_primary_school       4499.554       496.240        0.071      9.067    0.000       3526.756       5472.353 \n----------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe model has an Adj. R-Squared of 0.766 which is not bad but also not great and I believe we could do better than this, ANOVA results show that the differences between variables are statistically significant and are unlikely to be due to chance\nIts AIC (Akaike Information Criteria) 153589.830 would be kept in mind for later comparison of other models\n\n\nNext we will calculating the variance inflation factor (VIF) using the check_collinearity() of the performance package, and then explore its results within the table created by the kable() from either kableExtra or knitr package\n\nvif &lt;- check_collinearity(price_mlr)\nkable(vif, \n      caption = \"Variance Inflation Factor (VIF) Results\") %&gt;%\n  kable_styling(font_size = 18) \n\n\nVariance Inflation Factor (VIF) Results\n\n\nTerm\nVIF\nVIF_CI_low\nVIF_CI_high\nSE_factor\nTolerance\nTolerance_CI_low\nTolerance_CI_high\n\n\n\n\nfloor_area_sqm\n1.057888\n1.036092\n1.092846\n1.028537\n0.9452798\n0.9150417\n0.9651653\n\n\nstorey_level\n1.219417\n1.186839\n1.257676\n1.104272\n0.8200639\n0.7951171\n0.8425746\n\n\nremaining_lease_time\n2.012568\n1.941069\n2.089498\n1.418650\n0.4968776\n0.4785837\n0.5151799\n\n\nprox_CBD\n1.978201\n1.908356\n2.053416\n1.406485\n0.5055098\n0.4869934\n0.5240111\n\n\nprox_eldercare\n1.314164\n1.276684\n1.356721\n1.146370\n0.7609401\n0.7370710\n0.7832795\n\n\nprox_market_foodcentre\n2.400750\n2.310604\n2.497097\n1.549435\n0.4165364\n0.4004650\n0.4327872\n\n\nprox_MRT\n1.255822\n1.221328\n1.295691\n1.120634\n0.7962914\n0.7717892\n0.8187806\n\n\nprox_park\n1.102899\n1.077209\n1.137135\n1.050190\n0.9067016\n0.8794028\n0.9283247\n\n\nprox_mall\n1.260166\n1.225447\n1.300231\n1.122571\n0.7935462\n0.7690939\n0.8160285\n\n\nprox_supermarkets\n1.198851\n1.167381\n1.236238\n1.094920\n0.8341322\n0.8089060\n0.8566185\n\n\nwithin_350m_kindergartens\n2.210474\n2.129463\n2.297297\n1.486766\n0.4523916\n0.4352942\n0.4696020\n\n\nwithin_350m_childcare\n1.442154\n1.398298\n1.490838\n1.200897\n0.6934074\n0.6707638\n0.7151550\n\n\nwithin_350mm_busstop\n1.106823\n1.080858\n1.141125\n1.052056\n0.9034869\n0.8763283\n0.9251906\n\n\nwithin_350mm_preschoolslocation\n2.740045\n2.633628\n2.853395\n1.655308\n0.3649575\n0.3504598\n0.3797043\n\n\nwithin_1km_chas\n1.783973\n1.723491\n1.849511\n1.335655\n0.5605466\n0.5406835\n0.5802176\n\n\nwithin_1km_primary_school\n1.681112\n1.625605\n1.741544\n1.296577\n0.5948444\n0.5742032\n0.6151557\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVIF itself is below 5 and tolerance is within the 0.25 to 4 hence indicating that there is unlikely any issue with multicollinearity with this regression model\n\n\nThe plot below is to better visualize the VIF\n\nplot(vif) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#geographically-weighted-regression-with-gwr-method",
    "href": "R-GAA/Project03/Project03.html#geographically-weighted-regression-with-gwr-method",
    "title": "Take-home Exercise 3",
    "section": "4.2 Geographically Weighted Regression with gwr method",
    "text": "4.2 Geographically Weighted Regression with gwr method\nNext I would try out the Geographically Weighted Regression or gwr method using GWmodel package, however just running the model and then calibrating them would take a lot of time hence I would be calibrating the model at the same time.\nThe first step is to calibrate the model by calculating the adaptive bandwidth based on the training data or train_data. To do this bw.gwr() would be used in the code chunk below creating the bw_adaptive\n\nset.seed(1234)\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm + storey_level + \n                        remaining_lease_time + prox_CBD + prox_eldercare + \n                        prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                        prox_supermarkets + within_350m_kindergartens +\n                        within_350m_childcare + within_350mm_busstop + \n                        within_350mm_preschoolslocation + within_1km_chas +\n                        within_1km_primary_school,\n                      data=train_data,\n                      approach=\"CV\",\n                      kernel=\"gaussian\",\n                      adaptive=TRUE,\n                      longlat=FALSE)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(bw_adaptive, \"data/rds/bw_adaptive.rds\")\n\n\nbw_adaptive &lt;- read_rds(\"data/rds/bw_adaptive.rds\")\nbw_adaptive\n\n[1] 38\n\n\nThe results is that the calibrated adaptive bandwidth for the model should be 38\nThe next step is\n\nset.seed(1234)\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~ floor_area_sqm + storey_level + \n                            remaining_lease_time + prox_CBD + prox_eldercare + \n                            prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                            within_350m_childcare + within_350mm_busstop + \n                            within_350mm_preschoolslocation + within_1km_chas +\n                            within_1km_primary_school,\n                          data=train_data,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(gwr_adaptive, \"data/rds/gwr_adaptive.rds\")\n\n\ngwr_adaptive &lt;- read_rds(\"data/rds/gwr_adaptive.rds\")\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-09 21:02:57.55229 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + storey_level + \n    remaining_lease_time + prox_CBD + prox_eldercare + prox_market_foodcentre + \n    prox_MRT + prox_park + prox_mall + within_350m_childcare + \n    within_350mm_busstop + within_350mm_preschoolslocation + \n    within_1km_chas + within_1km_primary_school, data = train_data, \n    bw = bw_adaptive, kernel = \"gaussian\", adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_level remaining_lease_time prox_CBD prox_eldercare prox_market_foodcentre prox_MRT prox_park prox_mall within_350m_childcare within_350mm_busstop within_350mm_preschoolslocation within_1km_chas within_1km_primary_school\n   Number of data points: 6355\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-345595  -26786   -3503   21956  535104 \n\n   Coefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)                     -1.510e+05  7.585e+03 -19.904  &lt; 2e-16 ***\n   floor_area_sqm                   5.279e+03  9.815e+01  53.785  &lt; 2e-16 ***\n   storey_level                     9.902e+03  3.270e+02  30.278  &lt; 2e-16 ***\n   remaining_lease_time             3.674e+02  3.765e+00  97.587  &lt; 2e-16 ***\n   prox_CBD                        -9.436e+03  1.805e+02 -52.279  &lt; 2e-16 ***\n   prox_eldercare                   4.321e+03  1.127e+03   3.835 0.000127 ***\n   prox_market_foodcentre          -1.617e+04  5.960e+02 -27.126  &lt; 2e-16 ***\n   prox_MRT                        -2.974e+04  1.663e+03 -17.888  &lt; 2e-16 ***\n   prox_park                       -1.602e+04  2.242e+03  -7.145 9.97e-13 ***\n   prox_mall                       -1.416e+04  1.664e+03  -8.507  &lt; 2e-16 ***\n   within_350m_childcare            8.335e+02  2.177e+02   3.830 0.000130 ***\n   within_350mm_busstop             5.856e+02  2.157e+02   2.715 0.006637 ** \n   within_350mm_preschoolslocation -8.841e+02  2.610e+02  -3.387 0.000711 ***\n   within_1km_chas                 -4.614e+02  8.846e+01  -5.216 1.89e-07 ***\n   within_1km_primary_school        4.307e+03  4.971e+02   8.664  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 43140 on 6340 degrees of freedom\n   Multiple R-squared: 0.7622\n   Adjusted R-squared: 0.7616 \n   F-statistic:  1451 on 14 and 6340 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 1.17974e+13\n   Sigma(hat): 43092.71\n   AIC:  153694.5\n   AICc:  153694.6\n   BIC:  147587.7\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 38 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                          Min.     1st Qu.      Median\n   Intercept                       -4.0054e+07 -2.9240e+05 -7.0469e+04\n   floor_area_sqm                  -5.9340e+04  2.5181e+03  3.3501e+03\n   storey_level                    -1.5804e+03  5.6834e+03  7.5921e+03\n   remaining_lease_time            -1.9320e+03  1.8958e+02  2.9800e+02\n   prox_CBD                        -1.3404e+06 -2.7368e+04 -5.5153e+03\n   prox_eldercare                  -7.1923e+05 -5.2017e+03  1.2601e+04\n   prox_market_foodcentre          -4.4488e+06 -2.4613e+04  3.9812e+03\n   prox_MRT                        -1.1081e+06 -7.2842e+04 -3.3150e+04\n   prox_park                       -5.8831e+05 -3.0059e+04  2.7048e+02\n   prox_mall                       -1.8786e+06 -4.2443e+04 -6.8576e+03\n   within_350m_childcare           -5.3707e+04 -2.7396e+03 -1.3046e+02\n   within_350mm_busstop            -1.9138e+04 -1.1816e+03  1.9434e+02\n   within_350mm_preschoolslocation -3.4975e+04 -2.7861e+03 -2.1528e+02\n   within_1km_chas                 -1.2560e+04 -1.2234e+03  1.3256e+02\n   within_1km_primary_school       -3.5193e+05 -7.9389e+03 -1.0362e+02\n                                       3rd Qu.       Max.\n   Intercept                        1.8794e+05 6319227.82\n   floor_area_sqm                   4.4999e+03  127881.24\n   storey_level                     9.2998e+03   19478.85\n   remaining_lease_time             4.3956e+02     794.34\n   prox_CBD                         1.8830e+04 5131948.02\n   prox_eldercare                   4.3843e+04  877257.18\n   prox_market_foodcentre           3.7015e+04  625951.00\n   prox_MRT                        -6.2888e+02  901122.86\n   prox_park                        2.7799e+04 1112226.80\n   prox_mall                        2.2010e+04  666309.80\n   within_350m_childcare            2.7012e+03   31247.77\n   within_350mm_busstop             1.6702e+03   13729.73\n   within_350mm_preschoolslocation  1.7522e+03   29700.89\n   within_1km_chas                  1.7448e+03   27523.38\n   within_1km_primary_school        4.9770e+03  456192.00\n   ************************Diagnostic information*************************\n   Number of data points: 6355 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1168.128 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 5186.872 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 146751.4 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 145452.5 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 146518.1 \n   Residual sum of squares: 2.788363e+12 \n   R-square value:  0.9437848 \n   Adjusted R-square value:  0.9311223 \n\n   ***********************************************************************\n   Program stops at: 2024-11-09 21:03:19.81276 \n\n\n\n\n\n\n\n\nNote\n\n\n\nInterestingly this gwr.basic() also include the results of another linear regression hence we could quickly compare this result of the GWR model with the previously ran linear model. In this case the Adjusted R-square value of the GWR model is 0.9311223 sinificantly better than 0.76 of the Multilinear regression model. In addition to this its AIC is also at 146751.4 lower than 153694.5 in this model ore the previously recorded 153589.830.\nOverall this Geographically Weighted Regression model seems to perform significantly better than the Multilinear regression model\n\n\nNext I’ll calculate the calibration bandwidth for the testing data or test_data\n\nset.seed(1234)\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm + storey_level + \n                                 remaining_lease_time + prox_CBD + prox_eldercare + \n                                 prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                                 prox_supermarkets + within_350m_kindergartens +\n                                 within_350m_childcare + within_350mm_busstop +\n                                 within_350mm_preschoolslocation + within_1km_chas +\n                                 within_1km_primary_school,\n                               data=test_data,\n                               approach=\"CV\",\n                               kernel=\"gaussian\",\n                               adaptive=TRUE,\n                               longlat=FALSE)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(gwr_bw_test_adaptive, \"data/rds/gwr_bw_test_adaptive.rds\")\n\n\ngwr_bw_test_adaptive &lt;- read_rds(\"data/rds/gwr_bw_test_adaptive.rds\")\ngwr_bw_test_adaptive\n\n[1] 287\n\n\nThe results is that the calibrated adaptive bandwidth for the test data should be 38\nNext I would attempt to predict the data based on the train_data and test_data to have something for comparison\n\nset.seed(1234)\ngwr_pred &lt;- gwr.predict(resale_price ~ floor_area_sqm + storey_level +\n                          remaining_lease_time + prox_CBD + prox_eldercare +\n                          prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                          prox_supermarkets + within_350m_kindergartens +\n                          within_350m_childcare + within_350mm_busstop +\n                          within_350mm_preschoolslocation + within_1km_chas +\n                          within_1km_primary_school,\n                        data=train_data,\n                        predictdata = test_data,\n                        bw=287,\n                        kernel = 'gaussian',\n                        adaptive=TRUE,\n                        longlat = FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nUnfortunately, I was not able to overcame the “no regression point is fixed” error for this gwr.predict() function and unable to showcase them here"
  },
  {
    "objectID": "R-GAA/Project03/Project03.html#geographically-weighted-random-forest-method-of-spatialml-package",
    "href": "R-GAA/Project03/Project03.html#geographically-weighted-random-forest-method-of-spatialml-package",
    "title": "Take-home Exercise 3",
    "section": "4.3 Geographically Weighted Random Forest method of SpatialML package",
    "text": "4.3 Geographically Weighted Random Forest method of SpatialML package\nFirst let drop the geometry column of training data sets\n\ntrain_data &lt;- train_data %&gt;% \n  st_drop_geometry()\n\nThen we would run an inital Random Forest model with default setting to check how well the model would turn out using the ranger() of ranger package. In addition, I will reduce the number of tree down to 53 instead of 500 since 500 trees would take a lot of time to run for later calibration\n\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_level + \n               remaining_lease_time + prox_CBD + prox_eldercare + \n               prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n               prox_supermarkets + within_350m_kindergartens +\n               within_350m_childcare + within_350mm_busstop + \n               within_350mm_preschoolslocation + within_1km_chas +\n               within_1km_primary_school,\n             num.trees = 53,\n             mtry = 5,\n             importance = \"impurity\",\n             data=train_data)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(rf, \"data/rds/rf.rds\")\n\n\nrf &lt;- read_rds(\"data/rds/rf.rds\")\nrf\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_level + remaining_lease_time +      prox_CBD + prox_eldercare + prox_market_foodcentre + prox_MRT +      prox_park + prox_mall + prox_supermarkets + within_350m_kindergartens +      within_350m_childcare + within_350mm_busstop + within_350mm_preschoolslocation +      within_1km_chas + within_1km_primary_school, num.trees = 53,      mtry = 5, importance = \"impurity\", data = train_data) \n\nType:                             Regression \nNumber of trees:                  53 \nSample size:                      6355 \nNumber of independent variables:  16 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       633848167 \nR squared (OOB):                  0.9188036 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis model return a OOB prediction error (MSE) of 636284892 and R squared (OOB) of 0.9184915, based on R squared alone this is a pretty good model.\n\n\nNext I will attempt to calibrate this model using the bw_adaptive of 38 calculated previously and using the grf() of SpacialML package. This would fit a local version of the Random Forest algorithm, accounting for spatial non-stationarity\nThe code chunk below show the code for this.\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_level + \n                       remaining_lease_time + prox_CBD + prox_eldercare + \n                       prox_market_foodcentre + prox_MRT + prox_park + prox_mall +\n                       prox_supermarkets + within_350m_kindergartens +\n                       within_350m_childcare + within_350mm_busstop + \n                       within_350mm_preschoolslocation + within_1km_chas +\n                       within_1km_primary_school,\n                     dframe=train_data,\n                     ntree = 53,\n                     bw=38,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(gwRF_adaptive, \"data/rds/gwRF_adaptive.rds\")\n\n\ngwRF_adaptive &lt;- read_rds(\"data/rds/gwRF_adaptive.rds\")\n\n\n\ngwRF_adaptive$LocalModelSummary\n\n$l.VariableImportance\n                                      Min          Max        Mean         StD\nfloor_area_sqm                          0 1.444374e+12 12036908744 38731166250\nstorey_level                     44363034 2.112563e+11  6706363630 11869802696\nremaining_lease_time            312420137 4.938088e+11 19019149378 43649567043\nprox_CBD                                0 3.672406e+11  6845136742 20900812241\nprox_eldercare                    8062395 1.036893e+12  7365166244 30251391477\nprox_market_foodcentre            1815629 4.008882e+11  5927098746 17506021731\nprox_MRT                          1871799 9.538612e+11  7094318007 27526763708\nprox_park                         3383980 1.442077e+12  6911451236 30460063518\nprox_mall                         5138662 6.983204e+11  6454574985 26122120904\nprox_supermarkets                       0 1.749969e+12  7368199448 34438841202\nwithin_350m_kindergartens               0 1.154855e+11   987896884  5095969294\nwithin_350m_childcare                   0 1.871588e+11  2809696103 10499177446\nwithin_350mm_busstop                    0 3.120124e+11  3792415215 14101062371\nwithin_350mm_preschoolslocation         0 2.646240e+11  2349973922  7794490999\nwithin_1km_chas                         0 2.410287e+11  4166481778 14685127753\nwithin_1km_primary_school               0 3.213113e+11  2408800082 15656650488\n\n$l.MSE.OOB\n[1] 858973085\n\n$l.r.OOB\n[1] 0.8899476\n\n$l.MSE.Pred\n[1] 82225526\n\n$l.r.Pred\n[1] 0.9894652\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe results show both Global ML and Local model with R-squared (Not OOB) at 97.845 and R-squared predicted (Not OOB) at 98.947 respectively, which is quite good. In addition to this all AIC metrics (Not OOB) is at 120401.135 and and 115853.725. This seems to indicate that the local of calibrated version of the Geographically Weighted Random Forest seems to be a better model compared to the model ran by default.\nOverall, the Machine Learning method of Geographically Weighted Random Forest is performing the best compared to the Geographically Weighted Regression or the Non-spatial multiple linear regression\nThe overall trade of is time to run a actually calibrate the model could take a while, for my calibrated Geographically Weighted Random Forest it tooks around 4 hours to finish. Geographically Weighted Regression or the Non-spatial multiple linear regression are performing much better in this regards.\n\n\nNow that the model is inplace, I’ll try to actually predict the data for the July to Sep 2024. Since this the best model, it would be best to use this model to predict data itself to show case its accuracy.\nFirst step is to drop the geometry column of test data sets test_data\n\ntest_data &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\nThen the data would be predicted using the predict.grf() method of SpatialML package\n\nset.seed(1234)\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                         test_data, \n                         x.var.name=\"X\",\n                         y.var.name=\"Y\", \n                         local.w=1,\n                         global.w=0)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nGRF_pred &lt;- write_rds(gwRF_pred, \"data/rds/GRF_pred.rds\")\n\nNow with the newly created prediction data, they would be mapped to the original test_data using cbind() of base R code\n\nGRF_pred &lt;- read_rds(\"data/rds/GRF_pred.rds\")\nGRF_pred_df &lt;- as.data.frame(GRF_pred)\ntest_data_p &lt;- cbind(test_data, GRF_pred_df)\n\nOnce this has finished running I will write this data to a rds file for later usage\n\nwrite_rds(test_data_p, \"data/rds/test_data_p.rds\")\n\n\ntest_data_p &lt;- read_rds(\"data/rds/test_data_p.rds\")\n\nLet’s quickly check the root Root Mean Squared Error between the actual resale price and the predicted data\n\nMetrics::rmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\n[1] 69067.91\n\n\nI would now use the ggplot() of ggplot2 package to plot out this grapoh showing prediction data compared to actual data\n\ntheme_set(theme_light())\nggplot(data = test_data_p,\n       aes(x = GRF_pred/1000,\n           y = resale_price/1000)) +\n  geom_point() +\n  ggtitle(\"Model prediction graph\") +\n  xlab(\"Resale price (predicted) thousands $SGD\") +\n  ylab(\"Resale price (actual) thousands $SGD\") +\n  geom_abline(color = \"blue4\", size = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe graph has an diagonal line and the closer the dots are to the line the more accurate the results were. From initial observation it seems that the model has done ok in term of predicting the resale price of HDB units.\nHowever it seems there maybe more or less variables that could be added in or removed to calibrate the model further make its prediction event more accurate. There seems to be more points on the left of the line rather than the right, this signify that the predicted price are often lower than the actual resale prices. This does make sense as all our data actually did not include any economics indicators such as inflation, tax rate raise (2024 in Singapore), etc. Including these metrics in to the model building would greatly improve the model.\nIn addition, the appearance of outliers, points more on the top left and top rights has also impacted the model itself and its prediction capability. The exclusion of these outliers could potentially be beneficial for the building of a better model as well."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html",
    "href": "R-GAA/Project02/Project02.html",
    "title": "Project 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion USD from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion USD in 2020. However, it is important to note that the tourism economy of Thailand are not evenly distributed.\nWe are interested to discover:\n\nIf the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above.\n\n\n\n\n\npacman::p_load(sf, sfdep, spdep, tmap, plotly, tidyverse, Kendall)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#objectives",
    "href": "R-GAA/Project02/Project02.html#objectives",
    "title": "Project 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion USD from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion USD in 2020. However, it is important to note that the tourism economy of Thailand are not evenly distributed.\nWe are interested to discover:\n\nIf the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#the-tasks",
    "href": "R-GAA/Project02/Project02.html#the-tasks",
    "title": "Project 2",
    "section": "",
    "text": "The specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#the-packages",
    "href": "R-GAA/Project02/Project02.html#the-packages",
    "title": "Project 2",
    "section": "",
    "text": "pacman::p_load(sf, sfdep, spdep, tmap, plotly, tidyverse, Kendall)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#importing-the-raw-data",
    "href": "R-GAA/Project02/Project02.html#importing-the-raw-data",
    "title": "Project 2",
    "section": "2.1 Importing the raw data",
    "text": "2.1 Importing the raw data\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Domestic Tourism Statistics at Kaggle. We are required to use version 2 of the data set.\nThailand - Subnational Administrative Boundaries at HDX. We are required to use the province boundary data set.\n\nThe code chunk below is used to import Thailand - Subnational Administrative Boundaries as well as filtering out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create THSAB_sf\n\nTHSAB_sf &lt;- st_read(dsn = \"data/geo\", \n                         layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\Portfolio\\R-GAA\\Project02\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThis code chunk is to import Thailand Domestic Tourism Statistics data and create tourism.\nIn the below code I will also create 2 new columns for the Month and Year separately for the purpose of using them for later analysis as well as converting the province_thai column name to ADM1_TH for the purpose of left-joining with the GEO data later and removing the date column as the 2 new Month and Year column are already created.\n\ntourism &lt;- read_csv(\"data/non-geo/thailand_domestic_tourism_2019_2023_ver2.csv\") %&gt;%\n  mutate(`month` = as.numeric(format(as.Date(`date`), \"%m\"))) %&gt;%\n  mutate(`year` = as.numeric(format(as.Date(`date`), \"%Y\"))) %&gt;%\n  rename(`ADM1_TH` = `province_thai`) %&gt;%\n  select(2:9)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#visualising-foreign-revenue-indicator",
    "href": "R-GAA/Project02/Project02.html#visualising-foreign-revenue-indicator",
    "title": "Project 2",
    "section": "3.1 Visualising Foreign Revenue Indicator",
    "text": "3.1 Visualising Foreign Revenue Indicator\nFirst I want to merge the yearly foreign revenue table revenue_foreign_year with the GEO data THSAB_sf for easier analysis later. This is done using the code below\n\nrevenue_foreign &lt;- revenue_foreign_year %&gt;%\n  left_join(THSAB_sf) %&gt;%\n  select(1:2,4, 7, 20)\n\nBefore we start the analysis let create a spactime data revenue_foreign_st using revenue_foreign for the purpose of study later\n\nrevenue_foreign_st &lt;- spacetime(revenue_foreign,\n                                THSAB_sf,\n                                .loc_col = \"ADM1_EN\",\n                                .time_col = \"year\")\n\nFor the basic visualization I would still want to see if there are any potential cluster and I want to see the changes of Foreign Revenue cluster over the year. Hence for this purpose I would plot 4 graph using the data extract from revenue_foreign_2019. Therefore I will be using bclust style which is a good combination between kmeans and hclust to fill the data\n\nrevenue_foreign_2019 &lt;- revenue_foreign %&gt;%\n  filter(year == 2019)\nrevenue_foreign_2020 &lt;- revenue_foreign %&gt;%\n  filter(year == 2020)\nrevenue_foreign_2021 &lt;- revenue_foreign %&gt;%\n  filter(year == 2021)\nrevenue_foreign_2022 &lt;- revenue_foreign %&gt;%\n  filter(year == 2022)\n\nRF2019 &lt;- tm_shape(st_as_sf(revenue_foreign_2019)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2019\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2019\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2020 &lt;- tm_shape(st_as_sf(revenue_foreign_2020)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2020\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2020\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2021 &lt;- tm_shape(st_as_sf(revenue_foreign_2021)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2021\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2021\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2022 &lt;- tm_shape(st_as_sf(revenue_foreign_2022)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2022\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2022\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\n\ntmap_arrange(RF2019, RF2020, RF2021, RF2022, asp=1, ncol=4)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#global-measures-of-spatial-autocorrelation",
    "href": "R-GAA/Project02/Project02.html#global-measures-of-spatial-autocorrelation",
    "title": "Project 2",
    "section": "3.2 Global Measures of Spatial Autocorrelation",
    "text": "3.2 Global Measures of Spatial Autocorrelation\nI’ve previously created the Queen contiguity weight matrix thai_wm_q with snap = 400. Next we need to create Row-standardised weights matrix using the code below\n\nthai_rswm_q &lt;- nb2listw(thai_wm_q,\n                        style=\"W\",\n                        zero.policy = TRUE)\nthai_rswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 354 \nPercentage nonzero weights: 5.970653 \nAverage number of links: 4.597403 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1      S2\nW 77 5929 77 37.54724 320.752\n\n\n\n3.2.1 Computing Global Carlo Moran’s I\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep. from 2019 to 2022\n\nwm_q_2019 &lt;- revenue_foreign_2019 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2020 &lt;- revenue_foreign_2020 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2021 &lt;- revenue_foreign_2021 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2022 &lt;- revenue_foreign_2022 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n2019\n\nglobal_moran_perm(wm_q_2019$sum_rev,\n                  wm_q_2019$nb,\n                  wm_q_2019$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.0065291, observed rank = 751, p-value = 0.498\nalternative hypothesis: two.sided\n\n\n2020\n\nglobal_moran_perm(wm_q_2020$sum_rev,\n                  wm_q_2020$nb,\n                  wm_q_2020$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.0044982, observed rank = 788, p-value = 0.424\nalternative hypothesis: two.sided\n\n\n2021\n\nglobal_moran_perm(wm_q_2021$sum_rev,\n                  wm_q_2021$nb,\n                  wm_q_2021$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.011613, observed rank = 942, p-value = 0.116\nalternative hypothesis: two.sided\n\n\n2022\n\nglobal_moran_perm(wm_q_2022$sum_rev,\n                  wm_q_2022$nb,\n                  wm_q_2022$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.020738, observed rank = 691, p-value = 0.618\nalternative hypothesis: two.sided\n\n\nAnother way to do this is using the below test method code chunk since we already have the listw of thai_rswm_q\n\nset.seed(1234)\nbperm_2019 = moran.mc(revenue_foreign_2019$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2019\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2019$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.024516, observed rank = 862, p-value = 0.138\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2020 = moran.mc(revenue_foreign_2020$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2020\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2020$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.027343, observed rank = 879, p-value = 0.121\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2021 = moran.mc(revenue_foreign_2021$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2021\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2021$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.014846, observed rank = 855, p-value = 0.145\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2022 = moran.mc(revenue_foreign_2022$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2022\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2022$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.015294, observed rank = 799, p-value = 0.201\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll the test over the year from 2019-2022 indicate that p-value &gt; 0.05 hence the null hypothesis are not rejected\n\n\n\n\n3.2.2 Visualising Global Moran’s I\nThe code chunk below is used to plot a histogram of Simulated Moran’s I\n\n2019202020212022\n\n\n\nhist(bperm_2019$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2019 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2020$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2020 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2021$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2021 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2022$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2022 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\n\nMI_corr_2019 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2019$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2019)\n\n\n\n\n\n\n\n\n\nMI_corr_2020 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2020$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2020)\n\n\n\n\n\n\n\n\n\nMI_corr_2021 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2021$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2021)\n\n\n\n\n\n\n\n\n\nMI_corr_2022 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2022$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2022)\n\n\n\n\n\n\n\n\n\n\n3.2.3 Computing local Moran’s I\nUsing the above created wm_q data, we could create the LISA Map and visualizaing the local Moran’s I. The below code is used to create the lisa mapping for Local Moran’s I of Foreign revenue at Province level by using local_moran() of sfdep package.\n\nlisa_2019 &lt;- wm_q_2019 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2020 &lt;- wm_q_2020 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2021 &lt;- wm_q_2021 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2022 &lt;- wm_q_2022 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n3.2.4 Visualising local Moran’s I\nVisualising local Moran’s I and p-value for each year\n\n2019202020212022\n\n\n\ntmap_mode(\"plot\")\n\nmap2019_1&lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2019\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2019_2 &lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2019_1, map2019_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2020_1&lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2020\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2020_2 &lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2020_1, map2020_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2021_1&lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2021\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2021_2 &lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2021_1, map2021_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2022_1&lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2022\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2022_2 &lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2022_1, map2022_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Plotting LISA map\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\nlisa_sig_2019 &lt;- lisa_2019  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2019 &lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2019\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2019)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2020 &lt;- lisa_2020  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2020 &lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2020\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2020)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2021 &lt;- lisa_2021  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2021 &lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2021\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2021)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2022 &lt;- lisa_2022  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2022 &lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2022\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2022)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_mode(\"plot\")\ntmap_arrange(lisa_map_2019, lisa_map_2020, lisa_map_2021, lisa_map_2022, ncol = 4)"
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "R-GAA/Project02/Project02.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "Project 2",
    "section": "3.3 Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "3.3 Hot Spot and Cold Spot Area Analysis (HCSA)\n\n3.3.1 Computing local Gi* statistics\n\nwm_idw_2019 &lt;- revenue_foreign_2019 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2020 &lt;- revenue_foreign_2020 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2021 &lt;- revenue_foreign_2021 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2022 &lt;- revenue_foreign_2022 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nHCSA_2019 &lt;- wm_idw_2019 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2020 &lt;- wm_idw_2020 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2021 &lt;- wm_idw_2021 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2022 &lt;- wm_idw_2022 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_sig_2019 &lt;- HCSA_2019  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2020 &lt;- HCSA_2020  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2021 &lt;- HCSA_2021  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2022 &lt;- HCSA_2022  %&gt;%\n  filter(p_sim &lt; 0.05)\n\n\ntmap_mode(\"plot\")\n\nHCSA_map_2019 &lt;- tm_shape(st_as_sf(HCSA_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2019\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2019)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2020 &lt;- tm_shape(st_as_sf(HCSA_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2020\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2020)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2021 &lt;- tm_shape(st_as_sf(HCSA_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2021\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2021)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2022 &lt;- tm_shape(st_as_sf(HCSA_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2022\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2022)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\ntmap_arrange(HCSA_map_2019, HCSA_map_2020, HCSA_map_2021, HCSA_map_2022, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFigure above reveals the changes of hotspot and cold spot over the year."
  },
  {
    "objectID": "R-GAA/Project02/Project02.html#emerging-hotspot-analysis",
    "href": "R-GAA/Project02/Project02.html#emerging-hotspot-analysis",
    "title": "Project 2",
    "section": "3.4 Emerging Hotspot Analysis",
    "text": "3.4 Emerging Hotspot Analysis\nWe previously already create Spacetime revenue_foreign_st that included the foreign revenue data from 2019-2022\n\nis_spacetime_cube(revenue_foreign_st)\n\n[1] TRUE\n\n\n\n3.4.1 Computing Gi*\nDeriving the spatial weights\n\nrevenue_foreign_nb &lt;- revenue_foreign_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wt = st_inverse_distance(nb,\n                                  geometry,\n                                  scale = 1,\n                                  alpha = 1),\n  .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\ngi_stars &lt;- revenue_foreign_nb %&gt;%\n  group_by(year) %&gt;%\n  mutate(gi_star = local_gstar_perm(sum_rev,\n                                    nb,\n                                    wt)) %&gt;%\n  unnest(gi_star)\n\nMann-Kendall test data.frame We can replicate this for each location by using group_by() of dplyr package.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(ADM1_EN) %&gt;%\n  summarise(mk = list(unclass(\n      Kendall::MannKendall(gi_star)\n    )\n  )) %&gt;%\n  unnest_wider(mk)\n\n\n\n3.4.2 Mann-Kendall Test on Gi*\nWith these Gi* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Bankok county.\n\ncbg &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(ADM1_EN == \"Bangkok\") %&gt;%\n  select(ADM1_EN, year, gi_star)\n\nInteractive Mann-Kendall Plot\n\nggplotly(ggplot(data = cbg, \n       aes(x = year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light())\n\n\n\n\n\n\n\n3.4.3 Performing Emerging Hotspot Analysis\nUsing ehsa We can also sort to show significant emerging hot/cold spots\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:10)\nhead(emerging)\n\n# A tibble: 6 × 6\n  ADM1_EN          tau    sl     S     D  varS\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bangkok       -0.667 0.308    -4  6.00  8.67\n2 Chachoengsao  -0.667 0.308    -4  6.00  8.67\n3 Chanthaburi   -0.667 0.308    -4  6.00  8.67\n4 Chon Buri     -0.667 0.308    -4  6.00  8.67\n5 Lampang        0.667 0.308     4  6.00  8.67\n6 Nakhon Pathom -0.667 0.308    -4  6.00  8.67\n\n\n\nset.seed(1234)\nrevenue_ehsa &lt;- emerging_hotspot_analysis(\n  x = revenue_foreign_nb,\n  .var = \"sum_rev\",\n  k = 1,\n  nsim = 199,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\n\n\nrevenue_foreign_ehsa &lt;- THSAB_sf %&gt;%\n  left_join(revenue_ehsa,\n            by = join_by(ADM1_EN == location))\n\nVisualising the distribution of EHSA classes\n\nggplot(data = revenue_foreign_ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWe could see majority of location does not has any pattern\n\ntmap_mode('plot')\nrevenue_foreign_ehsa_sig &lt;- revenue_foreign_ehsa  %&gt;%\n  filter(p_value &lt; 1)\n\ntm_shape(st_as_sf(revenue_foreign_ehsa)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_text(\"ADM1_EN\", size=0.5) +\n  tm_shape(st_as_sf(revenue_foreign_ehsa_sig)) +\n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)"
  }
]